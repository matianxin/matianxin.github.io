<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[CTF中Web题目的常见题型及解题技巧]]></title>
    <url>%2F2020%2F05%2F19%2FCTF%2FCTFWeb%2F</url>
    <content type="text"><![CDATA[基础知识类题目考察基本的查看网页源代码、HTTP请求、修改页面元素等。 这些题很简单，比较难的比赛应该不会单独出，就算有应该也是Web的签到题。 实际做题的时候基本都是和其他更复杂的知识结合起来出现。 查看网页源代码按F12就都看到了，flag一般都在注释里，有时候注释里也会有一条hint或者是对解题有用的信息。 Bugku web2: http://123.206.87.240:8002/web2/ Bugku web3: http://123.206.87.240:8002/web3/ 发送HTTP请求可以用hackbar，有的也可以写脚本。 Bugku web基础$_GET: http://123.206.87.240:8002/get/ Bugku web基础$_POST: http://123.206.87.240:8002/post/ Bugku 点击一百万次: http://123.206.87.240:9001/test/ 举个写脚本的例子，（题目是Bugku web基础$POST）： 123import requestsr = requests.post('http://123.206.87.240:8002/post/', data=&#123;'what' : 'flag'&#125;)print(r.text) 不常见类型的请求发送考OPTIONS请求，如果要发送这类请求，写一个脚本应该就能解决了。 HTTP头相关的题目主要是查看和修改HTTP头。 目前做过的Web题目有很大一部分都是与HTTP头相关的，而且这种题目也相当常见，不和其他知识结合的情况下也算是属于基础题的范畴吧。 技巧：不同的类型有不同的利用方法，基本都离不开抓包改包，有些简单的也可以利用浏览器F12的网络标签解决。 但是最根本的应对策略，是熟悉一些常见请求头的格式、作用等，这样题目考到的时候就很容易知道要怎样做了。 查看响应头有时候响应头里会有hint或者题目的关键信息，也有的时候会直接把flag放在响应头里给你，但是直接查看响应头拿flag的题目并不多，因为太简单了。 只是查看的话，可以不用抓包，用F12的“网络”标签就可以解决了。 Bugku 头等舱: http://123.206.87.240:9009/hd.php 修改请求头、伪造Cookie常见的有set-cookie、XFF和Referer，总之考法很灵活，做法比较固定，知道一些常见的请求头再根据题目随机应变就没问题了。 有些题目还需要伪造Cookie，根据题目要求做就行了。 可以用BurpSuite抓包，也可以直接在浏览器的F12“网络”标签里改。 实验吧 头有点大： http://ctf5.shiyanbar.com/sHeader/ Bugku 程序员本地网站： http://123.206.87.240:8002/localhost/ Bugku 管理员系统： http://123.206.31.85:1003/ XCTF xff_referer： https://adworld.xctf.org.cn/task/answer?type=web&amp;number=3&amp;grade=0&amp;id=5068 Git源码泄露flag一般在源码的某个文件里，但也有和其他知识结合、需要进一步利用的情况，比如XCTF社区的mfw这道题。 技巧：GitHack一把梭 XCTF mfw: https://adworld.xctf.org.cn/task/answer?type=web&amp;number=3&amp;grade=1&amp;id=5002 Python爬虫信息处理这类题目一般都是给一个页面，页面中有算式或者是一些数字，要求在很短的时间内求出结果并提交，如果结果正确就可以返回flag。 因为所给时间一般都很短而且计算比较复杂，所以只能写脚本。这种题目的脚本一般都需要用到requests库和BeautifulSoup库（或者re库（正则表达式）），个人感觉使用BeautifulSoup简单一些。 技巧：requests库和BeautifulSoup库熟练掌握后，再多做几道题或者写几个爬虫的项目，一般这类题目就没有什么问题了。主要还是对BeautifulSoup的熟练掌握，另外还需要一点点web前端（HTML）的知识。 Bugku 秋名山老司机： http://123.206.87.240:8002/qiumingshan/ 1234567891011121314151617#这道题的脚本如下，还可以继续优化#经常出现执行了但是不弹flag的情况，多试几次就行了from bs4 import BeautifulSoupimport requestsr = requests.Session()s = r.get("http://123.206.87.240:8002/qiumingshan/")s.encoding = 'utf-8'text = s.textsoup = BeautifulSoup(text)tag = soup.divexpress = str(tag.string)express = express[0 : -3]answer = eval(express)ans = &#123;"value" : answer&#125;flag = r.post('http://123.206.87.240:8002/qiumingshan/', data = ans)print(flag.text) 实验吧 速度爆破： http://www.shiyanbar.com/ctf/1841HGAME2019的部分题目似乎还出现了反爬虫措施。 PHP代码审计代码审计覆盖面特别广，分类也很多，而且几乎什么样的比赛都会有，算是比较重要的题目类型之一吧。 技巧：具体问题具体分析，归根结底还是要熟练掌握PHP这门语言，了解一些常见的会造成漏洞的函数及利用方法等。 hash加密相关PHP弱类型hash比较缺陷这是代码审计最基础的题目了，也比较常见。 典型代码： 123if(md5($a) == md5($b)) &#123; //注意两个等号“==” echo $flag;&#125; 加密函数也有可能是sha1或者其他的，但是原理都是不变的。 这个漏洞的原理如下： 12== 在进行比较的时候，会先将两边的变量类型转化成相同的，再进行比较。0e在比较的时候会将其视作为科学计数法，所以无论0e后面是什么，0的多少次方还是0。 所以只要让b在经过相应的函数加密之后都是以0e开头就可以。 以下是一些md5加密后开头为0e的字符串： 12345678910111213141516171819202122232425QNKCDZO0e830400451993494058024219903391s878926199a0e545993274517709034328855841020s155964671a0e342768416822451524974117254469s214587387a0e848240448830537924465865611904s214587387a0e848240448830537924465865611904s878926199a0e545993274517709034328855841020s1091221200a0e940624217856561557816327384675s1885207154a0e509367213418206700842008763514aabg7XSs 另外，这个也可以用数组绕过，这个方法在下面会详细说。 数组返回NULL绕过PHP绝大多数函数无法处理数组，向md5函数传入数组类型的参数会使md5()函数返回NULL（转换后为False），进而绕过某些限制。 如果上面的代码变成： 123if(md5($a) === md5($b)) &#123; //两个等号变成三个 echo $flag;&#125; 那么利用弱类型hash比较缺陷将无法绕过，这时可以使用数组绕过。 传入?a[]=1&amp;b[]=2就可以成功绕过判断。 这样的方法也可以用来绕过sha1()等hash加密函数相关的判断，也可以绕过正则判断，可以根据具体情况来灵活运用。 正则表达式相关ereg正则%00截断ereg函数存在NULL截断漏洞，使用NULL可以截断过滤，所以可以使用%00截断正则匹配。 Bugku ereg正则%00截断：http://123.206.87.240:9009/5.php 数组绕过正则表达式相关的函数也可以使用数组绕过过滤，绕过方法详见数组返回NULL绕过。 上面那道题也可以用数组绕过。 单引号绕过preg_match()正则匹配在每一个字符前加上单引号可以绕过preg_match的匹配，原理暂时不明。 例如有如下代码： 1234567891011&lt;?php $p = $_GET['p']; if (preg_match('/[0-9a-zA-Z]&#123;2&#125;/',$p) === 1) &#123; echo 'False'; &#125; else &#123; $pp = trim(base64_decode($p)); if ($pp === 'flag.php') &#123; echo 'success'; &#125; &#125;?&gt; 1payload：p='Z'm'x'h'Z'y'5'w'a'H'A'= 不含数字与字母的WebShell如果题目使用preg_match()过滤掉了所有的数字和字母，但是没有过滤PHP的变量符号$，可以考虑使用这种方法。 典型代码： 123456789101112131415161718&lt;?phpinclude'flag.php';if(isset($_GET['code']))&#123; $code=$_GET['code']; if(strlen($code)&gt;50)&#123; die("Too Long."); &#125; if(preg_match("/[A-Za-z0-9_]+/",$code))&#123; die("Not Allowed."); &#125; @eval($code);&#125;else&#123; highlight_file(__FILE__);&#125;//$hint = "php function getFlag() to get flag";?&gt; 这种方法的核心是字符串的异或操作。 爆破脚本： 123456chr1 = ['@', '!', '"', '#', '$', '%', '&amp;', '\'', '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '&lt;', '=', '&gt;', '?', '[', '\\', ']', '^', '_', '`', '&#123;', '|', '&#125;', '~']chr2 = ['@', '!', '"', '#', '$', '%', '&amp;', '\'', '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '&lt;', '=', '&gt;', '?', '[', '\\', ']', '^', '_', '`', '&#123;', '|', '&#125;', '~']for i in chr1 : for j in chr2 : print(i + 'xor' + j + '=' + (chr(ord(i) ^ ord(j)))) 根据题目的要求，用异或出来的字符串拼出合适的Payload，并放在PHP变量中执行。变量名可以用中文。 比如这道题的 1Payload：?code=$啊="@@^|@@@"^"'%*:,!'";$啊(); Linux通配符绕过正则匹配典型代码如下，与前一种题目非常相似，但也不大一样： 12345678910111213141516&lt;?phpif(isset($_GET['code']))&#123; $code=$_GET['code']; if(strlen($code)&gt;50)&#123; die("Too Long."); &#125; if(preg_match("/[A-Za-z0-9_$]+/",$code))&#123; die("Not Allowed."); &#125; @eval($code);&#125;else&#123; highlight_file(__FILE__);&#125;//flag in /?&gt; 最主要的区别就是过滤了$和_，也就是说无法使用变量符号$了。 这时候可以考虑采用通配符绕过。 通配符有点像正则表达式，有自己的匹配规则： | 字符 | 解释 || * | 匹配任意长度任意字符 || ? | 匹配任意单个字符 || [list] | 匹配指定范围内(list)任意单个字符，也可以是单个字符组成的集合 || [^list] | 匹配指定范围外的任意单个字符或字符集合 || [!list] | 同[^list] || {str1,str2,…} | 匹配str1或者str2或者更多字符串，也可以是集合 | 所以构造一下通配符就是 1/???/??? /* 因为过滤了变量符号，没法通过上面那种方法来执行了。但是，可以通过闭合PHP标记来执行，也就是：?&gt;（/bin/cat /*）。 所以本题的 1payload为：?code=?&gt;&lt;?=/???/??? /*?&gt; 具体解法可以参照此篇文章的前两道题目：https://www.jianshu.com/p/ecc2414ec110 命令执行漏洞assert()函数引起的命令执行assert函数的参数为字符串时，会将字符串当做PHP命令来执行。 例如：assert(‘phpinfo()’)相当于执行 以一道题目为例： 本题目中题目文件夹下放置了一个隐藏的flag文件。 1234567891011121314&lt;?phperror_reporting(0);if (isset($_GET['file'])) &#123; if($_GET['file'] === "flag")&#123; highlight_file("flag.php"); &#125;else&#123; $page = $_GET['file']; &#125;&#125; else &#123; $page = "./flag.php";&#125;assert("file_exists('$page')");?&gt; 解法： 构造闭合 file_exists()函数，并使assert()执行恶意代码。 Linux命令ls -a可用于查看该目录下所有文件，包括隐藏文件。 123payload：?file=123') or system('ls -a');#?file=123') or system('cat .ffll44gg');# XSS题目这类题目会涉及到三种XSS类型，具体类型要根据题目来判断。一般都是向后台发送一个带有XSS Payload的文本，在返回的Cookie中含有flag，解法是在XSS Payload。 这类题目一般都会带有过滤和各种限制，需要了解一些常用的绕过方法。 技巧：XSS归根结底还是JavaScript，JavaScript的威力有多大，XSS的威力就有多大。要知道一些常用的XSS Payload，还要把三类XSS的原理弄明白。做题时需要用到XSS平台，网上有公用的，也可以自己在VPS上搭一个。 JavisOJ babyxss：http://web.jarvisoj.com:32800/ 绕过waf其实绝大多数比较难的题目多多少都会对输入有过滤，毕竟在现实的网络中肯定是会对输入进行限制的，但是这里还是把过滤单独列出来了。 技巧：多掌握一些不同的绕过方法。 长度限制有些题目会要求输入较长的文本，但对文本的长度进行了限制。 对于这种题目，既可以用BurpSuite抓包改包绕过，也可以直接在F12里改页面源代码。 Bugku 计算器（修改页面源代码）：http://123.206.87.240:8002/yanzhengma/ DVWA 存储型XSS的标题栏会对长度进行限制，使用BurpSuite抓包绕过。 双写双写可以绕过对输入内容过滤的单次判断，在XSS、SQL注入和PHP代码审计的题目中比较常见。 双写顾名思义就是将被过滤的关键字符写两遍，比如，如果要添加XSS Payload，又需要插入 ```标签，就可以构造如下的1234```phpPayload：&lt;scr&lt;script&gt;ipt&gt;` 来绕过对 ```标签的单次过滤限制。123456789101112这样的方法不仅对XSS有用，也可以用于代码审计和SQL注入。HGAME2019有一道XSS题目就是过滤了```html &lt;script&gt; ```，可以用双写绕过。#### 等价替代就是不用被过滤的字符，而使用没有被过滤却会产生相同效果的字符。比如，如果SQL注入题目中过滤了空格，可以用```php/**/ 绕过对空格的限制；XSS题目如果过滤了 ```标签，可以使用其他类型的12```phppayload；如果需要使用cat命令却被过滤，可以使用tac、more、less命令来替代等。 实验吧 简单的SQL注入：http://www.shiyanbar.com/ctf/1875 URL编码绕过如果过滤了某个必须要用的字符串，输入的内容是以GET方式获取的（也就是直接在地址栏中输入），可以采用url编码绕过的方式。比如，过滤了 cat，可以使用 c%61t来绕过。 Linux命令使用反斜杠绕过在Linux下，命令中加入反斜杠与原命令完全等价。例如，cat与 ca\t两条命令等价，效果完全相同。 可以利用这个特性来进行一些绕过操作（当然，这个仅限于命令执行漏洞）。 URL二次解码绕过这个类型本来应该放在代码审计里面，但是既然是一种绕过过滤的姿势，就写在这里了。 如果源码中出现了urldecode()函数，可以利用url二次解码来绕过。 以下是一些常用的HTML URL编码： | ASCII Value | URL-encode | ASCII Value | URL-encode | ASCII Value | URL-encode || æ | %00 | 0 | %30 | ` | %60 || | %01 | 1 | %31 | a | %61 || | %02 | 2 | %32 | b | %62 || | %03 | 3 | %33 | c | %63 || | %04 | 4 | %34 | d | %64 || | %05 | 5 | %35 | e | %65 || | %06 | 6 | %36 | f | %66 || | %07 | 7 | %37 | g | %67 || backspace | %08 | 8 | %38 | h | %68 || tab | %09 | 9 | %39 | i | %69 || linefeed | %0a | : | %3a | j | %6a || | %0b | ; | %3b | k | %6b || | %0c | &lt; | %3c | l | %6c || c return | %0d | = | %3d | m | %6d || | %0e | &gt; | %3e | n | %6e || | %0f | ? | %3f | o | %6f || | %10 | @ | %40 | p | %70 || | %11 | A | %41 | q | %71 || | %12 | B | %42 | r | %72 || | %13 | C | %43 | s | %73 || | %14 | D | %44 | t | %74 || | %15 | E | %45 | u | %75 || | %16 | F | %46 | v | %76 || | %17 | G | %47 | w | %77 || | %18 | H | %48 | x | %78 || | %19 | I | %49 | y | %79 || | %1a | J | %4a | z | %7a || | %1b | K | %4b | { | %7b || | %1c | L | %4c | | | %7c || | %1d | M | %4d | } | %7d || | %1e | N | %4e | ~ | %7e || | %1f | O | %4f | | %7f || space | %20 | P | %50 | € | %80 || ! | %21 | Q | %51 | | %81 || “ | %22 | R | %52 | ‚ | %82 || # | %23 | S | %53 | ƒ | %83 || $ | %24 | T | %54 | „ | %84 || % | %25 | U | %55 | … | %85 || &amp; | %26 | V | %56 | † | %86 || ‘ | %27 | W | %57 | ‡ | %87 || ( | %28 | X | %58 | ˆ | %88 || ) | %29 | Y | %59 | ‰ | %89 || * | %2a | Z | %5a | Š | %8a || + | %2b | [ | %5b | ‹ | %8b || , | %2c | \ | %5c | Œ | %8c || - | %2d | ] | %5d | | %8d || . | %2e | ^ | %5e | Ž | %8e || / | %2f | _ | %5f | | %8f | Bugku urldecode二次编码绕过：http://123.206.87.240:9009/10.php 数组绕过详见PHP代码审计的“数组返回NULL”绕过。 数组绕过的应用很广，很多题目都可以用数组绕过。 SQL注入SQL注入是一种灵活而复杂的攻击方式，归根结底还是考察对SQL语言的了解和根据输入不同数据网页的反应对后台语句的判断，当然也有sqlmap这样的自动化工具可以使用。 技巧：如果不用sqlmap或者是用不了，就一定要把SQL语言弄明白，sqlmap这样的自动化工具也可以使用。 使用sqlmapsqlmap的应用范围还不大明确，我都是如果sqlmap没法注入就手工注入。 sqlmap的使用：https://www.jianshu.com/p/4509bdf5e3d0 一些sqlmap的命令，留着备用：https://www.freebuf.com/sectool/164608.html 感觉这篇教程也不错：https://www.cnblogs.com/im404/p/3505894.html 命令合集：https://www.jianshu.com/p/fa77f2ed788b]]></content>
      <categories>
        <category>CTF</category>
      </categories>
      <tags>
        <tag>ctf</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CTF常见的题型]]></title>
    <url>%2F2020%2F05%2F19%2FCTFType%2F</url>
    <content type="text"><![CDATA[CTF常见的题型CTF比赛通常包含的题目类型有七种，包括MISC、PPC、CRYPTO、PWN、REVERSE、WEB、STEGA。 1.MISC(Miscellaneous)√ 类型，即安全杂项，题目或涉及流量分析、电子取证、人肉搜索、数据分析等等。 2.PPC(Professionally Program Coder)类型，即编程类题目，题目涉及到编程算法，相比ACM较为容易。 3.CRYPTO(Cryptography)√ 类型，即密码学，题目考察各种加解密技术，包括古典加密技术、现代加密技术甚至出题者自创加密技术。 4.PWN√ 类型，PWN在黑客俚语中代表着攻破、取得权限，多为溢出类题目。 5.REVERSE√ 类型，即逆向工程，题目涉及到软件逆向、破解技术。 6.STEGA(Steganography)类型，即隐写术，题目的Flag会隐藏到图片、音频、视频等各类数据载体中供参赛者获取。 7.WEB√ 类型，即题目会涉及到常见的Web漏洞，诸如注入、XSS、文件包含、代码执行等漏洞。]]></content>
      <categories>
        <category>CTF</category>
      </categories>
      <tags>
        <tag>ctf</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NFS 详解]]></title>
    <url>%2F2020%2F05%2F08%2FNFS%2F</url>
    <content type="text"><![CDATA[nfs客户端卡住问题碰到nfs客户端卡住的情况，umount -f /mnt提示device is busy，并且尝试访问挂载目录、df -h等操作都会使终端卡住，ctrl+c也不能强行退出。 造成这种现象的原因是nfs服务器/网络挂了，nfs客户端默认采用hard-mount选项，而不是soft-mount。他们的区别是soft-mount: 当客户端加载NFS不成功时，重试retrans设定的次数.如果retrans次都不成功，则放弃此操作，返回错误信息 “Connect time out”hard-mount: 当客户端加载NFS不成功时,一直重试，直到NFS服务器有响应。hard-mount 是系统的缺省值。在选定hard-mount 时，最好同时选 intr , 允许中断系统的调用请求，避免引起系统的挂起。当NFS服务器不能响应NFS客户端的 hard-mount请求时， NFS客户端会显示“NFS server hostname not responding, still trying” nfs参数介绍下面列出mount关于nfs相关的参数（1）-a：把/etc/fstab中列出的路径全部挂载。（2）-t：需要mount的类型，如nfs等。（3）-r：将mount的路径定为read only。（4）-v mount：过程的每一个操作都有message传回到屏幕上。（5）rsize=n：在NFS服务器读取文件时NFS使用的字节数，默认值是4096个字节。（6）wsize=n：向NFS服务器写文件时NFS使用的字节数，默认值是4096个字节。（7）timeo=n：从超时后到第1次重新传送占用的1/7秒的数目，默认值是7/7秒。（8）retry=n：在放弃后台mount操作之前可以尝试的次数，默认值是7 000次。（9）soft：使用软挂载的方式挂载系统，若Client的请求得不到回应，则重新请求并传回错误信息。（10）hard：使用硬挂载的方式挂载系统，该值是默认值，重复请求直到NFS服务器回应。（11）intr：允许NFS中断文件操作和向调用它的程序返回值，默认不允许文件操作被中断。（12）fg：一直在提示符下执行重复挂载。（13）bg：如果第1次挂载文件系统失败，继续在后台尝试执行挂载，默认值是失败后不在后台处理。（14）tcp：对文件系统的挂载使用TCP，而不是默认的UDP。 如#mount -t nfs -o soft 192.168.1.2:/home/nfs /mnt nfs传输尺寸至于传输尺寸的选择，可以进行实际测试：time dd if=/dev/zero of=/mnt/nfs.dat bs=16k count=16384即向nfs服务器上的nfs.dat文件里写入16384个16KB的块（也有经验说文件大小可以设定为nfs服务器内存的2倍）。得到输出如：输出了 16384+0 个块user 0m0.200s输出了 66535+0 个块user 0m0.420s192.168.1.4:/mnt /home/nfs nfs rsize=8192,wsize=8192,timeo=10,intr重新挂载nfs服务器，调整读写块大小后重复上述过程，可以找到最佳传输尺寸。 NFS服务器的故障排除故障排除思路:NFS出现了故障，可以从以下几个方面着手检查。（1）NFS客户机和服务器的负荷是否太高，服务器和客户端之间的网络是否正常。（2）/etc/exports文件的正确性。（3）必要时重新启动NFS或portmap服务。运行下列命令重新启动portmap和NFS：service portmap restartservice nfs start（4）检查客户端中的mount命令或/etc/fstab的语法是否正确。（5）查看内核是否支持NFS和RPC服务。普通的内核应有的选项为CONFIG_NFS_FS=m、CONFIG_NFS_V3=y、CONFIG_ NFSD=m、CONFIG_NFSD_V3=y和CONFIG_SUNRPC=m。我们可以使用常见的网络连接和测试工具ping及tracerroute来测试网络连接及速度是否正常，网络连接正常是NFS作用的基础。rpcinfo命令用于显示系统的RPC信息，一般使用-p参数列出某台主机的RPC服务。用rpcinfo-p命令检查服务器时，应该能看到portmapper、status、mountd nfs和nlockmgr。用该命令检查客户端时，应该至少能看到portmapper服务。 使用nfsstat命令查看NFS服务器状态nfsstat命令显示关于NFS和到内核的远程过程调用（RPC）接口的统计信息，也可以使用该命令重新初始化该信息。如果未给定标志，默认是nfsstat -csnr命令。使用该命令显示每条信息，但不能重新初始化任何信息。 nfsstat命令的主要参数如下。（1）-b：显示NFS V4服务器的其他统计信息。（2）c：只显示客户机端的NFS和RPC信息，允许用户仅查看客户机数据的报告。nfsstat命令提供关于被客户机发送和拒绝的RPC和NFS调用数目的信息。要只显示客户机NFS或者RPC信息，将该参数与-n或者-r参数结合。（3）-d：显示与NFS V4授权相关的信息。（4）-g：显示RPCSEC_GSS信息。（5）-m：显示每个NFS文件系统的统计信息，该文件系统和服务器名称、地址、安装标志、当前读和写大小，以及重新传输计数（6）-n：为客户机和服务器显示NFS信息。要只显示NFS客户机或服务器信息，将该参数与-c和-s参数结合。（7）-r：显示RPC信息。（8）-s：显示服务器信息。（9）-t：显示与NFS标识映射子系统的转换请求相关的统计信息，要只显示NFS客户机或服务器信息，将-c和-s选项结合。（10）-4：当与-c、-n、-s或-z参数组合使用时，将包含NFS V4客户机或服务器的信息，以及现有的NFS V2和V3数据。（11）-z：重新初始化统计信息。该参数仅供root用户使用，并且在显示上面的标志后可以和那些标志中的任何一个组合到统计信息的零特殊集合。 要显示关于客户机发送和拒绝的RPC和NFS调用数目的信息，输入：nfsstat -c要显示和打印与客户机NFS调用相关的信息，输入如下命令：nfsstat -cn要显示和打印客户机和服务器的与RPC调用相关的信息，输入如下命令：nfsstat -r要显示关于服务器接收和拒绝的RPC和NFS调用数目的信息，输入如下命令：nfsstat –s]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Openstack凝思系统虚拟桌面鼠标漂移的问题]]></title>
    <url>%2F2020%2F05%2F06%2FOpenstackGraphicMouse%2F</url>
    <content type="text"><![CDATA[问题描述在OpenstackQueens环境下分别安装Windows，Kali及凝思图型界面操作系统，发现凝思操作系统出现鼠标漂移的问题。经调查，默认情况下有图型界面的实例使用的输入设备类型为usbtablet，Windows，Kali在usbtablet下没有问题，凝思操作系统鼠标漂移。凝思在使用ps2mouse下没有问题，而ps2mouse下Windows，Kali鼠标出现漂移。 解决办法镜像定制化:在制作除凝思操作系统的镜像时，使用定制属性: 1hw_pointer_model='usbtablet' 即使用glanceClient创建镜像时， 12glanceClient.images.create( name=imageName, container_format='bare', disk_format='qcow2', hw_pointer_model='usbtablet') 或者镜像创建后，修改镜像信息: 1glance image-update --property hw_pointer_model=usbtablet [IMAGE-ID] 然后在nova配置文件中[DEFAULT]中增加: 123[DEFAULT]...pointer_model=ps2mouse 更新后重启计算节点的Nova-compute服务: 1systemctl restart openstack-nova-compute.service 这时，使用hw_pointer_model属性的镜像默认仍使用镜像定制的usbtablet，而凝思系统则使用ps2mouse的输入方式。]]></content>
      <categories>
        <category>Openstack</category>
      </categories>
      <tags>
        <tag>openstack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Openstack虚拟桌面协议SPICE代替vnc]]></title>
    <url>%2F2020%2F05%2F06%2FSPICE%2F</url>
    <content type="text"><![CDATA[题记VNC是OpenStack的Nova默认的连接协议，面对一些简单的管理工作表现也不错，但是如果用户经常使用Windows桌面，VNC就显得能力不足。一般情况下，使用Spice协议来代替VNC。 VNCVNC (Virtual Network Computing)是虚拟网络计算机的缩写。VNC 是一款优秀的远程控制工具软件，由著名的 AT&amp;T 的欧洲研究实验室开发的。VNC 是在基于 UNIX 和 Linux 操作系统的免费的开源软件，远程控制能力强大，高效实用，其性能可以和 Windows 和 MAC 中的任何远程控制软件媲美。 在 Linux 中，VNC 包括以下四个命令：vncserver，vncviewer，vncpasswd，和 vncconnect。大多数情况下我只需要其中的两个命令：vncserver 和 vncviewer。 SPICE 已经支持和即将支持的功能当前支持功能:• 图形界面 - processes and transmits 2D graphic commands• 视频流 - heuristically identifies video streams and transmits M-JPEG video streams• 图片压缩 - offers verios compression algorithm that were built specifically for Spice, including QUIC (based on SFALIC), LZ, GLZ (history-based global dictionary), and auto (heuristic compression choice per image)• 硬件鼠标- processes and transmits cursor-specific commands• 图像,颜色,鼠标缓存 - manages client caches to reduce bandwidth requirements• 在线切换 - supports clients while migrating Spice servers to new hosts, thus avoiding interruptions• Windows 驱动 - Windows drivers for QXL display device and VDI-port• 多监视器• 客户端支持linux和windows - can be easily ported to additional platforms.• 立体声音频 - supports audio playback and captures; audio data stream is optionally compressed using CELT• 加密 - using OpenSSL• 两种鼠标模式- provides client (more user-friendly) and server (increased accuracy and fully synchronized) modes• 音频视频同步 - synchronizes video streams with audio clocks• Spice 代理 - running on the guest and performs tasks for the client• 剪切板共享 - allows copy paste between clients and the virtual machine 未来将支持的新功能:• 网络隧道 (in progress) - using virtual network interface to enable sharing of network resources. Currently the focus is on printer sharing but is not limited to that.• Off-screen surfaces (in progress) - supports off-screen surfaces as infrastructure for future DirectDraw, video acceleration and 3D acceleration. GDI and X11 will also benefit from this feature. It will also lay foundation for multi-head support• 共享usb (in progress) - allows clients to share their USB devices with Spice servers• Direct Draw• 客户端GUI - Enables user-friendly configuration• 屏幕管理 - add support for enabling selection of the screen used by the client• 配置文件 - enables persistent user and administrative settings• 共享光驱 - share your CD with Spice server• 视频加速• 3D加速• 支持Aero• Linux features parity• OSX client• Simultaneous clients connection Openstack启用SPICE协议安装软件包控制节点: 1yum install spice-server spice-protocol openstack-nova-spicehtml5proxy spice-html5 计算节点: 1yum install spice-server spice-protocol spice-html5 12345678spice-html5来自epel源，spice-server，spice-protocol来自CentOS官方源如果找不到spice-html5，添加epel源wget http://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpmrpm -ivh epel-release-latest-7.noarch.rpmyum repolist ##检查是否已添加至源列表)如果离线安装:需安装epel-release-latest-7.noarch,openstack-nova-spicehtml5proxy-17.0.10-1.el7.noarch,spice-html5-0.1.7-1.el7.noarch,spice-protocol-0.12.14-1.el7.noarch,spice-server-0.14.0-9.el7.x86_64的RPM包 修改配置文件控制节点: 12345678910111213vi /etc/nova/nova.conf这里明确两点：指定vnc_enabled=false，否则即使配置了spice，系统也仍然使用vnc一定要注释掉原vnc配置[default]vnc_enabled=false[spice]html5proxy_host=192.100.200.140html5proxy_port=6082keymap=en-us 计算节点: 1234567891011vi /etc/nova/nova.conf[default]vnc_enabled=false[spice]html5proxy_base_url=http://192.100.200.140:6082/spice_auto.htmlserver_listen=0.0.0.0#server_proxyclient_address=192.100.200.150enabled=truekeymap=en-us 重启服务控制节点: 123456789停止novncproxy并取消自启动systemctl stop openstack-nova-novncproxy.servicesystemctl disable openstack-nova-novncproxy.service启用spicehtml5proxy开机自启动并启动它systemctl enable openstack-nova-spicehtml5proxy.servicesystemctl start openstack-nova-spicehtml5proxy.service 计算节点: 1systemctl restart openstack-nova-compute.service]]></content>
      <categories>
        <category>Openstack</category>
      </categories>
      <tags>
        <tag>openstack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Openstack控制节点删除计算节点的方法]]></title>
    <url>%2F2020%2F04%2F14%2FOpenstackDelCompute%2F</url>
    <content type="text"><![CDATA[在控制节点Controller的操作：删除计算节点名称为compute5：1.查看计算主机及服务相关： 1234567891011[root@controller ~]# openstack host list+------------+-------------+----------+| Host Name | Service | Zone |+------------+-------------+----------+| controller | consoleauth | internal || controller | scheduler | internal || controller | conductor | internal || compute4 | compute | nova || compute6 | compute | nova || compute5 | compute | nova |+------------+-------------+----------+ 1234567891011[root@controller ~]# nova service-list+--------------------------------------+------------------+------------+----------+---------+-------+----------------------------+-----------------+-------------+| Id | Binary | Host | Zone | Status | State | Updated_at | Disabled Reason | Forced down |+--------------------------------------+------------------+------------+----------+---------+-------+----------------------------+-----------------+-------------+| 936a4177-d08d-4f62-bb8c-7da7047ab6f8 | nova-consoleauth | controller | internal | enabled | up | 2020-04-14T05:59:07.000000 | - | False || c1395bf8-47dd-47b0-bc8c-e10fc083ad40 | nova-scheduler | controller | internal | enabled | up | 2020-04-14T05:59:11.000000 | - | False || 28958175-5745-4b9f-b71d-e431168f6119 | nova-conductor | controller | internal | enabled | up | 2020-04-14T05:59:10.000000 | - | False || c586b1d2-f326-4e7e-8a3a-fead78a151c0 | nova-compute | compute4 | nova | enabled | up | 2020-04-14T05:59:11.000000 | - | False || b6f3a4e3-a0ec-4f41-b47d-976bd45581b6 | nova-compute | compute6 | nova | enabled | up | 2020-04-14T05:59:03.000000 | - | False || bb93240f-58f4-4d3c-a040-a66a9b2f0ea9 | nova-compute | compute5 | nova | enabled | down | 2020-04-14T05:59:12.000000 | - | False |+--------------------------------------+------------------+------------+----------+---------+-------+----------------------------+-----------------+-------------+ 计算节点compute的State状态是down，但Status状态还是enabled可用。2.修改compute5为不可用状态。 1[root@controller ~]# nova service-disable bb93240f-58f4-4d3c-a040-a66a9b2f0ea9 查看是否修改成功 1234567891011[root@controller ~]# nova service-list+--------------------------------------+------------------+------------+----------+---------+-------+----------------------------+-----------------+-------------+| Id | Binary | Host | Zone | Status | State | Updated_at | Disabled Reason | Forced down |+--------------------------------------+------------------+------------+----------+---------+-------+----------------------------+-----------------+-------------+| 936a4177-d08d-4f62-bb8c-7da7047ab6f8 | nova-consoleauth | controller | internal | enabled | up | 2020-04-14T05:59:07.000000 | - | False || c1395bf8-47dd-47b0-bc8c-e10fc083ad40 | nova-scheduler | controller | internal | enabled | up | 2020-04-14T05:59:11.000000 | - | False || 28958175-5745-4b9f-b71d-e431168f6119 | nova-conductor | controller | internal | enabled | up | 2020-04-14T05:59:10.000000 | - | False || c586b1d2-f326-4e7e-8a3a-fead78a151c0 | nova-compute | compute4 | nova | enabled | up | 2020-04-14T05:59:11.000000 | - | False || b6f3a4e3-a0ec-4f41-b47d-976bd45581b6 | nova-compute | compute6 | nova | enabled | up | 2020-04-14T05:59:03.000000 | - | False || bb93240f-58f4-4d3c-a040-a66a9b2f0ea9 | nova-compute | compute5 | nova | disabled| down | 2020-04-14T05:59:12.000000 | - | False |+--------------------------------------+------------------+------------+----------+---------+-------+----------------------------+-----------------+-------------+ 3.在数据库（nova）中清理 12345678910111213141516171819[root@controller ~]# mysql -pEnter password:Welcome to the MariaDB monitor. Commands end with ; or g.Your MariaDB connection id is 980Server version: 10.1.20-MariaDB MariaDB ServerCopyright (c) 2000, 2016, Oracle, MariaDB Corporation Ab and others.Type 'help;' or 'h' for help. Type 'c' to clear the current input statement.MariaDB [(none)]&gt; use novaReading table information for completion of table and column namesYou can turn off this feature to get a quicker startup with -ADatabase changedMariaDB [nova]&gt; delete from nova.services where host="compute5";Query OK, 1 row affected (0.01 sec)MariaDB [nova]&gt; delete from compute_nodes where hypervisor_hostname="compute5";Query OK, 0 rows affected (0.00 sec)MariaDB [nova]&gt; select host from nova.services;MariaDB [nova]&gt; select hypervisor_hostname from compute_nodes; 4.校验 12[root@controller ~]# openstack host list[root@controller ~]# nova service-list 再次查看计算节点，就发现compute已经被删除了。]]></content>
      <categories>
        <category>Openstack</category>
      </categories>
      <tags>
        <tag>openstack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Openstack创建实例报错：No valid host was found.]]></title>
    <url>%2F2020%2F04%2F14%2FOpenstackNoValidHost%2F</url>
    <content type="text"><![CDATA[错误原因：UUID冲突，nova_api数据库中resource_providers表与nova.compute_nodes表中host的UUID不一致引起的，导致原因可能为删除计算节点时没有清空resource_providers表中的数据。 解决办法：12345678[root@controller]# nova-manage cell_v2 list_hosts+-----------+--------------------------------------+----------+| Cell Name | Cell UUID | Hostname |+-----------+--------------------------------------+----------+| cell1 | 9abb6c5c-25e2-440c-8295-4826d055298c | compute4 || cell1 | 9abb6c5c-25e2-440c-8295-4826d055298c | compute5 || cell1 | 9abb6c5c-25e2-440c-8295-4826d055298c | compute6 |+-----------+--------------------------------------+----------+ 1234567891011121314151617181920MariaDB [(none)]&gt; use nova_api;Reading table information for completion of table and column namesYou can turn off this feature to get a quicker startup with -ADatabase changedMariaDB [nova_api]&gt; select uuid,name from resource_providers where name='compute5';+--------------------------------------+----------+| uuid | name |+--------------------------------------+----------+| 305cc2a4-75e8-496f-b843-152400257c35 | compute5 |+--------------------------------------+----------+1 row in set (0.00 sec)MariaDB [nova_api]&gt; select uuid,host from nova.compute_nodes where host='compute5';+--------------------------------------+----------+| uuid | host |+--------------------------------------+----------+| 924c0d27-1952-4663-8f01-e8cecc67f964 | compute5 |+--------------------------------------+----------+1 row in set (0.00 sec) 可以看到，UUID为compute5的计算节点在resource_providers表中与nova.compute_nodes表中数据不一致，将resource_providers表中数据修改： 123MariaDB [nova_api]&gt; update resource_providers set uuid='924c0d27-1952-4663-8f01-e8cecc67f964' where name='compute5' and uuid='305cc2a4-75e8-496f-b843-152400257c35';Query OK, 1 row affected (0.01 sec)Rows matched: 1 Changed: 1 Warnings: 0 修改后，compute5上实例可以成功创建。]]></content>
      <categories>
        <category>Openstack</category>
      </categories>
      <tags>
        <tag>openstack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[交换机端口开启巨帧功能]]></title>
    <url>%2F2020%2F04%2F14%2FswitchJumbo%2F</url>
    <content type="text"><![CDATA[命令功能：开启/关闭端口巨帧功能。 命令模式：全局配置模式 命令格式：1set jumbo port &lt;portlist&gt;&#123;enable | disable&#125; 命令参数解释： 参数 | 描述 portlist | 端口列表 enable | 开启端口巨帧功能 disable | 关闭端口巨帧功能 使用说明：ZXR10 2950设备可以转发10k大小的巨帧。 缺省：巨帧功能默认为disable状态。]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>tool</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Centos7断电导致generating /run/initramfs/rdsosreport.txt 问题]]></title>
    <url>%2F2020%2F04%2F14%2Frdsosreport%2F</url>
    <content type="text"><![CDATA[开机就进入命令窗口，窗口提示信息如下：1234generating "/run/initramfs/rdsosreport.txt"entering emergencymode. exit the shell to continuetype "journalctl" to view system logs.you might want to save "/run/initramfs/rdsosreport.txt" to a usb stick or /boot after mounting them and attach it to a bug report. 解决办法：1xfs_repair /dev/mapper/centos-root -L 1reboot]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Openstack 安全组规则不生效]]></title>
    <url>%2F2020%2F02%2F14%2FOpenstackSecurityGroup%2F</url>
    <content type="text"><![CDATA[编辑 /etc/sysctl.conf 文件这里强调，安全组主要是依靠计算节点的iptables的forward链来生效的，每加一条规则就会根据网卡作为匹配条件，来生成一条iptables的规则。 如果没有任何规则，默认是丢弃所有的包。 猜测到的原因是因为，没有开启包转发功能，所以修改 12345net.ipv4.ip_forward=1net.ipv4.conf.default.rp_filter=1net.bridge.bridge-nf-call-ip6tables=1net.bridge.bridge-nf-call-iptables=1net.bridge.bridge-nf-call-arptables=1 重启网络 1/etc/init.d/network restart 编辑 /etc/neutron/plugins/ml2/openvswitch_agent.ini 文件增加或修改项，包括控制节点和计算节点 123[securitygroup]enable_security_group = truefirewall_driver = neutron.agent.linux.iptables_firewall.OVSHybridIptablesFirewallDriver 控制节点重启所有网络服务，计算节点重启Openvswitch服务。 注：neutron 也提供两种安全组的实现：IptablesFirewallDriver 和 OVSHybridIptablesFirewallDriverneutron.agent.linux.iptables_firewall.IptablesFirewallDriver iptables-based FirewallDriver implementationneutron.agent.linux.iptables_firewall.OVSHybridIptablesFirewallDriver: subclass of IptablesFirewallDriver with additional bridge默认值是 neutron.agent.firewall.NoopFirewallDriver，表示不使用 neutron security group。]]></content>
      <categories>
        <category>Openstack</category>
      </categories>
      <tags>
        <tag>openstack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JMX开启kafka]]></title>
    <url>%2F2019%2F11%2F21%2Fjmx_kafka%2F</url>
    <content type="text"><![CDATA[开启JMXkafka开启JMX的2种方式：1.启动kafka时增加JMX_PORT=9988，即 1JMX_PORT=9988 bin/kafka-server-start.sh -daemon config/server.properties 2.修改kafka-run-class.sh脚本，第一行增加JMX_PORT=9988即可。 事实上这两种配置方式背后的原理是一样的，我们看一下kafka的启动脚本kafka-server-start.sh的最后一行 1exec $base_dir/kafka-run-class.sh $EXTRA_ARGS kafka.Kafka"$@" 实际上就是调用kafka-run-class.sh脚本，其中有一段这样的内容： 1234# JMX port to useif [ $JMX_PORT ]; then KAFKA_JMX_OPTS="$KAFKA_JMX_OPTS -Dcom.sun.management.jmxremote.port=$JMX_PORT "fi 所以，本质是给参数JMX_PORT赋值，第二种方式在脚本的第一行增加JMX_PORT=9988，$JMX_PORT就能取到值；而第一种方式有点逼格，本质是设置环境变量然后执行启动脚本，类似下面这种方式给JMX_PORT赋值： 12[root@kafka]$ export JMX_PORT=9988[root@kafka]$ bin/kafka-server-start.sh -daemon config/server.properties jmx所有相关参数都在脚本kafka-run-class.sh中，如下所示： 123456789# JMX settingsif [ -z "$KAFKA_JMX_OPTS" ]; then KAFKA_JMX_OPTS="-Dcom.sun.management.jmxremote -Djava.rmi.server.hostname=10.0.55.229 -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false "fi# JMX port to useif [ $JMX_PORT ]; then KAFKA_JMX_OPTS="$KAFKA_JMX_OPTS -Dcom.sun.management.jmxremote.port=$JMX_PORT "fi 某些服务器可能无法正确绑定ip，这时候我们需要显示指定绑定的host：- 1Djava.rmi.server.hostname=192.100.200.46 jconsole连接配置好jmx并启动kafka后，可以启动jconsole验证jmx配置是否正确（连接远程进程的host就是参数java.rmi.server.hostname指定的值，port就是参数JMX_PORT指定的值）： JMX开启远程访问（包括kafka启动）： 1sudo KAFKA_JMX_OPTS="-Dcom.sun.management.jmxremote=true -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Djava.rmi.server.hostname=192.100.200.46 -Djava.net.preferIPv4Stack=true -Dcom.sun.management.jmxremote.port=9988" bin/kafka-server-start.sh config/server.properties]]></content>
      <categories>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kafka集群管理工具kafka-manager的安装使用]]></title>
    <url>%2F2019%2F11%2F21%2Fkafka_manager%2F</url>
    <content type="text"><![CDATA[kafka-manager简介kafka-manager是目前最受欢迎的kafka集群管理工具，最早由雅虎开源，用户可以在Web界面执行一些简单的集群管理操作。具体支持以下内容： 管理多个集群轻松检查集群状态（主题，消费者，偏移，代理，副本分发，分区分发）运行首选副本选举使用选项生成分区分配以选择要使用的代理运行分区重新分配（基于生成的分配）使用可选主题配置创建主题（0.8.1.1具有与0.8.2+不同的配置）删除主题（仅支持0.8.2+并记住在代理配​​置中设置delete.topic.enable = true）主题列表现在指示标记为删除的主题（仅支持0.8.2+）批量生成多个主题的分区分配，并可选择要使用的代理批量运行重新分配多个主题的分区将分区添加到现有主题更新现有主题的配置kafka-manager 项目地址：https://github.com/yahoo/kafka-manager kafka-manager安装下载安装包使用Git或者直接从Releases中下载，这里我们下载 2.0.0.2版本：https://github.com/yahoo/kafka-manager/releases 1wget https://github.com/yahoo/kafka-manager/archive/2.0.0.2.tar.gz 解压安装包 12tar zxvf 2.0.0.2.tar.gzcd kafka-manager-2.0.0.2 sbt编译yum安装sbt(因为kafka-manager需要sbt编译) 123curl https://bintray.com/sbt/rpm/rpm &gt; bintray-sbt-rpm.repomv bintray-sbt-rpm.repo /etc/yum.repos.d/yum install sbt 验证：检查sbt是否安装成功 12sbt[info] [launcher] getting org.scala-sbt sbt 1.3.3 (this may take some time)... 编译kafka-manager 1./sbt clean dist 安装将编译好的/root/kafka-manager-2.0.0.2/target/universal/kafka-manager-2.0.0.zip文件解压 1unzip /root/kafka-manager-2.0.0.2/target/universal/kafka-manager-2.0.0.zip 启动服务启动zk集群，kafka集群，再启动kafka-manager服务。bin/kafka-manager 默认的端口是9000，可通过 -Dhttp.port，指定端口;-Dconfig.file=conf/application.conf指定配置文件: 1nohup bin/kafka-manager -Dconfig.file=conf/application.conf -Dhttp.port=9000 &amp; WebUI查看：http://KAFKA_IP:9000/ 出现如下界面则启动成功。]]></content>
      <categories>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS7安装kafka]]></title>
    <url>%2F2019%2F11%2F19%2Fcentos7_kdfka%2F</url>
    <content type="text"><![CDATA[安装准备：基于Centos7.5 1804 minimal安装JDK1.81yum -y install java-1.8.0-openjdk* 安装kafka下载kafka 1wget http://mirrors.tuna.tsinghua.edu.cn/apache/kafka/2.3.0/kafka_2.12-2.3.0.tgz 解压 1tar -zxvf kafka_2.12-2.3.0.tgz 修改配置，使kafka远程访问 12345vi config/server.properties# listeners=PLAINTEXT://:9092-&gt;listeners=PLAINTEXT://KAFKA_IP:9092 启动kafka进入kafka目录 1cd kafka_2.12-2.3.0 启动zookeeper 1bin/zookeeper-server-start.sh -daemon config/zookeeper.properties 检查zookeeper端口2181是否正常监听 12netstat -an|grep 2181tcp6 0 0 :::2181 :::* LISTEN 检查kafka默认的JVM参数配置是否需要修改Kafka默认设置1G，即”-Xmx1G -Xms1G”。如果你的测试机内存较低，需要修改才能成功启动。参数配置位于：bin/kafka-server-start.sh 启动Kafka 1bin/kafka-server-start.sh config/server.properties 如果一切顺利，就会看到如下启动成功的日志： 1[2019-11-18 21:42:37,067] INFO [KafkaServer id=0] started (kafka.server.KafkaServer) 检查Kafka的端口9092监听是否正常 12[root@localhost kafka_2.12-2.3.0]# netstat -an|grep 9092tcp6 0 0 :::9092 :::* LISTEN 测试kafka创建一个测试主题 1234cd kafka_2.12-2.3.0bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic testCreated topic "test". 查看刚刚创建的test主题 123bin/kafka-topics.sh --list --zookeeper localhost:2181test 向刚刚创建的test主题中写入数据 123bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test&gt;Hello, World!&gt; “Hello World!”为写入的数据。 查看刚刚写入的数据另外开一个ssh tab连接，然后执行如下命令 1234bin/kafka-console-consumer.sh --zookeeper localhost:2181 --topic test --from-beginningUsing the ConsoleConsumer with old consumer is deprecated and will be removed in a future major release. Consider using the new consumer by passing [bootstrap-server] instead of [zookeeper].Hello, World!]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS网卡配置文件设置为网桥模式]]></title>
    <url>%2F2019%2F11%2F08%2FOpenstackProviderBridge%2F</url>
    <content type="text"><![CDATA[原eth0配置文件/etc/sysconfig/network-scripts/ifcfg-eth0 12345678910111213TYPE=EthernetPROXY_METHOD=noneBROWSER_ONLY=noBOOTPROTO=staticDEFROUTE=noIPV4_FAILURE_FATAL=noNAME=eth0UUID=63754cd8-9152-44a4-b051-ea88c0182bd4DEVICE=eth0ONBOOT=yesIPADDR=191.100.200.30NETMASK=255.255.255.0GATEWAY=192.100.10.161 改成网桥配置后,eth0配置文件及br-provider网桥配置文件/etc/sysconfig/network-scripts/ifcfg-eth0 123456789TYPE=OVSPortDEVICETYPE=ovsNAME=eth0UUID=63754cd8-9152-44a4-b051-ea88c0182bd4DEVICE=eth0ONBOOT=yesIPADDR=0.0.0.0NETMASK=255.255.255.0OVS_BRIDGE=br-provider /etc/sysconfig/network-scripts/ifcfg-br-provider 123456789DEVICE=br-providerDEVICETYPE=ovsTYPE=OVSBridgeBOOTPROTO=staticIPADDR=192.100.200.30NETMASK=255.255.255.0GATEWAY=192.100.10.161ONBOOT=yes]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[构建openstack Queens版本本地yum源]]></title>
    <url>%2F2019%2F10%2F24%2FYum_OpenstackQueens%2F</url>
    <content type="text"><![CDATA[选择一台CentOS服务器，安装以下软件：1yum install yum-utils createrepo yum-plugin-priorities httpd 开启httpd服务1systemctl start httpd 获取repo文件并使用reposync同步源1yum install -y https://repos.fedorapeople.org/repos/openstack/openstack-queens/rdo-release-queens-2.noarch.rpm 查看源ID列表 1yum repolist 同步openstack-queens这个repo12cd /var/www/html/reposync --repoid=openstack-queens 第一次同步时间较长，同步结束后，执行1createrepo –update /var/www/html/openstack-queens 创建完成后，就可以使用web测试：1http://[ip]/openstack-queens/ 选择另外一台服务器作为客户机12345678910cd /etc/yum.repos.dmv CentOS-Base.repo CentOS-Base.repo_bakcp CentOS-Media.repo CentOS-Media.repo_bakvim CentOS-Media.repo[openstack-queens]name=OpenStack Queens Repositorybaseurl=http://47.98.122.105/openstack-queens/enabled=1gpgcheck=0 配置完成，清除缓存并查看软件包12yum clean allyum list 问题客户端yum安装报错： 12345Error downloading packages: glusterfs-libs-7.5-1.el7.x86_64: failed to retrieve Packages/g/glusterfs-libs-7.5-1.el7.x86_64.rpm from centos7-glustererror was [Errno 2] Local file does not exist: /etc/yum.repos.d/pdate/Packages/g/glusterfs-libs-7.5-1.el7.x86_64.rpm glusterfs-7.5-1.el7.x86_64: failed to retrieve Packages/g/glusterfs-7.5-1.el7.x86_64.rpm from centos7-glustererror was [Errno 2] Local file does not exist: /etc/yum.repos.d/pdate/Packages/g/glusterfs-7.5-1.el7.x86_64.rpm 问题解决：YUM服务器删除对应的repodata文件夹，使用如下命令 1createrepo -pdo /var/www/html/kdpa/ /var/www/html/kdpa/ 重新生成repo链接，问题解决]]></content>
      <categories>
        <category>Openstack</category>
      </categories>
      <tags>
        <tag>openstack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Openstack 扩大 RabbitMQ Socket Limit 方法]]></title>
    <url>%2F2019%2F10%2F15%2FrabbitmqSocket%2F</url>
    <content type="text"><![CDATA[在RabbitMQ中，Socket descriptors 是 File descriptors 的子集，它们也是一对此消彼长的关系。然而，它们的默认配额并不大，File descriptors 默认值为“1024”，而 Socket descriptors 的默认值也只有“829”，同时，File descriptors 所能打开的最大文件数也受限于操作系统的配额。因此，如果要调整 File descriptors 文件句柄数，就需要同时调整操作系统和RabbitMQ参数。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162[root@controller ~]# rabbitmqctl statusStatus of node rabbit@controller[&#123;pid,3407&#125;,&#123;running_applications, [&#123;rabbit,"RabbitMQ","3.6.16"&#125;, &#123;rabbit_common, "Modules shared by rabbitmq-server and rabbitmq-erlang-client", "3.6.16"&#125;, &#123;xmerl,"XML parser","1.3.14"&#125;, &#123;ranch,"Socket acceptor pool for TCP protocols.","1.3.2"&#125;, &#123;mnesia,"MNESIA CXC 138 12","4.14.3"&#125;, &#123;syntax_tools,"Syntax tools","2.1.1"&#125;, &#123;ssl,"Erlang/OTP SSL application","8.1.3.1"&#125;, &#123;os_mon,"CPO CXC 138 46","2.4.2"&#125;, &#123;public_key,"Public key infrastructure","1.4"&#125;, &#123;crypto,"CRYPTO","3.7.4"&#125;, &#123;asn1,"The Erlang ASN1 compiler version 4.0.4","4.0.4"&#125;, &#123;recon,"Diagnostic tools for production use","2.3.2"&#125;, &#123;compiler,"ERTS CXC 138 10","7.0.4.1"&#125;, &#123;sasl,"SASL CXC 138 11","3.0.3"&#125;, &#123;stdlib,"ERTS CXC 138 10","3.3"&#125;, &#123;kernel,"ERTS CXC 138 10","5.2"&#125;]&#125;,&#123;os,&#123;unix,linux&#125;&#125;,&#123;erlang_version, "Erlang/OTP 19 [erts-8.3.5.3] [source] [64-bit] [smp:96:96] [async-threads:1024] [hipe] [kernel-poll:true]\n"&#125;,&#123;memory, [&#123;connection_readers,20241544&#125;, &#123;connection_writers,1200000&#125;, &#123;connection_channels,4846312&#125;, &#123;connection_other,53785896&#125;, &#123;queue_procs,6683888&#125;, &#123;queue_slave_procs,0&#125;, &#123;plugins,0&#125;, &#123;other_proc,23512680&#125;, &#123;metrics,2466240&#125;, &#123;mgmt_db,0&#125;, &#123;mnesia,847160&#125;, &#123;other_ets,3015688&#125;, &#123;binary,1999169528&#125;, &#123;msg_index,176072&#125;, &#123;code,21467691&#125;, &#123;atom,891849&#125;, &#123;other_system,75976100&#125;, &#123;allocated_unused,483082808&#125;, &#123;reserved_unallocated,0&#125;, &#123;total,766799872&#125;]&#125;,&#123;alarms,[]&#125;,&#123;listeners,[&#123;clustering,25672,"::"&#125;,&#123;amqp,5672,"::"&#125;]&#125;,&#123;vm_memory_calculation_strategy,rss&#125;,&#123;vm_memory_high_watermark,0.4&#125;,&#123;vm_memory_limit,53742133248&#125;,&#123;disk_free_limit,50000000&#125;,&#123;disk_free,996377710592&#125;,&#123;file_descriptors, [&#123;total_limit,878&#125;, &#123;total_used,776&#125;, &#123;sockets_limit,829&#125;, &#123;sockets_used,774&#125;]&#125;,&#123;processes,[&#123;limit,1048576&#125;,&#123;used,9483&#125;]&#125;,&#123;run_queue,0&#125;,&#123;uptime,161034&#125;,&#123;kernel,&#123;net_ticktime,60&#125;&#125;] 修改系统内核参数系统级别 123vim /etc/sysctl.conffs.file-max=655350 1sysctl -p | grep file-max 用户级别 123456vim /etc/security/limits.conf* soft nofile 65535* hard nofile 65535* soft nproc 65535* hard nproc 65535 修改rabbitmq配置如果是以systemd方式管理rabbitmq服务，则需要修改rabbitmq的service文件。 1vim /usr/lib/systemd/system/rabbitmq-server.service 添加如下参数，其值请根据实际情况进行调整： 12[Service]LimitNOFILE=16384 重启rabbitmq即可： 12systemctl daemon-reloadsystemctl restart rabbitmq-server]]></content>
      <categories>
        <category>rabbitmq</category>
      </categories>
      <tags>
        <tag>rabbitmq</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Chrony - 一步搞定NTP时间同步问题]]></title>
    <url>%2F2019%2F09%2F24%2Fchrony%2F</url>
    <content type="text"><![CDATA[什么是chrony？A client/server for the Network Time Protocol, this program keeps your computer’s clock accurate. It was specially designed to support systems with intermittent internet connections, but it also works well in permanently connected environments. It can use also hardware reference clocks, system real-time clock or manual input as time references.chrony是一个ntp协议的实现程序，既可以当做服务端，也可以充当客户端；它专为间歇性互联网连接的系统而设计，当然也能良好应用于持久互联网连接的环境；chrony有三个时间参考：硬件时钟、实时时钟以及手动同步。 chrony的程序环境主配置文件：/etc/chrony.conf客户端程序：/usr/bin/chronyc服务端程序：/usr/sbin/chronyd 为本地服务器配置一个时间服务器环境时间服务器：192.168.1.10允许本网段192.168.1.0/24同步时间。 配置服务端在编辑配置文件之前，我一般习惯于备份一下，以备重新调整。网络时间服务器可以自行查找，网上很多资料，下边配置两个个国内常用的时间服务器： ]# cd /etc/etc]# cp chrony.conf{,.bak}etc]# vim chrony.conf … # iburst为固定格式，记住就可以，没有深究。 server cn.pool.ntp.org iburst server tw.pool.ntp.org iburst ... # 允许指定网络的主机同步时间，不指定就是允许所有，默认不开启。 allow 192.168.1.0/24 ... # 还有一个默认不开启的选项，意思是，即使服务端没有同步到精确的网络时间，也允许向客户端同步不精确的时间。可以视情况而定。 # Serve time even if not synchronized to any NTP server. #local stratum 10~]# systemctl start chronyd.service~]# systemctl enable chronyd.service 客户端同步配置192.168.1.10作为时间服务器 etc]# vim chrony.conf … server 192.168.1.10 iburst …~]# systemctl start chronyd.service~]# systemctl enable chronyd.service 进入chronyc客户端交互式模式：~]# chronyc 查看现有的时间服务器 chronyc&gt; sources 210 Number of sources = 1 MS Name/IP address Stratum Poll Reach LastRx Last sample =============================================================================== ^* 192.168.1.10 10 6 377 53 -4546ns[-5000ns] +/- 234us查看时间服务器状态 chronyc&gt; sourcestats 210 Number of sources = 1 Name/IP Address NP NR Span Frequency Freq Skew Offset Std Dev ============================================================================== 192.168.1.10 10 6 584 +0.001 0.828 +35ns 74us chronyc&gt;exit也可以直接在命令行使用：~]# chronyc sources~]# chronyc sourcestats chrony兼容ntpdate，客户端可以使用ntpdate手动同步时间 12~]# ntpdate 192.168.1.10 9 Nov 02:54:30 ntpdate[39551]: adjust time server 192.168.43.101 offset -0.000003 sec]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[collectd + influxDB + Grafana 搭建监控平台]]></title>
    <url>%2F2019%2F08%2F06%2Fmonitor%2F</url>
    <content type="text"><![CDATA[概述常用配置： influxdb + grafana安装在一台机器负责监控数据收集及展示collectd安装在一台或多台被监控服务端,跟监控端的25826端口对接，上传本地监控的数据influxdb监控25826端口以获得数据，自身处于8086端口，grafana从8086获得数据进行展示 InfluxDB 是 Go 语言开发的一个开源分布式时序数据库，非常适合存储指标、事件、分析等数据 collectd C 语言写的一个系统性能采集工具 Grafana 是纯 Javascript 开发的前端工具，用于访问 InfluxDB，自定义报表、显示图表等 本次安装版本 collectd 5.8.1 influxDB 1.7.7 Grafana 6.2.5 Collectdcollectd是一个守护(daemon)进程，用来收集系统性能和提供各种存储方式来存储不同值的机制。比如以RRD 文件形式，当系统运行和存储信息的时候，Collectd会周期性统计系统的相关统计信息。那些信息可以用来找到当前系统性能瓶颈。（如作为性能分析 performance analysis）和预测系统未来的load（如能力部署capacity planning） 安装1# yum install -y collectd 修改配置123456789101112131415161718192021222324252627# vim /etc/collectd.conf#更改以下内容，前面的#记得删除掉 没有就使用find / -name collectd.conf 查找在哪里 FQDNLookup true Hostname "localhost" #直接使用hostname命令查看 BaseDir "/var/lib/collectd" PIDFile "/var/run/collectd.pid" PluginDir "/usr/lib64/collectd" TypesDB "/usr/share/collectd/types.db" LoadPlugin syslog LoadPlugin rrdtool LoadPlugin disk LoadPlugin interface LoadPlugin load LoadPlugin memory LoadPlugin network LoadPlugin processes LoadPlugin users &lt;Plugin interface&gt; Interface "eth0" IgnoreSelected false &lt;/Plugin&gt; &lt;Plugin network&gt; Server "127.0.0.1" "25826" #这里填写的是influxDB安装的服务器ip &lt;Plugin rrdtool&gt; DataDir "/usr/var/lib/collectd/rrd" #如果你是使用下载安装前面应该会多出一个$&#123;prefix&#125;,未尝试是否有影响 &lt;/Plugin&gt; 安装rrdtool插及依赖包1# yum install collectd-rrdtool rrdtool rrdtool-devel 启动collectd12# systemctl start collectd.service #启动# systemctl enable collectd.service #配置开机自启 确认配置是否成功123# cd /var/lib/collectd# lsrrd 如果/var/lib/collectd目录下生成rrd文件，说明有数据了，如果没有应该是配置问题 Influxdb配置yum源配置yum(此方法为最新版本InfluxDB) 12345678# cat &lt;&lt;EOF | sudo tee /etc/yum.repos.d/influxdb.repo&gt;[influxdb]&gt;name = InfluxDB Repository - RHEL \$releasever&gt;baseurl = https://repos.influxdata.com/rhel/\$releasever/\$basearch/stable&gt;enabled = 1&gt;gpgcheck = 1&gt;gpgkey = https://repos.influxdata.com/influxdb.key&gt;EOF 安装1# yum install -y influxdb 启动InfluxDB12# systemctl start influxdb.service#启动# systemctl enable influxdb.service #配置influxdb开机自启 启动后TCP端口:8083 为InfluxDB 管理控制台 TCP端口:8086 为客户端和InfluxDB通信时的HTTP API启动后InfluxDB用户认证默认是关闭的，先创建用户:geekwolf geekwolf命令行输入influx 配置数据库12345678910111213141516171819202122232425262728293031323334# influx #进入InfluxDBConnected to http://localhost:8086 version 1.7.7InfluxDB shell version: 1.7.7&gt; create database collectdb #创建数据库&gt; show databases #查看数据库name: databases\------namecollectdb&gt; create user matianxin with password &apos;matianxin&apos; #创建一个用户和密码&gt; show users #查看所有用户user adminmatianxin false&gt; grant all on collectdb to matianxin #把上面创建的数据库的所有权限赋给geekwolf用户&gt; help showUsage: connect &lt;host:port&gt; connects to another node specified by host:port auth prompts for username and password pretty toggles pretty print for the json format use &lt;db_name&gt; sets current database format &lt;format&gt; specifies the format of the server responses: json, csv, or column precision &lt;/format&gt;&lt;format&gt; specifies the format of the timestamp: rfc3339, h, m, s, ms, u or ns consistency &lt;level&gt; sets write consistency level: any, one, quorum, or all history displays command history settings outputs the current settings for the shell exit/quit/ctrl+d quits the influx shell show databases show database names show series show series information show measurements show measurement information show tag keys show tag key information show field keys show field key information A full list of influxql commands can be found at: https://docs.influxdata.com/influxdb/v0.10/query_language/spec&gt; quit #退出 启用认证修改配置文件启用认证 12# sed -i 's#auth-enabled = false#auth-enabled = true#g' /etc/influxdb/influxdb.conf# systemctl restart influxdb.service #重启influxdb 配置InfluxDB支持Collectd12345678910# vim /etc/influxdb/influxdb.conf [collectd] enabled = true bind-address = "127.0.0.1:25826" database = "collectdb" typesdb = "/usr/share/collectd/types.db" #查找一下types.db文件不一定在这个路径，如果路径配置错误就不能监听成功 batch-size = 5000 batch-pending = 10 batch-timeout = "10s" read-buffer = 0 1# systemctl restart influxdb.service #重启influxdb 查看25826这个端口是否已经监听，如果有，则代表启动正常 12# netstat -anp| grep 25826udp 0 0 127.0.0.1:25826 0.0.0.0:* 2950/influxd 确认数据123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990[root@localhost ~]# influxConnected to http://localhost:8086 version 1.7.7InfluxDB shell version: 1.7.7&gt; use collectdbUsing database collectdb&gt; show field keysname: cpu_valuefieldKey fieldType-------- ---------value floatname: disk_io_timefieldKey fieldType-------- ---------value floatname: disk_readfieldKey fieldType-------- ---------value floatname: disk_valuefieldKey fieldType-------- ---------value floatname: disk_weighted_io_timefieldKey fieldType-------- ---------value floatname: disk_writefieldKey fieldType-------- ---------value floatname: interface_rxfieldKey fieldType-------- ---------value floatname: interface_txfieldKey fieldType-------- ---------value floatname: load_longtermfieldKey fieldType-------- ---------value floatname: load_midtermfieldKey fieldType-------- ---------value floatname: load_shorttermfieldKey fieldType-------- ---------value floatname: memory_valuefieldKey fieldType-------- ---------value floatname: processes_valuefieldKey fieldType-------- ---------value floatname: users_valuefieldKey fieldType-------- ---------value float&gt; select * from cpu_value limit 10;name: cpu_valuetime host instance type type_instance value---- ---- -------- ---- ------------- -----1565060949576064259 localhost 0 cpu user 26631565060949576080440 localhost 0 cpu system 29521565060949576087737 localhost 0 cpu wait 891565060949576094319 localhost 0 cpu nice 01565060949576100786 localhost 0 cpu interrupt 01565060949576106664 localhost 0 cpu softirq 1601565060949576108568 localhost 0 cpu steal 01565060949576110126 localhost 0 cpu idle 2480671565060959576531164 localhost 0 cpu user 26641565060959576547037 localhost 0 cpu system 2955&gt; Grafana安装12# wget https://dl.grafana.com/oss/release/grafana-6.2.5-1.x86_64.rpm# yum localinstall grafana-6.2.5-1.x86_64.rpm 启动grafana12# systemctl start grafana-server.service #启动# systemctl enable grafana-server.service #配置开机自启 配置Grafana访问地址:http://127.0.0.1:3000 默认账号为admin 密码 admin 配置Data Source Name: InfluxDB -&gt; default URL: http://localhost:8086 Access: Server(Default) Basic Auth: √ Basic Auth Details User: matianxin Password: matianxin InfluxDB Details Database: collectdb …]]></content>
      <categories>
        <category>监控</category>
      </categories>
      <tags>
        <tag>monitor</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Openstack Queens 环境搭建（十）Ceilometer + Gnocchi]]></title>
    <url>%2F2019%2F08%2F02%2FOpenstackQueens10%2F</url>
    <content type="text"><![CDATA[ceilometer概述什么是ceilometerCeilometer是OpenStack中的一个子项目，它像一个漏斗一样，能把OpenStack内部发生的几乎所有的事件都收集起来，然后为计费和监控以及其它服务提供数据支撑。 Ceilometer服务主要功能： 有效地轮询与OpenStack服务相关的计量数据。 通过监视从服务发送的通知来收集事件和计量数据。 将收集的数据发布到各种目标，包括数据存储和消息队列。 ceilometer如何进行监控Ceilometer监控通过在计算节点部署Compute服务，轮询其计算节点上的instance，获取各自的CPU、网络、磁盘等监控信息，发送到RabbitMQ，Collector服务负责接收信息进行持久化存储，也可通过libvirt和openstack服务的API采集数据。 监控信息存储Ceilometer以前提供了存储和API解决方案。截至N版本，此功能已被正式弃用并且不鼓励。为了有效存储和统计分析ceilometer数据，建议使用Gnocchi。采集信息的存储可以有多种，如文件，数据库等，openstack queens版本默认采用gnocchi+文件形式存储。 gnocchi概述gnocchi是什么Gnocchi是一个多租户时间序列，计量和资源数据库。提供了HTTP REST接口来创建和操作数据。Gnocchi设计用于超大规模计量数据的存储，同时向操作者和用户提供对度量和资源信息的访问。Gnocchi是OpenStack项目的一部分。因此它支持OpenStack，但也能完全独立的工作。 gnocchi项目起源早期的openstack各类资源的计量数据(measurement) 存储在SQL数据库中的sample表中。随着云环境中需要被监控的资源增多和时间的推移，计量数据的增长变得难以预测；计量数据的使用方面，查询操作首先要从巨大的sample单表中过滤所需条目，然后还会涉及到相关的聚合计算；可想而知，由此带来的性能开销绝对是无法忍受的，并且随着时间的推移这个瓶颈会愈加明显直至奔溃。要解决上述问题方法有很多，比如分表：每个监控指标（Metirc）一张表，那么一个资源可能会有多张表（比如一个instance至少会有cpu，cpu.util，memory，memory.usage，disk.* 等监控指标metrics）；这似乎有点夸张，即使这样都可以接受的话，那么查询时对计量数据的聚合操作也还是个问题。为解决以上问题，红帽的Julien Danjou，发起了Gnocchi项目来解决这类问题。其总体思路是：把各个计量指标Metric的计量数据measurement直接写入后端存储中；并在measurement写入之前根据预先设定的归档策略进行聚合操作；查询时直接读取对应的文件即可获得聚合后的监控信息点；gnocchi提供资源索引，这样能更快的找到每个资源的基础信息metadata和其相关的metrics信息。注：数据存储分级：(资源项)resource-&gt;(资源指标条目)metric-&gt;(实际数据)measure 为什么使用gnocchiGnocchi已能够具备在云计算环境中提供可用的时间序列数据库的需要，提供存储大量度量数据并且易于扩展的能力。Gnocchi项目于2014年开始，作为OpenStack Ceilometer项目的分支，以解决Ceilometer在将标准数据库用作计量数据的存储后端时遇到的性能问题。Gnocchi使用各种技术压缩存储数据，降低了数据存储的空间。 ceilometer+gnocchi环境搭建安装部署ceilometer服务前，主机必须已经正确安装keystone、nova、neutron、image服务。部署过程须用到uwsgi和redis。 Compute节点安装安装ceilometer相关包：12# yum install openstack-ceilometer-compute# yum install openstack-ceilometer-ipmi (optional) 编辑/etc/ceilometer/ceilometer.conf文件并完成以下操作 在该[DEFAULT]部分中，配置RabbitMQ 消息队列访问 123[DEFAULT]...transport_url = rabbit://openstack:123456@controller 在该[service_credentials]部分中，配置服务凭据： 12345678910[service_credentials]auth_url = http://controller:5000project_domain_id = defaultuser_domain_id = defaultauth_type = passwordusername = ceilometerproject_name = servicepassword = 123456interface = internalURLregion_name = RegionOne 配置计算服务，编辑/etc/nova/nova.conf文件并在以下[DEFAULT]部分配置通知： 12345678[DEFAULT]...instance_usage_audit = Trueinstance_usage_audit_period = hournotify_on_state_change = vm_and_task_state[oslo_messaging_notifications]...driver = messagingv2 配置计算以轮询IPMI，编辑/etc/sudoers文件并添加包含： 1ceilometer ALL = (root) NOPASSWD: /usr/bin/ceilometer-rootwrap /etc/ceilometer/rootwrap.conf * 编辑/etc/ceilometer/polling.yaml以包含所需： 1234- name: ipmi interval: 300 meters: - hardware.ipmi.temperature 完成安装 启动代理并将其配置为在系统引导时启动： 1234# systemctl enable openstack-ceilometer-compute.service# systemctl start openstack-ceilometer-compute.service# systemctl enable openstack-ceilometer-ipmi.service (optional)# systemctl start openstack-ceilometer-ipmi.service (optional) 重新启动Compute服务： 1# systemctl restart openstack-nova-compute.service Controller节点安装获取admin凭据来访问仅管理员CLI命令1$ . admin-openrc 要创建服务凭据，请完成以下步骤： 创建ceilometer用户： 1234567891011openstack user create --domain default --password-prompt ceilometerUser Password:123456Repeat User Password:123456+-----------+----------------------------------+| Field | Value |+-----------+----------------------------------+| domain_id | e0353a670a9e496da891347c589539e9 || enabled | True || id | c859c96f57bd4989a8ea1a0b1d8ff7cd || name | ceilometer |+-----------+----------------------------------+ 将admin角色添加到ceilometer用户。 1$ openstack role add --project service --user ceilometer admin 在Keystone注册Gnocchi服务 创建gnocchi用户： 1234567891011$ openstack user create --domain default --password-prompt gnocchiUser Password:123456Repeat User Password:123456+-----------+----------------------------------+| Field | Value |+-----------+----------------------------------+| domain_id | e0353a670a9e496da891347c589539e9 || enabled | True || id | 8bacd064f6434ef2b6bbfbedb79b0318 || name | gnocchi |+-----------+----------------------------------+ 创建gnocchi服务实体 1234567891011$ openstack service create --name gnocchi \ --description "Metric Service" metric+-------------+----------------------------------+| Field | Value |+-------------+----------------------------------+| description | Metric Service || enabled | True || id | 205978b411674e5a9990428f81d69384 || name | gnocchi || type | metric |+-------------+----------------------------------+ 将admin角色添加到gnocchi用户 1$ openstack role add --project service --user gnocchi admin 创建Ceilometer服务API端点 123456789101112131415$ openstack endpoint create --region RegionOne \ metric public http://controller:8041+--------------+----------------------------------+| Field | Value |+--------------+----------------------------------+| enabled | True || id | b808b67b848d443e9eaaa5e5d796970c || interface | public || region | RegionOne || region_id | RegionOne || service_id | 205978b411674e5a9990428f81d69384 || service_name | gnocchi || service_type | metric || url | http://controller:8041 |+--------------+----------------------------------+ 123456789101112131415$ openstack endpoint create --region RegionOne \ metric internal http://controller:8041+--------------+----------------------------------+| Field | Value |+--------------+----------------------------------+| enabled | True || id | c7009b1c2ee54b71b771fa3d0ae4f948 || interface | internal || region | RegionOne || region_id | RegionOne || service_id | 205978b411674e5a9990428f81d69384 || service_name | gnocchi || service_type | metric || url | http://controller:8041 |+--------------+----------------------------------+ 123456789101112131415$ openstack endpoint create --region RegionOne \ metric admin http://controller:8041+--------------+----------------------------------+| Field | Value |+--------------+----------------------------------+| enabled | True || id | b2c00566d0604551b5fe1540c699db3d || interface | admin || region | RegionOne || region_id | RegionOne || service_id | 205978b411674e5a9990428f81d69384 || service_name | gnocchi || service_type | metric || url | http://controller:8041 |+--------------+----------------------------------+ 安装Gnocchi 安装服务组件 1# yum install openstack-gnocchi-api openstack-gnocchi-metricd python-gnocchiclient 新建文件 /etc/httpd/conf.d/10-gnocchi_wsgi.conf 123456789101112131415161718192021Listen 8041&lt;VirtualHost *:8041&gt; ServerName controller ## Vhost docroot DocumentRoot "/var/www/cgi-bin/gnocchi" ## Directories, there should at least be a declaration for /var/www/cgi-bin/gnocchi &lt;Directory "/var/www/cgi-bin/gnocchi"&gt; Options Indexes FollowSymLinks MultiViews AllowOverride None Require all granted &lt;/Directory&gt; ## Logging ErrorLog "/var/log/httpd/gnocchi_wsgi_error.log" ServerSignature Off CustomLog "/var/log/httpd/gnocchi_wsgi_access.log" combined SetEnvIf X-Forwarded-Proto https HTTPS=1 WSGIApplicationGroup %&#123;GLOBAL&#125; WSGIDaemonProcess gnocchi display-name=gnocchi_wsgi group=gnocchi processes=8 threads=8 user=gnocchi WSGIProcessGroup gnocchi WSGIScriptAlias / "/var/www/cgi-bin/gnocchi/app"&lt;/VirtualHost&gt; 创建文件夹路径下app文件 12mkdir /var/www/cgi-bin/gnocchi/vim /var/www/cgi-bin/gnocchi/app app文件如下： 123456789101112131415161718192021#!/usr/bin/python# Licensed under the Apache License, Version 2.0 (the "License");# you may not use this file except in compliance with the License.# You may obtain a copy of the License at## http://www.apache.org/licenses/LICENSE-2.0## Unless required by applicable law or agreed to in writing, software# distributed under the License is distributed on an "AS IS" BASIS,# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or# implied.# See the License for the specific language governing permissions and# limitations under the License.if __name__ == '__main__': import sys from gnocchi.cli import api sys.exit(api.api())else: from gnocchi.cli import api from gnocchi.rest import app application = app.load_app(api.prepare_service()) 文件夹修改权限 12# chown -R gnocchi.gnocchi /var/www/cgi-bin/gnocchi# chmod +777 /var/www/cgi-bin/gnocchi 重启httpd服务 1# systemctl restart httpd 为Gnocchi的索引器创建数据库 使用数据库访问客户端以root用户身份连接到数据库服务器 1234$ mysql -u root -pCREATE DATABASE gnocchi;GRANT ALL PRIVILEGES ON gnocchi.* TO &apos;gnocchi&apos;@&apos;localhost&apos; IDENTIFIED BY &apos;123456&apos;;GRANT ALL PRIVILEGES ON gnocchi.* TO &apos;gnocchi&apos;@&apos;%&apos; IDENTIFIED BY &apos;123456&apos;; 编辑/etc/gnocchi/gnocchi.conf文件并添加Keystone选项 在[api]中，配置gnocchi以使用keystone： 12[api]auth_mode = keystone 在[keystone_authtoken]部分中，配置keystone身份验证： 12345678910111213[keystone_authtoken]...auth_type = passwordauth_url = http://controller:5000/v3project_domain_name = Defaultuser_domain_name = Defaultproject_name = serviceusername = gnocchipassword = 123456interface = internalURLregion_name = RegionOne[indexer]url = mysql+pymysql://gnocchi:GNOCCHI_DBPASS@controller/gnocchi 在[storage]部分中，配置位置以存储度量标准数据。在这种情况下，我们将它存储到本地文件系统。有关更耐用和高性能驱动程序的列表，请参阅Gnocchi文档： 123456[storage]# coordination_url is not required but specifying one will improve# performance with better workload division across workers.coordination_url = redis://controller:6379file_basepath = /var/lib/gnocchidriver = file 初始化Gnocchi： 1gnocchi-upgrade 完成Gnocchi安装启动Gnocchi服务并将其配置为在系统引导时启动 12# systemctl enable openstack-gnocchi-api.service openstack-gnocchi-metricd.service# systemctl start openstack-gnocchi-api.service openstack-gnocchi-metricd.service 安装Ceilometer包1# yum install openstack-ceilometer-notification openstack-ceilometer-central 编辑/etc/ceilometer/pipeline.yaml文件并完成以下部分，配置Gnocchi连接： 12345publishers: # set address of Gnocchi # + filter out Gnocchi-related activity meters (Swift driver) # + set default archive policy - gnocchi://?filter_project=service&amp;archive_policy=low 编辑/etc/ceilometer/ceilometer.conf文件并完成以下操作 在该[DEFAULT]部分中，配置RabbitMQ消息队列访问： 123[DEFAULT]...transport_url = rabbit://openstack:123456@controller 在该[service_credentials]部分中，配置服务凭据： 1234567891011[service_credentials]...auth_type = passwordauth_url = http://controller:5000/v3project_domain_id = defaultuser_domain_id = defaultproject_name = serviceusername = ceilometerpassword = 123456interface = internalURLregion_name = RegionOne 在Gnocchi创建Ceilometer资源。Gnocchi应该在这个阶段运行： 1# ceilometer-upgrade 完成安装启动Ceilometer服务并将其配置为在系统引导时启动： 1234# systemctl enable openstack-ceilometer-notification.service \ openstack-ceilometer-central.service# systemctl start openstack-ceilometer-notification.service \ openstack-ceilometer-central.service glance neutron服务配置Image 编辑/etc/glance/glance-api.conf文件并完成以下操作：在[DEFAULT]，[oslo_messaging_notifications]部分中，配置通知和RabbitMQ消息代理访问： 1234[DEFAULT]transport_url = rabbit://openstack:123456@controller[oslo_messaging_notifications]driver = messagingv2 编辑/etc/glance/glance-registry.conf文件并完成以下操作：在[DEFAULT]，[oslo_messaging_notifications]部分中，配置通知和RabbitMQ消息代理访问： 1234[DEFAULT]transport_url = rabbit://openstack:123456@controller[oslo_messaging_notifications]driver = messagingv2 完成安装重新启动Image服务： 1# systemctl restart openstack-glance-api.service openstack-glance-registry.service Neutron 配置网络服务以使用Ceilometer 编辑/etc/neutron/neutron.conf并完成以下操作：在这些[oslo_messaging_notifications]部分中，启用通知：123[oslo_messaging_notifications]...driver = messagingv2 完成安装 重启网络服务：1# systemctl restart neutron-server.service 配置环境变量及gnocchi文件夹权限更改目录权限，否则gnocchi存储文件数据时报错，没有权限 1# chmod +777 /var/lib/gnocchi /root/admin-openrc添加gnocchi相关环境变量 1# export OS_AUTH_TYPE=password 监控项列表注意：使用ceilometer监控虚拟机的memory.usage，要求libvirt版本1.1.1+，qemu版本1.5+。并且需要镜像支持balloon，即镜像中安装有balloon的driver。（一般linux镜像都默认包含，但是windows镜像需要另行安装）虚拟机的Cpu：使用时间（cpu）,使用率（cpu_util），分配vcpu个数（vcpus）虚拟机的硬盘：分配总大小（disk.root.size），占用宿主机大小（disk.usage）虚拟机的内存：总大小（memory），使用大小（memory.usage）虚拟机的网络：网络流量（bandwidth），网络下ip个数（ip.floating）虚拟机的镜像：镜像大小（image.download）节点的cpu、硬盘、内存等使用信息可通过linux常用命令监视 nova实例监控项 Name Type Unit Resource Origin Support Note 翻译 memory Gauge MB instance ID Notification Libvirt, Hyper-V Volume of RAM allocated to the instance 分配给实例的RAM量 memory.usage Gauge MB instance ID Pollster Libvirt, Hyper-V, vSphere, XenAPI Volume of RAM used by the instance from the amount of its allocated memory 实例从其分配的内存量中使用的RAM量 … 参考：https://docs.openstack.org/ceilometer/latest/admin/telemetry-measurements.html 详细监控方法介绍采集信息查询方法此方案中采集信息以gnocchi+文件方式存储。获取采集信息有两种方式，分别为 gnocchi-api和gnocchi 命令。以下采用gnocchi命令做详细介绍：注：数据存储分级：(资源项)resource-&gt;(资源指标条目)metric-&gt;(实际数据)measure 获取权限 1# . admin-openrc 获取监控资源列表 1# gnocchi resource list 获取实例监控资源对象信息列表获取某一instance下所有监控条目 12345678910111213141516171819202122232425[root@controller ~]# gnocchi resource show 8bb00e90-7e3f-5ef6-bee5-00e7c6bbc5fc+-----------------------+-------------------------------------------------------------------+| Field | Value |+-----------------------+-------------------------------------------------------------------+| created_by_project_id | e06fb4ce67a24aa687dffb9d8dcc6b1e || created_by_user_id | 4895e340408e42c383ea8d82e459b182 || creator | 4895e340408e42c383ea8d82e459b182:e06fb4ce67a24aa687dffb9d8dcc6b1e || ended_at | None || id | 8bb00e90-7e3f-5ef6-bee5-00e7c6bbc5fc || metrics | disk.device.allocation: 427a316e-1ef9-4054-8253-604b80e8f003 || | disk.device.capacity: 6915cd9b-1346-46db-8e86-438fd1884b49 || | disk.device.latency: fcfbc2ae-eddf-4ff9-bda9-33b9a2ba9c3d || | disk.device.read.bytes: 6df3416c-ad02-401e-84e4-5ac0b75febf2 || | disk.device.read.latency: 4014d3c6-51ea-4e24-b8e6-649ed6ee896d || | disk.device.usage: 7cd08e21-a0e1-4fce-8dc8-0e8b96d711d4 || | disk.device.write.bytes: b29abf7c-a9c4-4afc-89a5-ef766739028b || | disk.device.write.latency: f82d69ec-98d6-45e8-bc13-cd040f8cc2ef || original_resource_id | 86d618b5-aadc-4346-b620-7f44fc35c7ad-vda || project_id | 40026f6973464ee9a19ad04f6221e213 || revision_end | None || revision_start | 2019-05-07T05:08:10.450142+00:00 || started_at | 2019-05-07T05:08:10.450123+00:00 || type | instance_disk || user_id | 5a4da861d8e84cbdabd98b9804bc1547 |+-----------------------+-------------------------------------------------------------------+ 获取实例监控条目详细信息 1# gnocchi measures show 897356e6-d0bb-43cd-9cbd-43ab2808384d]]></content>
      <categories>
        <category>Openstack</category>
      </categories>
      <tags>
        <tag>openstack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Openstack Queens 环境搭建（九）Cinder服务]]></title>
    <url>%2F2019%2F08%2F01%2FOpenstackQueens9%2F</url>
    <content type="text"><![CDATA[存储服务概念OpenStack块存储服务(Cinder)将持久性存储添加到一个虚拟机。块存储提供了管理卷的基础设施，并与OpenStack计算交互，为实例提供卷。该服务还支持卷快照和卷类型的管理。块存储服务由以下组件组成:cinder-api接收API请求，并将其路由到cinders -volume以进行操作。cinder-volume直接与块存储服务和进程(如cinders -scheduler)交互。它还可以通过消息队列与这些进程进行交互。cinders -volume服务响应发送到块存储服务的读写请求，以维护状态。它可以通过驱动程序体系结构与各种存储提供者交互。cinder-scheduler daemon选择要在其上创建卷的最佳存储提供程序节点。与nova-scheduler类似的组件。cinder-backup daemonCinder-backup服务向备份存储提供程序提供任何类型的备份卷。与cinders -volume服务一样，它可以通过驱动程序体系结构与各种存储提供者交互。Messaging queue在块存储进程之间路由信息。 Controller节点：基础配置在安装和配置块存储服务之前，必须创建数据库、服务凭据和API端点。 创建数据库，为cinder数据库授权1234# mysql -uroot -p123456MariaDB [(none)]&gt; create database cinder;MariaDB [(none)]&gt; grant all privileges on cinder.* to 'cinder'@'localhost' identified by '123456';MariaDB [(none)]&gt; grant all privileges on cinder.* to 'cinder'@'%' identified by '123456'; 创建服务凭据生成管理凭证，以获得访问只有管理CLI命令: 1# . admin-openrc 创建cinder用户： 1# openstack user create --domain default --password-prompt cinder 添加admin角色到cinder用户中： （无返回值） 1# openstack role add --project service --user cinder admin 创建cinder2和cinderv3服务实体:注：块存储服务需要两个服务实体。 1234# openstack service create --name cinderv2 \ --description "OpenStack Block Storage" volumev2# openstack service create --name cinderv3 \ --description "OpenStack Block Storage" volumev3 123456# openstack endpoint create --region RegionOne \ volumev3 public http://controller:8776/v3/%\(project_id\)s# openstack endpoint create --region RegionOne \ volumev3 internal http://controller:8776/v3/%\(project_id\)s# openstack endpoint create --region RegionOne \ volumev3 admin http://controller:8776/v3/%\(project_id\)s 安装配置组件1# yum install -y openstack-cinder 编辑 /etc/cinder/cinder.conf文件1# vi /etc/cinder/cinder.conf 在[database]选项，配置数据库访问:12[database]connection = mysql+pymysql://cinder:123456@controller/cinder 在[DEFAULT]部分，配置RabbitMQ消息队列访问: 12[DEFAULT]transport_url = rabbit://openstack:123456@controller 在[DEFAULT]和[keystone_authtoken]选项，配置身份服务访问: 123456789101112[DEFAULT]auth_strategy = keystone[keystone_authtoken]auth_uri = http://controller:5000auth_url = http://controller:5000memcached_servers = controller:11211auth_type = passwordproject_domain_id = defaultuser_domain_id = defaultproject_name = serviceusername = cinderpassword = 123456 在[DEFAULT]选项，配置my_ip选项，使用Controller节点的管理接口IP地址: 12[DEFAULT]my_ip = 192.100.200.68 在[oslo_concurrency]选项，配置lock路径: 12[oslo_concurrency]lock_path = /var/lib/cinder/tmp 同步块存储数据（忽略此输出中的任何弃用消息。） 1# su -s /bin//sh -c "cinder-manage db sync" cinder 配置计算服务以使用块存储编辑 /etc/nova/nova.conf 文件 123# vi /etc/nova/nova.conf[cinder]os_regioon_name = RegionOne 启动服务123# systemctl restart openstack-nova-api.service# systemctl enable openstack-cinder-api.service openstack-cinder-scheduler.service# systemctl start openstack-cinder-api.service openstack-cinder-scheduler.service 存储节点：安装LVM包:1# yum install -y lvm2 启动LVM元数据服务，并将其配置为在系统启动时启动:注：一些发行版默认安装LVM。 12# systemctl enable lvm2-lvmetad.service# systemctl start lvm2-lvmetad.service 创建LVM物理卷/dev/sdb:12# pvcreate /dev/sdb Physical volume "/dev/sdb" successfully created. 创建LVM卷组cinder-volmes:注：块存储服务在此卷组中创建逻辑卷。 12# vgcreate cinder-volumes /dev/sdb Volume group "cinder-volumes" successfully created 只有实例可以访问块存储卷。然而，底层操作系统管理与卷相关联的设备。默认情况下，LVM卷扫描工具扫描/dev目录中包含卷的块存储设备。如果项目在其卷上使用LVM，那么扫描工具将检测这些卷并试图缓存它们，这会导致底层操作系统和项目卷出现各种问题。必须重新配置LVM，以便只扫描包含cinder-volume卷组的设备。 编辑 /etc/lvm/lvm.conf文件在devices选项，添加一个接受/dev/sdb设备并拒绝所有其他设备的过滤器:注：filter数组中的每个项都以for accept或r for reject开头，并包含一个用于设备名称的正则表达式。数组必须以r/结束。*/ 拒绝任何剩余设备。您可以使用vgs -vvvv命令来测试过滤器。 1234devices &#123;...filter = [ "a/sdb/", "r/.*/"]...&#125; 安装配置组件1# yum install -y openstack-cinder targetcli python-keystone 编辑/etc/cinder/cinder.conf 文件1# vi /etc/cinder/cinder.conf 在[database]选项，配置数据库访问 12[database]connection = mysql+pymysql://cinder:123456@controller/cinder 在[DEFAULT]选项，配置RabbitMQ消息队列访问: 12[DEFAULT]transport_url = rabbit://openstack:123456@controller 在[DEFAULT]和[keystone_authtoken]选项，配置身份服务访问: 123456789101112[DEFAULT]auth_strategy = keystone[keystone_authtoken]auth_uri = http://controller:5000auth_url = http://controller:5000memcached_servers = controller:11211auth_type = passwordproject_domain_id = defaultuser_domain_id = defaultproject_name = serviceusername = cinderpassword = 123456 在[DEFAULT]部分，配置my_ip选项: 1my_ip = 192.168.100.69 在[DEFAULT]选项，配置映像服务API的位置: 1glance_api_servers = http://controller:9292 在[lvm]选项，使用lvm驱动程序、Cinder卷组、iSCSI协议和iSCSI服务配置lvm后端。如果[lvm]部分不存在，则添加它: 12345[lvm]volume_driver = cinder.volume.drivers.lvm.LVMVolumeDrivervolume_group = cinder-volumesiscsi_protocol = iscsiiscsi_helper = lioadm 在[DEFAULT]部分，启用LVM后端: 12[DEFAULT]enabled_backends = lvm 在[oslo_concurrency]节中，配置lock路径 12[oslo_concurrency]lock_path = /var/lib/cinder/tmp 开启服务启动块存储卷服务，包括它的依赖项，并配置它们在系统启动时启动: 12# systemctl enable openstack-cinder-volume.service target.service# systemctl restart openstack-cinder-volume.service target.service 校验操作生成临时环境变量 1# . admin-openrc 列出服务组件以验证每个流程的成功启动: 1# openstack volume service list 创建一个1G的卷，并查看其状态 12# cinder create --display-name myVolume 1# cinder list]]></content>
      <categories>
        <category>Openstack</category>
      </categories>
      <tags>
        <tag>openstack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Openstack Queens 环境搭建（八）计算节点实例首次孵化优化]]></title>
    <url>%2F2019%2F07%2F29%2FOpenstackQueens8%2F</url>
    <content type="text"><![CDATA[Controller节点：创建镜像缓存目录，并调整权限123# mkdir -p /var/lib/glance/imagecache# chmod -R 777 /var/lib/glance/images# chmod -R 777 /var/lib/glance/imagecache 配置共享文件夹，将与控制节点管理网卡同网段的CIDR配置入文件，并赋读写权限123# vim /etc/exports/var/lib/glance/imagecache 192.100.10.0/24(rw) 加载刚才的配置文件1# exportfs -e 注意控制节点的iptables，要放开111（rpc）、2049(nfs)和892(nfs挂载)端口，一般在OpenStack中这几个端口都是默认放开的。设置nfs服务开机启动，并启动nfs1234# systemctl enable rpcbind# systemctl enable nfs# systemctl restart rpcbind# systemctl restart nfs 确认本地共享目录是否正确（showmount -e）计算节点计算节点配置nfs挂载以及系统开机自动挂载有脚本实现在计算节点创建文件夹，并将项目源码中的脚本拷贝上去1# mkdir -p /var/www/kdpa/bin 将项目源码中/kdpa/bin/目录下的sed_rclocal.sh、mount_nfs.sh脚本拷贝到计算节点新创建的目录下1234567891011121314151617sed_rclocal.sh#!/usr/bin/env bashrc_local_file="/etc/rc.d/rc.local"chmod 755 $&#123;rc_local_file&#125;# add mount nfs shell in rc.localmount_nfs_sh="/var/www/kdpa/bin/mount_nfs.sh"chmod 755 $&#123;mount_nfs_sh&#125;echo "/usr/bin/sh $&#123;mount_nfs_sh&#125;"&gt;&gt;$&#123;rc_local_file&#125;# add ovs set-manager shell in rc.localovs_set_manager_sh="/var/www/kdpa/bin/ovs_set_manager.sh"chmod 755 $&#123;ovs_set_manager_sh&#125;echo "/usr/bin/sh $&#123;ovs_set_manager_sh&#125;"&gt;&gt;$&#123;rc_local_file&#125; 123456789101112mount_nfs.sh#!/usr/bin/env bashnfs_service_image_cache_path="/var/lib/glance/imagecache"local_image_cache_path="/var/lib/nova/instances/_base"controller_host=$(cat /etc/hosts | grep controller | awk '&#123;print $1&#125;')mkdir -p $&#123;local_image_cache_path&#125;chmod -R 777 $&#123;local_image_cache_path&#125;mount -t nfs4 $&#123;controller_host&#125;:$&#123;nfs_service_image_cache_path&#125; $&#123;local_image_cache_path&#125; 执行脚本mount_nfs.sh挂载控制节点nfs共享目录1# sh /var/www/kdpa/bin/mount_nfs.sh 验证挂载是否成功1# mount -l | grep 服务端IP 执行脚本sed_rclocal.sh调整系统开机启动脚本1# sh /var/www/kdpa/bin/sed_rclocal.sh 最终查看/etc/rc.d/rc.local文件是否添加成功]]></content>
      <categories>
        <category>Openstack</category>
      </categories>
      <tags>
        <tag>openstack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SQLAlchemy 中的 Engine 是什么]]></title>
    <url>%2F2019%2F07%2F17%2FSQLAlchemyEngine%2F</url>
    <content type="text"><![CDATA[连接池很重要，因为每次发送sql查询的时候都需要先建立连接，如果程序启动的时候事先就初始化一批连接放在连接池，每次用完后又放回连接池给其它请求使用，就能大大提高查询的效率。 Engine 初始化Engine 的初始化非常简单，通过工厂函数 create_engine 就可以创建。 123from sqlalchemy import create_engineengine = create_engine('mysql://user:password@localhost:3306/test?charset=utf8') 构建好 Engine 对象的同时，连接池和Dialect也创建好了，但是这时候并不会立马与数据库建立真正的连接，只有你调用 Engine.connect() 或者 Engine.execute(sql) 执行SQL请求的时候，才会建立真正的连接。因此 Engine 和 Pool 的行为称之为延迟初始化，等真正要派上用场的时候才去建立连接。 需要注意的是，创建引擎时，如果数据库的密码含有特殊字符，需要先编码处理 123&gt;&gt;&gt; import urllib.parse&gt;&gt;&gt; urllib.parse.quote_plus("kx%jj5/g")'kx%25jj5%2Fg' 其它数据库方言初始化 engine 的方式可参考官方文档： https://docs.sqlalchemy.org/en/14/core/engines.html#database-urls create_engine 还有很多可选参数，这里介绍几个重要的参数。 12345engine = create_engine('mysql://user:password@localhost:3306/test?charset=utf8', echo=False pool_size=100, pool_recycle=3600, pool_pre_ping=True) echo ：为 True 时候会把sql语句打印出来，当然，你可以通过配置logger来控制输出，这里不做讨论。 pool_size： 是连接池的大小，默认为5个，0表示连接数无限制 pool_recycle： MySQL 默认情况下如果一个连接8小时内容没有任何动作（查询请求）就会自动断开链接，出现 MySQL has gone away的错误。设置了 pool_recycle 后 SQLAlchemy 就会在指定时间内回收连接。如果设置为3600 就表示 1小时后该连接会被自动回收。 pool_pre_ping ： 这是1.2新增的参数，如果值为True，那么每次从连接池中拿连接的时候，都会向数据库发送一个类似 select 1 的测试查询语句来判断服务器是否正常运行。当该连接出现 disconnect 的情况时，该连接连同pool中的其它连接都会被回收。 参考链接： https://docs.sqlalchemy.org/en/14/core/engines.html#database-urls https://stackoverflow.com/questions/34322471/sqlalchemy-engine-connection-and-session-difference https://docs.sqlalchemy.org/en/13/core/pooling.html#dealing-with-disconnects]]></content>
      <categories>
        <category>SQLAlchemy</category>
      </categories>
      <tags>
        <tag>sqlalchemy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux top 命令详解]]></title>
    <url>%2F2019%2F07%2F12%2Ftop%2F</url>
    <content type="text"><![CDATA[12345678910111213141516171819202122232425$ toptop - 10:14:44 up 15 days, 20:18, 8 users, load average: 6.99, 3.36, 2.62Tasks: 985 total, 2 running, 983 sleeping, 0 stopped, 0 zombie%Cpu(s): 0.7 us, 13.2 sy, 7.2 ni, 78.7 id, 0.1 wa, 0.0 hi, 0.0 si, 0.0 stKiB Mem : 16142800 total, 1201456 free, 14020500 used, 920844 buff/cacheKiB Swap: 8191996 total, 6339272 free, 1852724 used. 1396492 avail Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND22195 root 20 0 4995348 264576 18712 S 7.9 1.6 26:02.20 gnome-shell28393 root 26 6 438932 95064 6416 S 3.3 0.6 16:52.90 python25155 root 20 0 163096 3436 1684 S 1.7 0.0 0:22.93 top 4092 root 20 0 413152 96096 4812 S 1.3 0.6 33:59.19 Xvnc23570 nova 26 6 456196 29632 4524 S 1.3 0.2 131:28.54 nova-conductor23724 nova 26 6 513364 42712 4908 S 1.3 0.3 135:03.59 nova-api12833 neutron 26 6 497996 135052 4608 S 1.0 0.8 59:59.61 neutron-dhcp-ag20537 nobody 20 0 254952 10144 4152 S 1.0 0.1 0:01.45 php-fpm28987 root 20 0 162940 3248 1620 R 1.0 0.0 0:00.22 top 5016 root 20 0 527732 3760 1500 S 0.7 0.0 0:34.49 ibus-daemon 7012 root 20 0 928104 26384 8988 S 0.7 0.2 9:19.03 gnome-terminal- 8040 openvsw+ 26 6 1316280 160352 12192 S 0.7 1.0 170:13.54 ovs-vswitchd 9408 etcd 26 6 11.6g 9896 2052 S 0.7 0.1 176:00.49 etcd23586 glance 26 6 502376 13540 4984 R 0.7 0.1 99:49.88 glance-api 1 root 26 6 201696 5700 2100 S 0.3 0.0 5:16.81 systemd 3 root 20 0 0 0 0 S 0.3 0.0 1:49.34 ksoftirqd/0 统计信息区前五行是系统整体的统计信息。 第一行是任务队列信息，同 uptime。 命令的执行结果。其内容如下： 10:14:44 系统当前时间 up 15 days, 20:18 系统运行时间，格式为时:分 8 users 当前登录用户数 load average: 6.99, 3.36, 2.62 系统负载，即任务队列的平均长度。 三个数值分别为1分钟、5分钟、15分钟前到现在的平均值。 第二、三行为进程和CPU的信息。当有多个CPU时，这些内容可能会超过两行。内容如下： Tasks: 985 total 进程总数 2 running 正在运行的进程数 983 sleeping 睡眠的进程数 0 stopped 停止的进程数 0 zombie 僵尸进程数 Cpu(s): 0.7% us 用户空间占用CPU百分比 13.2% sy 内核空间占用CPU百分比 7.2% ni 用户进程空间内改变过优先级的进程占用CPU百分比 78.7% id 空闲CPU百分比 0.1% wa 等待输入输出的CPU时间百分比 0.0% hi 0.0% si 最后两行为内存信息。内容如下： Mem: 16142800 total 物理内存总量 14020500 used 使用的物理内存总量 1201456 free 空闲内存总量 920844 buff/cache 用作内核缓存的内存量 Swap: 8191996 total 交换区总量 1852724k used 使用的交换区总量 6339272 free 空闲交换区总量 1396492 avail Mem 统计信息区域的下方显示了各个进程的详细信息。首先来认识一下各列的含义。 序号 列名 含义 a PID 进程id b PPID 父进程id c RUSER Real user name d UID 进程所有者的用户id e USER 进程所有者的用户名 f GROUP 进程所有者的组名 g TTY 启动进程的终端名。不是从终端启动的进程则显示为 ? h PR 优先级 i NI nice值。负值表示高优先级，正值表示低优先级 j P 最后使用的CPU，仅在多CPU环境下有意义 k %CPU 上次更新到现在的CPU时间占用百分比 l TIME 进程使用的CPU时间总计，单位秒 m TIME+ 进程使用的CPU时间总计，单位1/100秒 n %MEM 进程使用的物理内存百分比 o VIRT 进程使用的虚拟内存总量，单位kb。VIRT=SWAP+RES p SWAP 进程使用的虚拟内存中，被换出的大小，单位kb。 q RES 进程使用的、未被换出的物理内存大小，单位kb。RES=CODE+DATA r CODE 可执行代码占用的物理内存大小，单位kb s DATA 可执行代码以外的部分(数据段+栈)占用的物理内存大小，单位kb t SHR 共享内存大小，单位kb u nFLT 页面错误次数 v nDRT 最后一次写入到现在，被修改过的页面数。 w S 进程状态。 D=不可中断的睡眠状态 R=运行 S=睡眠 T=跟踪/停止 Z=僵尸进程 x COMMAND 命令名/命令行 y WCHAN 若该进程在睡眠，则显示睡眠中的系统函数名 z Flags 任务标志，参考 默认情况下仅显示比较重要的PID、USER、PR、NI、VIRT、RES、SHR、S、%CPU、%MEM、TIME+、COMMAND列。可以通过下面的快捷键来更改显示内容。更改显示内容通过 f 键可以选择显示的内容。按 f 键之后会显示列的列表，按 a-z 即可显示或隐藏对应的列，最后按回车键确定。按 o 键可以改变列的显示顺序。按小写的 a-z 可以将相应的列向右移动，而大写的 A-Z可以将相应的列向左移动。最后按回车键确定。按大写的 F 或 O 键，然后按 a-z 可以将进程按照相应的列进行排序。而大写的 R 键可以将当前的排序倒转。 命令使用1．工具（命令）名称top 2．工具（命令）作用显示系统当前的进程和其他状况；top是一个动态显示过程,即可以通过用户按键来不断刷新当前状态.如果在前台执行该命令,它将独占前台,直到用户终止该程序为止.比较准确的说,top命令提供了实时的对系统处理器的状态监视.它将显示系统中CPU最“敏感”的任务列表.该命令可以按CPU使用.内存使用和执行时间对任务进行排序；而且该命令的很多特性都可以通过交互式命令或者在个人定制文件中进行设定. 3．环境设置在Linux下使用。 4．使用方法4．1使用格式top [-][d][p][q][c][C][S][s][n] 4．2参数说明 d 指定每两次屏幕信息刷新之间的时间间隔。当然用户可以使用s交互命令来改变之。 p 通过指定监控进程ID来仅仅监控某个进程的状态。 q 该选项将使top没有任何延迟的进行刷新。如果调用程序有超级用户权限，那么top将以尽可能高的优先级运行。 S 指定累计模式 s 使top命令在安全模式中运行。这将去除交互命令所带来的潜在危险。 i 使top不显示任何闲置或者僵死进程。 c 显示整个命令行而不只是显示命令名 4.3其他下面介绍在top命令执行过程中可以使用的一些交互命令。从使用角度来看，熟练的掌握这些命令比掌握选项还重要一些。这些命令都是单字母的，如果在命令行选项中使用了s选项，则可能其中一些命令会被屏蔽掉。 Ctrl+L 擦除并且重写屏幕。 h或者? 显示帮助画面，给出一些简短的命令总结说明。 k 终止一个进程。系统将提示用户输入需要终止的进程PID，以及需要发送给该进程什么样的信号。一般的终止进程可以使用15信号；如果不能正常结束那就使用信号9强制结束该进程。默认值是信号15。在安全模式中此命令被屏蔽。 i 忽略闲置和僵死进程。这是一个开关式命令。 q 退出程序。 r 重新安排一个进程的优先级别。系统提示用户输入需要改变的进程PID以及需要设置的进程优先级值。输入一个正值将使优先级降低，反之则可以使该进程拥有更高的优先权。默认值是10。 S 切换到累计模式。 s 改变两次刷新之间的延迟时间。系统将提示用户输入新的时间，单位为s。如果有小数，就换算成ms。输入0值则系统将不断刷新，默认值是5s。需要注意的是如果设置太小的时间，很可能会引起不断刷新，从而根本来不及看清显示的情况，而且系统负载也会大大增加。 f或者F 从当前显示中添加或者删除项目。 o或者O 改变显示项目的顺序。 l 切换显示平均负载和启动时间信息。 m 切换显示内存信息。 t 切换显示进程和CPU状态信息。 c 切换显示命令名称和完整命令行。 M 根据驻留内存大小进行排序。 P 根据CPU使用百分比大小进行排序。 T 根据时间/累计时间进行排序。 W 将当前设置写入~/.toprc文件中。这是写top配置文件的推荐方法。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Openstack命令行]]></title>
    <url>%2F2019%2F07%2F11%2FOpenstackTerminal%2F</url>
    <content type="text"><![CDATA[Keystone列出所有的用户123456789101112$ openstack user list+----------------------------------+-----------+| ID | Name |+----------------------------------+-----------+| 2ac5fe07105a43518ae444a37640222b | demo || 5bc73e84451b4e71906232bd3849422d | neutron || 84b0b75df69f4413a1380c9efc249b2d | placement || 87bf7fb4671f4524a6fa64ad75856594 | admin || 9aaa5aa26b414d35b37b851ccf749a55 | nova || eee2818e703e47c5a434e5926c90e7fb | glance |+----------------------------------+-----------+ 列出认证服务目录1234567891011121314151617181920212223242526272829303132333435363738394041$ openstack catalog list+-----------+-----------+-----------------------------------------+| Name | Type | Endpoints |+-----------+-----------+-----------------------------------------+| placement | placement | RegionOne || | | internal: http://controller:8778 || | | RegionOne || | | public: http://controller:8778 || | | RegionOne || | | admin: http://controller:8778 || | | || keystone | identity | RegionOne || | | internal: http://controller:5000/v3/ || | | RegionOne || | | public: http://controller:5000/v3/ || | | RegionOne || | | admin: http://controller:5000/v3/ || | | || neutron | network | RegionOne || | | public: http://controller:9696 || | | RegionOne || | | admin: http://controller:9696 || | | RegionOne || | | internal: http://controller:9696 || | | || glance | image | RegionOne || | | admin: http://controller:9292 || | | RegionOne || | | internal: http://controller:9292 || | | RegionOne || | | public: http://controller:9292 || | | || nova | compute | RegionOne || | | internal: http://controller:8774/v2.1 || | | RegionOne || | | admin: http://controller:8774/v2.1 || | | RegionOne || | | public: http://controller:8774/v2.1 || | | |+-----------+-----------+-----------------------------------------+ Glance列出您可以访问的镜像123456789101112131415161718$ openstack image list+--------------------------------------+---------------------+--------+| ID | Name | Status |+--------------------------------------+---------------------+--------+| 70d455f6-1a00-48f5-9b21-5f8fca019014 | centos7-mini | active || 1a7e4d60-3c26-41bb-be0c-34c467b2d3c3 | pstunnel | active || b2464312-8acb-4ff9-ba20-75560e5577f4 | pstunnelA | active || 705c7007-8a28-49e8-8df7-61bbfbf41bbd | pstunnelB | active || ad02946e-9c1f-42f0-9a9e-235ae7012bc3 | router | active || ae8c618d-3c6e-449c-9d2a-3f08bf84cc5e | router22 | active || b54361d4-bc24-46cb-94b2-b1f672b01c2a | sw-f-in-centos7_ext | active || 3a1adf75-c82e-4540-91e0-2167cb921612 | sw-f-in-centos7_int | active || ebfc7c40-4ac3-42ab-96ae-65e3c17af194 | sw-z-in-centos7_ext | active || 7355a66c-3f05-4ed5-9088-540faf71ebf8 | sw-z-in-centos7_int | active || 3b19c8ad-6012-406a-9e7f-e66416deeb1e | testforimage | active || 436c5ab5-14e7-4eb8-95b9-e47d85e60e83 | win7 | active |+--------------------------------------+---------------------+--------+ 查看一个指定的镜像12345678910111213141516171819202122232425$ openstack image show [IMAGE-ID/IMAGE-Name]$ openstack image show 70d455f6-1a00-48f5-9b21-5f8fca019014+------------------+------------------------------------------------------+| Field | Value |+------------------+------------------------------------------------------+| checksum | f3ab346b3ca2b88d1347c24adf0b234b || container_format | bare || created_at | 2019-06-26T09:08:51Z || disk_format | qcow2 || file | /v2/images/70d455f6-1a00-48f5-9b21-5f8fca019014/file || id | 70d455f6-1a00-48f5-9b21-5f8fca019014 || min_disk | 0 || min_ram | 0 || name | centos7-mini || owner | 2e69bc10ab5f427bbbd6d40148d96309 || protected | False || schema | /v2/schemas/image || size | 3758882816 || status | active || tags | || updated_at | 2019-06-26T09:09:32Z || virtual_size | None || visibility | public |+------------------+------------------------------------------------------+ 上传QCOW2镜像123456789101112131415161718192021222324$ openstack image create "centos7-mini2" --file centos7-mini.qcow2 --disk-format qcow2 --container-format bare --public+------------------+------------------------------------------------------+| Field | Value |+------------------+------------------------------------------------------+| checksum | f3ab346b3ca2b88d1347c24adf0b234b || container_format | bare || created_at | 2019-07-11T07:56:51Z || disk_format | qcow2 || file | /v2/images/4072bdfb-f727-4e3f-a1f5-c3467b12a15e/file || id | 4072bdfb-f727-4e3f-a1f5-c3467b12a15e || min_disk | 0 || min_ram | 0 || name | centos7-mini2 || owner | 2e69bc10ab5f427bbbd6d40148d96309 || protected | False || schema | /v2/schemas/image || size | 3758882816 || status | active || tags | || updated_at | 2019-07-11T07:57:54Z || virtual_size | None || visibility | public |+------------------+------------------------------------------------------+ 删除指定的镜像123$ openstack image delete [IMAGE-ID/IMAGE-Name]$ openstack image delete 4072bdfb-f727-4e3f-a1f5-c3467b12a15e Nova列出实例1234567891011121314$ openstack server list+--------------------------------------+-------------------------+---------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------+---------------------+| ID | Name | Status | Networks | Image | Flavor |+--------------------------------------+-------------------------+---------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------+---------------------+| 2ec1ace1-ef83-4d30-837e-f7f2df344a6c | n-UDYIQCS6 | SHUTOFF | network_5261=192.168.1.3 | win7 | win7 || 88a7d91d-3c29-461e-8d07-a0d272697a61 | n-L9QGOHRE | SHUTOFF | network_12177=192.168.1.19 | win7 | win7 || 274543b3-7890-4446-a5d0-1c6acfb9e1f4 | vm_n-4ejn7bk2c9o@28 | ACTIVE | network_10888=192.168.1.18 | win7 | win7 || c2cb7d6c-4f3e-4f32-bbd5-19639d415481 | sw_n-pf6p66vh4f_ext@33 | SHUTOFF | network_10360=192.168.1.3; n-XH29F3CG-net=8.8.8.9; network_21065=192.168.1.6; network_16174=192.168.1.5 | sw-z-in-centos7_ext | sw-z-in-centos7_ext || 5e009b71-4a8b-478a-9f17-349153a26ad4 | n-UE0N48YS | SHUTOFF | network_11645=192.168.1.4; network_14835=192.168.1.13; network_20101=192.168.1.9; internal=20.0.0.23, 192.100.200.225; network_18377=192.168.1.3; network_8075=192.168.1.10 | pstunnel | pstunnel || 374aedbe-d7c3-4f89-a66d-020401a257dc | n-MF9XYLTQ_int | SHUTOFF | network_23143=192.168.1.7; network_14693=192.168.1.7; network_13651=192.168.1.11 | sw-z-in-centos7_int | sw-z-in-centos7_int || acd8efcb-9350-4e19-a105-a1bcfb529697 | rt_n-une1ph6cts@20 | SHUTOFF | Rnet_n-une1ph6cts_0=192.168.3.1; Rnet_n-une1ph6cts_1=192.168.2.1 | router | router || 1efe6a9a-d005-4b32-926f-bb94fb702a05 | vm_n-vp0eo8lm0c@20 | SHUTOFF | network_14083=192.168.1.10 | | |+--------------------------------------+-------------------------+---------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------+---------------------+ 显示实例详细信息1234567891011121314151617181920212223242526272829303132333435$ openstack server show [SERVER-ID/SERVER-Name]$ openstack server show 2ec1ace1-ef83-4d30-837e-f7f2df344a6c+-------------------------------------+----------------------------------------------------------+| Field | Value |+-------------------------------------+----------------------------------------------------------+| OS-DCF:diskConfig | MANUAL || OS-EXT-AZ:availability_zone | nova || OS-EXT-SRV-ATTR:host | compute || OS-EXT-SRV-ATTR:hypervisor_hostname | compute || OS-EXT-SRV-ATTR:instance_name | instance-00000294 || OS-EXT-STS:power_state | Shutdown || OS-EXT-STS:task_state | None || OS-EXT-STS:vm_state | stopped || OS-SRV-USG:launched_at | 2019-07-11T06:30:19.000000 || OS-SRV-USG:terminated_at | None || accessIPv4 | || accessIPv6 | || addresses | network_5261=192.168.1.3 || config_drive | || created | 2019-07-11T06:30:12Z || flavor | win7 (fd8297ef-d7ac-4b4f-8c1e-1c90fcce488c) || hostId | 37f71a5ebb11ac3ed0f0496a313bb9246829f1da1839c8fd3dc4f98b || id | 2ec1ace1-ef83-4d30-837e-f7f2df344a6c || image | win7 (436c5ab5-14e7-4eb8-95b9-e47d85e60e83) || key_name | None || name | n-UDYIQCS6 || project_id | 2e69bc10ab5f427bbbd6d40148d96309 || properties | || security_groups | name='default' || status | SHUTOFF || updated | 2019-07-11T06:31:30Z || user_id | 87bf7fb4671f4524a6fa64ad75856594 || volumes_attached | |+-------------------------------------+----------------------------------------------------------+ Neutron列出所有网络12345678$ openstack network list+--------------------------------------+----------------------+--------------------------------------+| ID | Name | Subnets |+--------------------------------------+----------------------+--------------------------------------+| 01e6c390-1d5e-4140-9ea3-98754bd0c58f | network_29287 | 8ed09dee-0e16-4514-8821-9a8e1f77c43a || 030f303e-e3d3-46e1-a85a-5ea6cea4d3ba | n-69PSABRW-net | e2ea53d0-1b29-43d3-910b-3296d96d004f |+--------------------------------------+----------------------+--------------------------------------+ 查看网络12345678910111213141516171819202122232425262728293031323334$ openstack network show NETWORK-ID$ openstack network show 01e6c390-1d5e-4140-9ea3-98754bd0c58f+---------------------------+--------------------------------------+| Field | Value |+---------------------------+--------------------------------------+| admin_state_up | UP || availability_zone_hints | || availability_zones | nova || created_at | 2019-06-19T02:16:06Z || description | || dns_domain | None || id | 01e6c390-1d5e-4140-9ea3-98754bd0c58f || ipv4_address_scope | None || ipv6_address_scope | None || is_default | None || is_vlan_transparent | None || mtu | 1500 || name | network_29287 || port_security_enabled | True || project_id | 2e69bc10ab5f427bbbd6d40148d96309 || provider:network_type | vxlan || provider:physical_network | None || provider:segmentation_id | 29287 || qos_policy_id | None || revision_number | 3 || router:external | Internal || segments | None || shared | True || status | ACTIVE || subnets | 8ed09dee-0e16-4514-8821-9a8e1f77c43a || tags | || updated_at | 2019-06-19T02:16:07Z |+---------------------------+--------------------------------------+ 列出所有子网12345678$ openstack subnet list+--------------------------------------+-----------------------+--------------------------------------+------------------+| ID | Name | Network | Subnet |+--------------------------------------+-----------------------+--------------------------------------+------------------+| 00414731-ab21-4ee8-8bf2-86d960cc4339 | network_sub_18721 | 84d6ee39-77b3-4a0b-80f8-67922b63079a | 192.168.1.0/24 || 014cb2e2-bdbe-4849-aec7-5fe0581e3b67 | network_sub_20494 | 6eb235c5-798c-49de-8fd3-08aa64f9abaa | 192.168.1.0/24 |+--------------------------------------+-----------------------+--------------------------------------+------------------+ 查看子网12345678910111213141516171819202122232425262728$ openstack subnet show SUBNET-ID$ openstack subnet show 00414731-ab21-4ee8-8bf2-86d960cc4339+-------------------+--------------------------------------+| Field | Value |+-------------------+--------------------------------------+| allocation_pools | 192.168.1.2-192.168.1.254 || cidr | 192.168.1.0/24 || created_at | 2019-07-11T02:09:49Z || description | || dns_nameservers | || enable_dhcp | True || gateway_ip | 192.168.1.1 || host_routes | || id | 00414731-ab21-4ee8-8bf2-86d960cc4339 || ip_version | 4 || ipv6_address_mode | None || ipv6_ra_mode | None || name | network_sub_18721 || network_id | 84d6ee39-77b3-4a0b-80f8-67922b63079a || project_id | 2e69bc10ab5f427bbbd6d40148d96309 || revision_number | 0 || segment_id | None || service_types | || subnetpool_id | None || tags | || updated_at | 2019-07-11T02:09:49Z |+-------------------+--------------------------------------+ 列出所有port12345678$ openstack port list+--------------------------------------+------+-------------------+--------------------------------------------------------------------------------+--------+| ID | Name | MAC Address | Fixed IP Addresses | Status |+--------------------------------------+------+-------------------+--------------------------------------------------------------------------------+--------+| 01f86921-016b-4df8-b526-ec81418b5df5 | | fa:16:3e:b9:c8:a8 | ip_address='20.0.0.2', subnet_id='c00a64e4-8921-427c-aa71-74f7a2c55954' | ACTIVE || 0280ba23-7a51-483c-a7a4-2351f277f739 | | fa:16:3e:ab:63:69 | ip_address='192.168.2.1', subnet_id='c4b84a4b-ce15-49e8-ae6b-72566224682c' | DOWN |+--------------------------------------+------+-------------------+--------------------------------------------------------------------------------+--------+ 查看port12345678910111213141516171819202122232425262728293031323334353637383940$ openstack port show PORT-ID$ openstack port show 01f86921-016b-4df8-b526-ec81418b5df5+-----------------------+-------------------------------------------------------------------------------+| Field | Value |+-----------------------+-------------------------------------------------------------------------------+| admin_state_up | UP || allowed_address_pairs | || binding_host_id | controller || binding_profile | || binding_vif_details | datapath_type='system', ovs_hybrid_plug='True', port_filter='True' || binding_vif_type | ovs || binding_vnic_type | normal || created_at | 2019-06-19T01:41:33Z || data_plane_status | None || description | || device_id | dhcpd3377d3c-a0d1-5d71-9947-f17125c357bb-361f196b-154a-4e51-bc34-7162d043be1b || device_owner | network:dhcp || dns_assignment | None || dns_name | None || extra_dhcp_opts | || fixed_ips | ip_address='20.0.0.2', subnet_id='c00a64e4-8921-427c-aa71-74f7a2c55954' || id | 01f86921-016b-4df8-b526-ec81418b5df5 || ip_address | None || mac_address | fa:16:3e:b9:c8:a8 || name | || network_id | 361f196b-154a-4e51-bc34-7162d043be1b || option_name | None || option_value | None || port_security_enabled | False || project_id | 2e69bc10ab5f427bbbd6d40148d96309 || qos_policy_id | None || revision_number | 6 || security_group_ids | || status | ACTIVE || subnet_id | None || tags | || trunk_details | None || updated_at | 2019-06-19T01:41:35Z |+-----------------------+-------------------------------------------------------------------------------+ 摘自：Openstack官方文档 https://docs.openstack.org/zh_CN/user-guide/cli-cheat-sheet.html]]></content>
      <categories>
        <category>Openstack</category>
      </categories>
      <tags>
        <tag>openstack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Openstack修改系统网络配额]]></title>
    <url>%2F2019%2F07%2F11%2FOpenstackNeutronQuotas%2F</url>
    <content type="text"><![CDATA[前言neutron在安装配置完成之后，openstack为了实现对所有tenant对网络资源的使用，针对neutron设置有专门的配额，以防止租户使用过多的资源，而对其他的tenant造成影响。和nova的quota相类似，neutron也使用单独的一个驱动来实现网络neutron的配额控制。 neutron默认的配额neutron默认的配额针对network，port，router，subnet，floatingip做了配额方面的限定，参考neutron的配置文件，获取quota的配额内容为: 12345678910111213141516[root@controller ~]# vim /etc/neutron/neutron.conf[quotas]quota_driver = neutron.db.quota_db.DbQuotaDriver 配额驱动quota_items = network,subnet,port quota限定的范畴default_quota = -1 默认的quota，-1表示没有限制(未启用)quota_network = 10 建立的network个数quota_subnet = 10 建立的subnet个数quota_port = 50 允许的port个数quota_security_group = 10 安全组的个数quota_security_group_rule = 100 安全组规规则条数quota_vip = 10 vip个数，以下的quota_member和quota_health_monitors 都用于LBaaS场景quota_pool = 10 pool个数quota_member = -1 member个数quota_health_monitors = -1 monitor个数quota_router = 10 router的个数quota_floatingip = 50 floating-ip个数 修改neutron的配额查看neutron默认的配额 123456789[root@controller ~]# keystone tenant-list+----------------------------------+----------+---------+| id | name | enabled |+----------------------------------+----------+---------+| 842ab3268a2c47e6a4b0d8774de805ae | admin | True || 7ff1dfb5a6f349958c3a949248e56236 | companyA | True | #得到tenant的uuid号| 10d1465c00d049fab88dec1af0f56b1b | demo | True || 3b57a14f7c354a979c9f62b60f31a331 | service | True |+----------------------------------+----------+---------+ 12345678910111213141516[root@controller ~]# neutron quota-show --tenant-id 7ff1dfb5a6f349958c3a949248e56236+---------------------+-------+| Field | Value |+---------------------+-------+| floatingip | 50 || health_monitor | -1 || member | -1 || network | 10 || pool | 10 || port | 50 | #port，每台虚拟机都需要一个ip，即一个port，很容易就超过配额| router | 10 || security_group | 10 || security_group_rule | 100 || subnet | 10 || vip | 10 |+---------------------+-------+ 修改neutron配额 12345678910111213141516[root@controller ~]# neutron quota-update --network 20 --subnet 20 --port 100 --router 5 --floatingip 100 --security-group 10 --security-group-rule 100 --tenant-id 7ff1dfb5a6f349958c3a949248e56236+---------------------+-------+| Field | Value |+---------------------+-------+| floatingip | 100 || health_monitor | -1 || member | -1 || network | 20 || pool | 10 || port | 100 || router | 5 || security_group | 10 || security_group_rule | 100 || subnet | 20 || vip | 10 |+---------------------+-------+ 校验neutron的quota配置 12345678910111213141516[root@controller ~]# neutron quota-show --tenant-id 7ff1dfb5a6f349958c3a949248e56236+---------------------+-------+| Field | Value |+---------------------+-------+| floatingip | 100 || health_monitor | -1 || member | -1 || network | 20 || pool | 10 || port | 100 || router | 5 || security_group | 10 || security_group_rule | 100 || subnet | 20 || vip | 10 |+---------------------+-------+ 统计port的个数1234567891011[root@controller ~]# neutron port-list+--------------------------------------+------+-------------------+---------------------------------------------------------------------------------------+| id | name | mac_address | fixed_ips |+--------------------------------------+------+-------------------+---------------------------------------------------------------------------------------+| 0060ec4a-957d-4571-b730-6b4a9bb3baf8 | | fa:16:3e:48:42:3d | &#123;"subnet_id": "9654a807-d4fa-49f1-abb6-2e45d776c69f", "ip_address": "10.16.4.19"&#125; || 00942be0-a3a9-471d-a4ba-336db0ee1539 | | fa:16:3e:73:75:03 | &#123;"subnet_id": "ad4a5ffc-3ccc-42c4-89a1-61e7b18632a3", "ip_address": "10.16.6.96"&#125; || 0119045c-8219-4744-bd58-a7e77294832c | | fa:16:3e:10:ed:7f | &#123;"subnet_id": "9654a807-d4fa-49f1-abb6-2e45d776c69f", "ip_address": "10.16.4.71"&#125; || 04f7d8ea-1849-4938-9ef7-e8114893132f | | fa:16:3e:50:86:1b | &#123;"subnet_id": "ad4a5ffc-3ccc-42c4-89a1-61e7b18632a3", "ip_address": "10.16.6.27"&#125; |[root@controller ~]# neutron port-list |wc -l #超过配额时，需要修改194 总结随着时间的推移，当越来越多的instance加入到openstack中，port也会相应增加，一个ip对应一个port，所以当port达到配额时，openstack会组织用户继续分配虚拟机，此时，就需要修改neutron的配额了，关于neutron配额的报错，可以参考neutron的日志/var/log/neutron/neutron-server.log，可以根据日志的信息，定位到报错的原因，具体不赘述。 附录neutron实现quota的代码解读 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197[root@controller ~]# vim /usr/lib/python2.6/site-packages/neutron/db/quota_db.pyimport sqlalchemy as safrom neutron.common import exceptionsfrom neutron.db import model_basefrom neutron.db import models_v2'''quota数据库表的表结构，tenant默认集成的配额从这里获取mysql&gt; desc quotas;+-----------+--------------+------+-----+---------+-------+| Field | Type | Null | Key | Default | Extra |+-----------+--------------+------+-----+---------+-------+| id | varchar(36) | NO | PRI | NULL | || tenant_id | varchar(255) | YES | MUL | NULL | || resource | varchar(255) | YES | | NULL | || limit | int(11) | YES | | NULL | |+-----------+--------------+------+-----+---------+-------+'''class Quota(model_base.BASEV2, models_v2.HasId): """Represent a single quota override for a tenant. If there is no row for a given tenant id and resource, then the default for the quota class is used. """ tenant_id = sa.Column(sa.String(255), index=True) resource = sa.Column(sa.String(255)) limit = sa.Column(sa.Integer)'''quota配额的具体实现，根据数据库的配置内容，实现quota的控制，即quota的增删改查方法'''class DbQuotaDriver(object): """Driver to perform necessary checks to enforce quotas and obtain quota information. The default driver utilizes the local database. """ ''' 得到租户tenant的quota，执行neutron quota-show --tenant-id uuid时调用的方法 ''' @staticmethod def get_tenant_quotas(context, resources, tenant_id): """Given a list of resources, retrieve the quotas for the given tenant. :param context: The request context, for access checks. :param resources: A dictionary of the registered resource keys. :param tenant_id: The ID of the tenant to return quotas for. :return dict: from resource name to dict of name and limit """ # init with defaults 得到quota默认的配额项item，即所谓的network，subnet，port和router等，以及对应的值 tenant_quota = dict((key, resource.default) for key, resource in resources.items()) # update with tenant specific limits 从数据库中获取最新的quota配置信息，并更新 q_qry = context.session.query(Quota).filter_by(tenant_id=tenant_id) tenant_quota.update((q['resource'], q['limit']) for q in q_qry) return tenant_quota ''' quota的删除，即执行neutron quota-delete 的方法，删除之后，tenant将会集成默认的的quota配置 ''' @staticmethod def delete_tenant_quota(context, tenant_id): """Delete the quota entries for a given tenant_id. Atfer deletion, this tenant will use default quota values in conf. """ #从neutron。quotas数据库中查询到所有的quota配置之后，过略某个具体的tenant的quota，之后执行delete()方法将其删除 with context.session.begin(): tenant_quotas = context.session.query(Quota) tenant_quotas = tenant_quotas.filter_by(tenant_id=tenant_id) tenant_quotas.delete() ''' 得到所有租户tenant的配额资源，即执行neutron quota-list所查看的内容 ''' @staticmethod def get_all_quotas(context, resources): """Given a list of resources, retrieve the quotas for the all tenants. :param context: The request context, for access checks. :param resources: A dictionary of the registered resource keys. :return quotas: list of dict of tenant_id:, resourcekey1: resourcekey2: ... """ tenant_default = dict((key, resource.default) for key, resource in resources.items()) all_tenant_quotas = &#123;&#125; for quota in context.session.query(Quota): tenant_id = quota['tenant_id'] # avoid setdefault() because only want to copy when actually req'd #如果quotas表中，没有找到配置选项，说明使用默认的quota配置，直接用默认的copy过来即可，有配置则继承quotas表中的配置 tenant_quota = all_tenant_quotas.get(tenant_id) if tenant_quota is None: tenant_quota = tenant_default.copy() tenant_quota['tenant_id'] = tenant_id all_tenant_quotas[tenant_id] = tenant_quota tenant_quota[quota['resource']] = quota['limit'] return all_tenant_quotas.values() ''' 更新quota的配置，即执行neutron quota-update命令的具体实现 ''' @staticmethod def update_quota_limit(context, tenant_id, resource, limit): with context.session.begin(): tenant_quota = context.session.query(Quota).filter_by( tenant_id=tenant_id, resource=resource).first() #有配置内容，则更新，没有则根据资源的配置内容，在数据库中添加对应的条目 if tenant_quota: tenant_quota.update(&#123;'limit': limit&#125;) else: tenant_quota = Quota(tenant_id=tenant_id, resource=resource, limit=limit) context.session.add(tenant_quota) def _get_quotas(self, context, tenant_id, resources, keys): """Retrieves the quotas for specific resources. A helper method which retrieves the quotas for the specific resources identified by keys, and which apply to the current context. :param context: The request context, for access checks. :param tenant_id: the tenant_id to check quota. :param resources: A dictionary of the registered resources. :param keys: A list of the desired quotas to retrieve. """ desired = set(keys) sub_resources = dict((k, v) for k, v in resources.items() if k in desired) # Make sure we accounted for all of them... if len(keys) != len(sub_resources): unknown = desired - set(sub_resources.keys()) raise exceptions.QuotaResourceUnknown(unknown=sorted(unknown)) # Grab and return the quotas (without usages) quotas = DbQuotaDriver.get_tenant_quotas( context, sub_resources, tenant_id) return dict((k, v) for k, v in quotas.items()) ''' neutron quota的校验，即在执行过程中，调用该方法，确认tenant的quota是否在合理的范围内 ''' def limit_check(self, context, tenant_id, resources, values): """Check simple quota limits. For limits--those quotas for which there is no usage synchronization function--this method checks that a set of proposed values are permitted by the limit restriction. This method will raise a QuotaResourceUnknown exception if a given resource is unknown or if it is not a simple limit resource. If any of the proposed values is over the defined quota, an OverQuota exception will be raised with the sorted list of the resources which are too high. Otherwise, the method returns nothing. :param context: The request context, for access checks. :param tenant_id: The tenant_id to check the quota. :param resources: A dictionary of the registered resources. :param values: A dictionary of the values to check against the quota. """ # Ensure no value is less than zero quota的配置值不能为负数 unders = [key for key, val in values.items() if val &lt; 0] if unders: raise exceptions.InvalidQuotaValue(unders=sorted(unders)) # Get the applicable quotas quotas = self._get_quotas(context, tenant_id, resources, values.keys()) # Check the quotas and construct a list of the resources that # would be put over limit by the desired values overs = [key for key, val in values.items() if quotas[key] &gt;= 0 and quotas[key] &lt; val] if overs: raise exceptions.OverQuota(overs=sorted(overs)) 摘自：https://segmentfault.com/a/1190000018750816]]></content>
      <categories>
        <category>Openstack</category>
      </categories>
      <tags>
        <tag>openstack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL中InnoDB和MyISAM的存储引擎区别]]></title>
    <url>%2F2019%2F07%2F10%2FMysqlEngine%2F</url>
    <content type="text"><![CDATA[InnoDB存储引擎：InnoDB存储引擎支持事务，其设计目标主要就是面向OLTP（On Line Transaction Processing 在线事务处理）的应用。特点为行锁设计、支持外键，并支持非锁定读。从5.5.8版本开始，InnoDB成为了MySQL的默认存储引擎。 InnoDB存储引擎采用聚集索引（clustered）的方式来存储数据，因此每个表都是按照主键的顺序进行存放，如果没有指定主键，InnoDB会为每行自动生成一个6字节的ROWID作为主键。 MyISAM存储引擎：MyISAM存储引擎不支持事务、表锁设计，支持全文索引，主要面向OLAP（On Line Analytical Processing 联机分析处理）应用，适用于数据仓库等查询频繁的场景。在5.5.8版本之前，MyISAM是MySQL的默认存储引擎。该引擎代表着对海量数据进行查询和分析的需求。它强调性能，因此在查询的执行速度比InnoDB更快。 MyISAM存储引擎还有一个特点是只缓存索引文件，而不缓存数据文件，这点非常独特。 InnoDB和MyISAM的区别:事务为了数据库操作的原子性，我们需要事务。保证一组操作要么都成功，要么都失败，比如转账的功能。我们通常将多条SQL语句放在begin和commit之间，组成一个事务。 InnoDB支持，MyISAM不支持。 主键由于InnoDB的聚集索引，其如果没有指定主键，就会自动生成主键。MyISAM支持没有主键的表存在。 外键为了解决复杂逻辑的依赖，我们需要外键。比如高考成绩的录入，必须归属于某位同学，我们就需要高考成绩数据库里有准考证号的外键。 InnoDB支持，MyISAM不支持。 索引为了优化查询的速度，进行排序和匹配查找，我们需要索引。比如所有人的姓名从a-z首字母进行顺序存储，当我们查找zhangsan或者第44位的时候就可以很快的定位到我们想要的位置进行查找。 InnoDB是聚集索引，数据和主键的聚集索引绑定在一起，通过主键索引效率很高。如果通过其他列的辅助索引来进行查找，需要先查找到聚集索引，再查询到所有数据，需要两次查询。 MyISAM是非聚集索引，数据文件是分离的，索引保存的是数据的指针。 从InnoDB 1.2.x版本，MySQL5.6版本后，两者都支持全文索引。 auto_increment对于自增数的字段，InnoDB要求必须有只有该字段的索引。但MyISAM可以将该字段与其他字段组成联合索引。 表行数很常见的需求是看表中有多少条数据，此时我们需要 1select count(*) from table_name; InnoDB不保存表行数，需要进行全表扫描。MyISAM用一个变量保存，直接读取该值，更快。当时当带有where查询的时候，两者一样。 存储数据库的文件都是需要在磁盘中进行存储，当应用需要时再读取到内存中。一般包含数据文件、索引文件。 InnoDB分为：.frm表结构文件.ibdata1共享表空间.ibd表独占空间.redo日志文件 MyISAM分为三个文件：.frm存储表定义.MYD存储表数据.MYI存储表索引 执行速度如果你的操作是大量的查询操作，如SELECT，使用MyISAM性能会更好。如果大部分是删除和更改的操作，使用InnoDB。 delete调用delete from table时，MyISAM会直接重建表，InnoDB会一行一行的删除，但是可以用truncate table代替。 锁MyISAM仅支持表锁，每次操作锁定整张表。InnoDB支持行锁，每次操作锁住最小数量的行数据。 表锁相比于行锁消耗的资源更少，且不会出现死锁，但同时并发性能差。行锁消耗更多的资源，速度较慢，且可能发生死锁，但是因为锁定的粒度小、数据少，并发性能好。如果InnoDB的一条语句无法确定要扫描的范围，也会锁定整张表。 当行锁发生死锁的时候，会计算每个事务影响的行数，然后回滚行数较少的事务。 数据恢复MyISAM崩溃后无法快速的安全恢复。InnoDB有一套完善的恢复机制。 数据缓存MyISAM仅缓存索引数据，通过索引查询数据。InnoDB不仅缓存索引数据，同时缓存数据信息，将数据按页读取到缓存池，按LRU（Latest Rare Use 最近最少使用）算法来进行更新。 如何选择存储引擎创建表的语句都是相同的，只有最后的type来指定存储引擎。 MyISAM1.大量查询总count2.查询频繁，插入不频繁3.没有事务操作 InnoDB1.需要高可用性，或者需要事务2.表更新频繁 摘自：https://segmentfault.com/a/1190000019713794]]></content>
      <categories>
        <category>Mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mysql 1040 Too many connections 错误与解决办法]]></title>
    <url>%2F2019%2F07%2F01%2FTooManyConnections%2F</url>
    <content type="text"><![CDATA[日志报如下错误: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364Traceback (most recent call last): File "/var/www/kdpa/project/api/host.py", line 339, in showInstanceConsole File "/var/www/kdpa/project/api/hostImpl.py", line 500, in showInstanceConsoleImpl opsVmId = self.hostDao.getOpsVmIdByUuid(uuid) File "/var/www/kdpa/project/core/modifier.py", line 46, in _dao return daoFn(*args, **kwargs) File "/var/www/kdpa/project/dao/host/hostDao.py", line 168, in getOpsVmIdByUuid host = session.query(Host).filter(Host.uuid == str(uuid)).first() File "/usr/lib64/python2.7/site-packages/sqlalchemy/orm/query.py", line 3222, in first ret = list(self[0:1]) File "/usr/lib64/python2.7/site-packages/sqlalchemy/orm/query.py", line 3012, in __getitem__ return list(res) File "/usr/lib64/python2.7/site-packages/sqlalchemy/orm/query.py", line 3324, in __iter__ return self._execute_and_instances(context) File "/usr/lib64/python2.7/site-packages/sqlalchemy/orm/query.py", line 3346, in _execute_and_instances querycontext, self._connection_from_session, close_with_result=True File "/usr/lib64/python2.7/site-packages/sqlalchemy/orm/query.py", line 3361, in _get_bind_args mapper=self._bind_mapper(), clause=querycontext.statement, **kw File "/usr/lib64/python2.7/site-packages/sqlalchemy/orm/query.py", line 3339, in _connection_from_session conn = self.session.connection(**kw) File "/usr/lib64/python2.7/site-packages/sqlalchemy/orm/session.py", line 1124, in connection execution_options=execution_options, File "/usr/lib64/python2.7/site-packages/sqlalchemy/orm/session.py", line 1130, in _connection_for_bind engine, execution_options File "/usr/lib64/python2.7/site-packages/sqlalchemy/orm/session.py", line 431, in _connection_for_bind conn = bind._contextual_connect() File "/usr/lib64/python2.7/site-packages/sqlalchemy/engine/base.py", line 2226, in _contextual_connect self._wrap_pool_connect(self.pool.connect, None), File "/usr/lib64/python2.7/site-packages/sqlalchemy/engine/base.py", line 2266, in _wrap_pool_connect e, dialect, self File "/usr/lib64/python2.7/site-packages/sqlalchemy/engine/base.py", line 1536, in _handle_dbapi_exception_noconnection util.raise_from_cause(sqlalchemy_exception, exc_info) File "/usr/lib64/python2.7/site-packages/sqlalchemy/util/compat.py", line 399, in raise_from_cause reraise(type(exception), exception, tb=exc_tb, cause=cause) File "/usr/lib64/python2.7/site-packages/sqlalchemy/engine/base.py", line 2262, in _wrap_pool_connect return fn() File "/usr/lib64/python2.7/site-packages/sqlalchemy/pool/base.py", line 363, in connect return _ConnectionFairy._checkout(self) File "/usr/lib64/python2.7/site-packages/sqlalchemy/pool/base.py", line 760, in _checkout fairy = _ConnectionRecord.checkout(pool) File "/usr/lib64/python2.7/site-packages/sqlalchemy/pool/base.py", line 492, in checkout rec = pool._do_get() File "/usr/lib64/python2.7/site-packages/sqlalchemy/pool/impl.py", line 139, in _do_get self._dec_overflow() File "/usr/lib64/python2.7/site-packages/sqlalchemy/util/langhelpers.py", line 68, in __exit__ compat.reraise(exc_type, exc_value, exc_tb) File "/usr/lib64/python2.7/site-packages/sqlalchemy/pool/impl.py", line 136, in _do_get return self._create_connection() File "/usr/lib64/python2.7/site-packages/sqlalchemy/pool/base.py", line 308, in _create_connection return _ConnectionRecord(self) File "/usr/lib64/python2.7/site-packages/sqlalchemy/pool/base.py", line 437, in __init__ self.__connect(first_connect_check=True) File "/usr/lib64/python2.7/site-packages/sqlalchemy/pool/base.py", line 639, in __connect connection = pool._invoke_creator(self) File "/usr/lib64/python2.7/site-packages/sqlalchemy/engine/strategies.py", line 114, in connect return dialect.connect(*cargs, **cparams) File "/usr/lib64/python2.7/site-packages/sqlalchemy/engine/default.py", line 451, in connect return self.dbapi.connect(*cargs, **cparams) File "build/bdist.linux-x86_64/egg/MySQLdb/__init__.py", line 81, in Connect return Connection(*args, **kwargs) File "build/bdist.linux-x86_64/egg/MySQLdb/connections.py", line 187, in __init__ super(Connection, self).__init__(*args, **kwargs2)OperationalError: (_mysql_exceptions.OperationalError) (1040, 'Too many connections')(Background on this error at: http://sqlalche.me/e/e3q8) 解决方式： 查看当前数据库最大连接数： 1234567MariaDB [(none)]&gt; select VARIABLE_VALUE from information_schema.GLOBAL_VARIABLES where VARIABLE_NAME='MAX_CONNECTIONS';+----------------+| VARIABLE_VALUE |+----------------+| 214 |+----------------+1 row in set (0.00 sec) 12345vi /etc/systemd/system/mariadb.service.d/limits.conf[Service]LimitNOFILE=65535LimitNPROC=65535 保存，退出。 12systemctl daemon-reloadsystemctl restart mariadb.service 重启后查看： 1234567MariaDB [(none)]&gt; select VARIABLE_VALUE from information_schema.GLOBAL_VARIABLES where VARIABLE_NAME='MAX_CONNECTIONS';+----------------+| VARIABLE_VALUE |+----------------+| 4096 |+----------------+1 row in set (0.00 sec) 4096配置在Openstack的mysql配置 /etc/my.cnf.d/openstack.cnf中max_connections = 4096。]]></content>
      <categories>
        <category>Mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[压力测试工具ab使用方法]]></title>
    <url>%2F2019%2F06%2F28%2Fab%2F</url>
    <content type="text"><![CDATA[安装在CentOS7-mini环境下，ab运行需要依赖apr-util，httpd-tools包，安装命令为： 1yum install apr-util httpd-tools -y 执行测试12#vi post.txtuuid=n-vp0eo8lm0c&amp; 1ab -c 10 -n 10 -p 'post.txt' -T 'application/x-www-form-urlencoded' http://192.100.200.140:8000/api/instance/show 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748This is ApacheBench, Version 2.3 &lt;$Revision: 1430300 $&gt;Copyright 1996 Adam Twiss, Zeus Technology Ltd, http://www.zeustech.net/Licensed to The Apache Software Foundation, http://www.apache.org/Benchmarking 192.100.200.140 (be patient).....doneServer Software: nginx/1.9.9Server Hostname: 192.100.200.140Server Port: 8000Document Path: /api/instance/showDocument Length: 23 bytesConcurrency Level: 10Time taken for tests: 1.378 secondsComplete requests: 10Failed requests: 6 (Connect: 0, Receive: 0, Length: 6, Exceptions: 0)Write errors: 0Non-2xx responses: 3Total transferred: 3398 bytesTotal body sent: 1940HTML transferred: 1787 bytesRequests per second: 7.26 [#/sec] (mean)Time per request: 1377.853 [ms] (mean)Time per request: 137.785 [ms] (mean, across all concurrent requests)Transfer rate: 2.41 [Kbytes/sec] received 1.37 kb/s sent 3.78 kb/s totalConnection Times (ms) min mean[+/-sd] median maxConnect: 0 0 0.1 0 1Processing: 68 543 548.0 246 1256Waiting: 68 543 548.0 246 1256Total: 69 544 548.0 247 1257Percentage of the requests served within a certain time (ms) 50% 247 66% 1095 75% 1157 80% 1196 90% 1257 95% 1257 98% 1257 99% 1257 100% 1257 (longest request) 参数说明：-n 10 表示请求总数为10 -c 10 表示并发用户数为10 http://192.100.200.140:8000/api/instance/show 表示这写请求的目标URL 测试结果也一目了然，测试出的吞吐率为：Requests per second: 2015.93 [#/sec] (mean) 初次之外还有其他一些信息。 Server Software 表示被测试的Web服务器软件名称 Server Hostname 表示请求的URL主机名 Server Port 表示被测试的Web服务器软件的监听端口 Document Path 表示请求的URL中的根绝对路径，通过该文件的后缀名，我们一般可以了解该请求的类型 Document Length 表示HTTP响应数据的正文长度 Concurrency Level 表示并发用户数，这是我们设置的参数之一 Time taken for tests 表示所有这些请求被处理完成所花费的总时间 Complete requests 表示总请求数量，这是我们设置的参数之一 Failed requests 表示失败的请求数量，这里的失败是指请求在连接服务器、发送数据等环节发生异常，以及无响应后超时的情况。如果接收到的HTTP响应数据的头信息中含有2XX以外的状态码，则会在测试结果中显示另一个名为 “Non-2xx responses”的统计项，用于统计这部分请求数，这些请求并不算在失败的请求中。 Total transferred 表示所有请求的响应数据长度总和，包括每个HTTP响应数据的头信息和正文数据的长度。注意这里不包括HTTP请求数据的长度，仅仅为web服务器流向用户PC的应用层数据总长度。 HTML transferred 表示所有请求的响应数据中正文数据的总和，也就是减去了Total transferred中HTTP响应数据中的头信息的长度。 Requests per second 吞吐率，计算公式：Complete requests / Time taken for tests Time per request 用户平均请求等待时间，计算公式：Time token for tests/（Complete requests/Concurrency Level） Time per requet(across all concurrent request) 服务器平均请求等待时间，计算公式：Time taken for tests/Complete requests，正好是吞吐率的倒数。也可以这么统计：Time per request/Concurrency Level Transfer rate 表示这些请求在单位时间内从服务器获取的数据长度，计算公式：Total trnasferred/ Time taken for tests，这个统计很好的说明服务器的处理能力达到极限时，其出口宽带的需求量。 Percentage of requests served within a certain time（ms） 这部分数据用于描述每个请求处理时间的分布情况，比如以上测试，80%的请求处理时间都不超过6ms，这个处理时间是指前面的Time per request，即对于单个用户而言，平均每个请求的处理时间。]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>tool</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[实验室虚拟化主站厂站拓扑搭建]]></title>
    <url>%2F2019%2F06%2F25%2FkdpaTopo%2F</url>
    <content type="text"><![CDATA[虚拟化正向隔离①创建虚拟化正向隔离组件 SW_1，开机②创建虚拟交换机 Switch_2，开机③创建虚拟交换机 Switch_3，开机④创建虚拟主机[win7-2-ok]-主站PC PC_4，开机⑤创建虚拟主机[win7-2-ok]-安全三区PC PC_5，开机⑥创建虚拟主机[win7-2-ok]-隔离SW_1管理工具 PC_6，开机⑦配置PC_4IP地址：192.168.1.10 255.255.255.0 192.168.1.1⑧配置PC_5IP地址：192.168.1.20 255.255.255.0 192.168.1.1⑨配置Switch_2：1-4口配置VLAN1001⑩配置Switch_3：1-4口配置VLAN1002⑪创建串口线，连接SW_1串口（内）与PC_6⑫创建网线，连接SW_1网卡0（内）与Switch_2网口1⑬创建网线，连接SW_1网卡0（外）与Switch_3网口1⑭创建网线，连接PC_4网卡0与Switch_2网口2⑮创建网线，连接PC_5网卡0与Switch_3网口2⑯配置正向隔离管理工具PC_6：配置管理工具。 PC_6右键 -&gt; 上传/下载 -&gt; 传入vc_redist.2015.x64与全QT界面隔离管理工具中包含vcruntime140.dll，libcrypto-1_1.dll。 -&gt; 关闭 安装vc_redist.2015.x64 打开全QT界面隔离管理工具Stonewall.exe 屏幕分辨率 -&gt; 1024×768 用户名：admin 密码：111111 串口登录。 串口：COM1。 频率：115200。 权限：管理员。 √ 正向隔离。 设备配置 -&gt; 基本配置 管理IP： 无 -&gt; 不填 设备名称： zsw -&gt; 自定义 点击写入 规则配置 -&gt; 主机信息1 -&gt; 添加 SW_1 -&gt; 进入内网隔离系统 -&gt; 清除主机信息与连接信息 -&gt; 删除/etc/mac.conf 和 /etc/rules √ IP和MAC地址绑定 主机名： PC_4 -&gt; 自定义 MAC1： FA:16:3E:40:11:01 -&gt; 主机网卡MAC地址 主机IP： 192.168.1.10 -&gt; 主机网卡ip地址 主机虚拟IP1：192.168.1.11 -&gt; ETH0协商IP与主机同网段时自动生成,不通网段时填写一个ETH0所在网段下任意IP地址 主机虚拟ip2：可以不填 规则配置 -&gt; 主机信息2 -&gt; 添加 √ IP和MAC地址绑定 主机名： PC_5 -&gt; 自定义 MAC1： FA:16:3E:40:11:02 -&gt; 主机网卡MAC地址 主机IP： 192.168.1.20 -&gt; 主机网卡ip地址 主机虚拟IP1：192.168.1.21 -&gt; ETH0协商IP与主机同网段时自动生成,不通网段时填写一个ETH0所在网段下任意IP地址 主机虚拟ip2：可以不填 规则配置 -&gt; 连接信息 规则名： rule -&gt; 自定义 (内网)IP地址： 192.100.1.10 (内网)虚拟IP： 192.100.1.11 (内网)网口号： ETH0/网口0 (外网)IP地址： 192.100.1.20 (外网)虚拟IP： 192.100.1.21 (外网)网口号： ETH0/网口0 协议： TCP ⑰配置后查看SW_1 /etc/mac.conf和/etc/rules是否与配置相同。重启SW_1内网隔离与外网隔离。 正向隔离： 发送端为内网 接收端为外网 sender：PC_4 receiver：PC_5 ⑱配置及启动正向隔离外网接收端工具。 PC_5 -&gt; 进入系统 -&gt; 右键 -&gt; Terminal -&gt; 以管理员身份运行 -&gt; 进入目录 -&gt; D:\software\隔离传输软件\正向\receive 以jar包方式运行 java -jar StoneWall-recv.jar ⑲配置及启动正向隔离内网发送端工具。 PC_4 -&gt; 进入系统 -&gt; 右键 -&gt; Terminal -&gt; 以管理员身份运行 -&gt; 进入目录 -&gt; D:\software\隔离传输软件\send 以jar包方式运行 java -jar StoneWall-send.jar 待发送的文件（任何类型）-&gt; 右键 -&gt; 发送 任务名称： task √ 主任务 目的IP地址： 192.168.1.21 -&gt; 接收端虚拟IP地址 目的端口号： 7777 目的文件夹： d:/test -&gt; 自定义 √ 立即发送 查看日志及接收端是否发送成功。 虚拟化反向隔离⑳创建虚拟化反向隔离组件 SW_2，开机㉑创建虚拟主机[win7-2-ok]-隔离SW_2管理工具 PC_7，开机㉒创建串口线，连接SW_2串口（外）与PC_7㉓创建网线，连接SW_2网卡0（内）与Switch_2网口3㉔创建网线，连接SW_2网卡0（外）与Switch_3网口3㉕⑯配置反向隔离管理工具PC_7：配置管理工具。 PC_7右键 -&gt; 上传/下载 -&gt; 传入vc_redist.2015.x64与全QT界面隔离管理工具中包含vcruntime140.dll，libcrypto-1_1.dll。 -&gt; 关闭 安装vc_redist.2015.x64 打开全QT界面隔离管理工具Stonewall.exe 屏幕分辨率 -&gt; 1024×768 用户名：admin 密码：111111 串口登录。 串口：COM1。 频率：115200。 权限：管理员。 √ 反向隔离。 设备配置 -&gt; 基本配置 管理IP： 无 -&gt; 不填 设备名称： fsw -&gt; 自定义 ETH0协商IP： 192.168.1.200 -&gt; 与发送端须配置成相同网段 ETH1协商IP： 0.0.0.0 -&gt; 可不做配置 规则配置 -&gt; 主机信息1 -&gt; 添加 SW_2 -&gt; 进入外网隔离系统 -&gt; 清除主机信息与连接信息 -&gt; 删除/etc/mac.conf 和 /etc/rules √ IP和MAC地址绑定 主机名： PC_5 -&gt; 自定义 MAC1： FA:16:3E:40:11:02 -&gt; 主机网卡MAC地址 主机IP： 192.168.1.20 -&gt; 主机网卡ip地址 主机虚拟IP1：192.168.1.22 -&gt; ETH0协商IP与主机同网段时自动生成,不通网段时填写一个ETH0所在网段下任意IP地址 主机虚拟ip2：可以不填 规则配置 -&gt; 主机信息2 -&gt; 添加 √ IP和MAC地址绑定 主机名： PC_4 -&gt; 自定义 MAC1： FA:16:3E:40:11:01 -&gt; 主机网卡MAC地址 主机IP： 192.168.1.10 -&gt; 主机网卡ip地址 主机虚拟IP1：192.168.1.12 -&gt; ETH0协商IP与主机同网段时自动生成,不通网段时填写一个ETH0所在网段下任意IP地址 主机虚拟ip2：可以不填 规则配置 -&gt; 连接信息 规则名： rule -&gt; 自定义 (内网)IP地址： 192.100.1.10 (内网)虚拟IP： 192.100.1.12 (内网)网口号： ETH0/网口0 (外网)IP地址： 192.100.1.20 (外网)虚拟IP： 192.100.1.22 (外网)网口号： ETH0/网口0 协议： UDP 证书秘钥 -&gt; 设备秘钥 设备密钥 √ RSA -&gt; 导出设备证书文件 -&gt; fsw.cer 上传/下载 -&gt; fsw.cer导出到本地 ㉖配置及启动反向隔离内网接收端工具。 上传/下载 -&gt; 新的隔离接收端工具new_udp_recv_4.2.2.jar -&gt; 复制到目录D:\software\隔离传输软件\反向\receive\下 右键 -&gt; Terminal -&gt; 以管理员身份运行 -&gt; 进入目录 -&gt; D:\software\隔离传输软件\反向\receive 以jar包方式运行 java -jar new_udp_recv_4.2.2.jar ㉗配置及启动反向隔离外网发送端工具。 上传/下载 -&gt; 新的隔离接收端工具new_udp_send_4.2.2.jar -&gt; 复制到目录D:\software\隔离传输软件\反向\send\下 上传/下载 -&gt; 反向隔离端证书 fsw.cer 检查config目录下是否有**.p12文件，若存在，则删除。 启动传输软件。 new_udp_recv_4.2.2.jar 右键 -&gt; 以管理员身份运行 -&gt; 命令行 -&gt; 进入目录 -&gt; D:\software\隔离传输软件\反向\send 12java -jar new_udp_recv_4.2.2.jar设置密码口令保护窗口如果报错，需先运行jreUpdate1.8.jar 出现提示：系统检测到密钥尚未存在，是否需要生成密钥？ -&gt; 是 创建登录口令：123456。 确定，提示操作成功。 登录窗口： 操作员名称： administrator 操作员密码： 12345678 密钥保护口令： 123456 管理 -&gt; 密钥管理 -&gt; 导出密钥。 -&gt; sender.cer 上传/下载 -&gt; 反向隔离发送端证书 sender.cer 设定 -&gt; 配置加密隧道 隧道名称： Tunnel-1 隧道的协商IP地址： 192.168.1.200 -&gt; 设备ETH0协商IP 隧道的协商端口： 4558 隧道每次通过的文件数： 100 隔离设备证书路径： ../.. fsw.cer 设定 -&gt; 配置链路信息 链路名称： Link-1 目的IP地址： 192.168.1.12 -&gt; 接收端IP地址，正反向隔离在相同环境的情况下，配置虚拟IP。 目的端口： 7777 发送失败等待周期(秒)： 30 使用隧道： Tunnel-1 ㉘PC_7导入发送端证书。 发送端证书 -&gt; 删除其他 发送端证书 -&gt; 添加 发送端IP： 192.168.1.20 -&gt; 此处为发送端IP，不为虚拟IP 证书标识： sender.cer ㉙重启SW_2内网隔离与外网隔离。㉚PC_5发送E文本。 待发送的文件（E文本）-&gt; 右键 -&gt; 发送 任务名称： task1 目的文件夹： d:/test -&gt; 自定义 选择链路 -&gt; Link-1 -&gt; 添加 不符合发送条件的文件备份至 -&gt; 当前文档目录 查看日志及接收端是否发送成功。 如果发现隔离发送进行中一直在校验E文本规范，不发送的情况，重新打开发送端工具。 虚拟化纵向㉛创建虚拟化纵向[电力纵向]PS_1，开机㉜创建虚拟化纵向[电力纵向]PS_2，开机㉝创建虚拟路由器R，配置：网卡1：192.168.1.0/24 网卡2：192.168.2.0/24，开机㉞创建虚拟交换机 Switch_1，开机㉟创建虚拟主机[win7-2]PC_3，开机㊱创建虚拟主机[win7-2]PC_2，开机㊲创建虚拟主机[win7-2]PC_1，开机㊳创建虚拟USB-KEY UK_1，插入PS_1㊴创建虚拟USB-KEY UK_2，插入PS_2㊵配置Switch_1：1-2口配置VLAN1003㊶配置PC_3IP地址：169.254.200.201 255.255.255.0 169.254.200.1㊷配置PC_2IP地址：169.254.200.201 255.255.255.0 169.254.200.1㊸配置PC_1IP地址：192.168.2.20 255.255.255.0 192.168.2.1㊹创建网线，连接PS_1网口0与Switch_2网口4㊺创建网线，连接PS_1网口1与R网口1㊻创建网线，连接PS_2网口1与R网口2㊼创建网线，连接PS_2网口0与Switch_1网口1㊽创建网线，连接PC_1网卡0与Switch_1网口2㊾创建网线，连接PC_2网卡0与PS_2网卡4㊿创建网线，连接PC_3网卡0与PS_1网卡4(51)PC_3本地安装及配置纵向PS_1：安装纵向管理工具： 目录地址：d:/software/PSTunnel2000加密装置千兆管理工具.exe 屏幕分辨率 -&gt; 1024×768添加新操作员： 管理工具 -&gt; 右键 -&gt; 以管理员身份运行 操作员：init 密码：Tun-2000 设备IP：169.254.200.200 -&gt; 操作员 -&gt; 操作员管理 -&gt; 添加：+操作员： user -&gt; 确定导出导入证书： 初始化 -&gt; 生成设备密钥及证书请求 -&gt; 下一步 -&gt; 填写省/市/设备名 -&gt; 下一步 -&gt; 生成.csr证书 证书转换。使用证书工具2.0将.csr证书转换为.cer证书。传入对端。 证书 -&gt; 证书导入 -&gt; 远程设备证书 -&gt; 选择证书 -&gt; 确定VLAN配置： 本地ETH1：192.168.1.50 255.255.255.0 VLAN：0 远程ETH1：192.168.2.50 255.255.255.0 VLAN：0路由配置： 目的地址：192.168.2.50 子网掩码：255.255.255.0 网关地址：192.168.1.1安全隧道: -&gt; 恢复隧道配置 -&gt; 11.pbak -&gt; 配置写入装置 -&gt; 确定 -&gt; 修改隧道 -&gt; 确定 隧道名标识： 11 -&gt; 不支持修改 本地IP： 192.168.1.50 -&gt; 本地ETH1 远程IP： 192.168.2.50 -&gt; 远程ETH1 255.255.255.0 0.0.0.0 0.0.0.0 MTU： 1500安全策略： -&gt; 恢复策略配置 -&gt; 11.pbak -&gt; 配置写入装置 -&gt; 确定 -&gt; 修改策略 -&gt; 确定 标识id：0 本地IP： 192.168.1.50 -&gt; 本地ETH1 远程IP： 192.168.2.50 -&gt; 远程ETH1 本地起始IP：192.168.1.1 本地终止IP：192.168.1.254 远程起始IP：192.168.2.1 远程终止IP：192.168.2.254 方向：双向 重置隧道： -&gt; 管理 -&gt; 重置隧道 -&gt; OPENED 注意： *新纵向的用户名,密码：root, Tun-2000 纵向内部查询路由命令：1$ monipead.arm -all (52)PC_2本地安装及配置纵向PS_2：配置与以上相同，反向。 (53)PC_4配置静态路由：1route add -p 192.168.2.0 mask 255.255.255.0 192.168.1.1 (54)PC_1配置静态路由：1route add -p 192.168.1.0 mask 255.255.255.0 192.168.2.1 (55)验证纵向的功能：使用PC_4 ping PC_1 1ping 192.168.2.50 查看PS_2的eth1口是否抓到ESP报文包。]]></content>
      <categories>
        <category>Kedong</category>
      </categories>
      <tags>
        <tag>kedong</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[虚拟化隔离安装配置说明]]></title>
    <url>%2F2019%2F06%2F21%2F%E8%99%9A%E6%8B%9F%E5%8C%96%E9%9A%94%E7%A6%BB%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE%E6%89%8B%E5%86%8C%2F</url>
    <content type="text"><![CDATA[虚拟化隔离安装配置说明创建纵向隔离组件[反向隔离] SW。创建主机组件[win7] PC。SW与PC通过串口线连接。 配置管理工具。打开QT界面隔离管理工具。 前提：windows虚拟机已经安装vc_redist.2015.x64 且QT管理工具中包含vcruntime140.dll，libcrypto-1_1.dll。 用户名：admin 密码：111111 串口登录。 串口：COM1。 频率：115200。 √ 反向隔离。 设备配置 -&gt; 基本配置 管理IP： 无 -&gt; 不填 设备名称： dev -&gt; 自定义 ETH0协商IP： 192.168.1.100 -&gt; 与发送端须配置成相同网段 ETH1协商IP： 0.0.0.0 -&gt; 可不做配置 规则配置 -&gt; 主机信息1 -&gt; 添加 √ IP和MAC地址绑定 主机名： sender -&gt; 自定义 MAC1： FA:16:3E:40:11:01 -&gt; 主机网卡MAC地址 主机IP： 192.168.1.10 -&gt; 主机网卡ip地址 主机虚拟IP1：ETH0协商IP与主机同网段时自动生成,不通网段时填写一个ETH0所在网段下任意IP地址 主机虚拟ip2：可以不填 规则配置 -&gt; 主机信息2 -&gt; 添加 √ IP和MAC地址绑定 主机名： receiver -&gt; 自定义 MAC1： FA:16:3E:40:11:02 -&gt; 主机网卡MAC地址 主机IP： 192.168.1.20 -&gt; 主机网卡ip地址 主机虚拟IP1：ETH0协商IP与主机同网段时自动生成,不通网段时填写一个ETH0所在网段下任意IP地址 主机虚拟ip2：可以不填 规则配置 -&gt; 连接信息 规则名： rule -&gt; 自定义 (内网)IP地址： 192.100.1.10 (内网)虚拟IP： 192.100.1.10 (内网)网口号： ETH0/网口0 (外网)IP地址： 192.100.1.20 (外网)虚拟IP： 192.100.1.20 (外网)网口号： ETH0/网口0 协议： UDP（反向） TCP正向 证书互导 设备密钥 √ RSA -&gt; 导出设备证书文件 -&gt; dev.cer 发送端证书 -&gt; 添加 发送端IP： 192.168.1.14 -&gt; 此处为发送端IP，不为虚拟IP 证书标识： sender.cer 管理工具配置好隔离之后，正反向隔离需重启生效。主机信息位置：/etc/mac.conf连接信息位置：/etc/rules 传输软件配置 检查config目录下是否有**.p12文件，若存在，则删除。 启动传输软件。 Stonewall-2000-Send.jar 右键 -&gt; 以管理员身份运行 -&gt; 命令行 -&gt; 12java -jar Stonewall-2000-Send.jar设置密码口令保护窗口如果报错，需先运行jreUpdate1.8.jar 出现提示：系统检测到密钥尚未存在，是否需要生成密钥？ -&gt; 是 创建登录口令：123456。 确定，提示操作成功。 登录窗口： 操作员名称： administrator 操作员密码： 12345678 密钥保护口令： 123456 管理 -&gt; 密钥管理 -&gt; 导出密钥。 -&gt; sender.cer 设定 -&gt; 配置加密隧道 隧道名称： Tunnel-1 隧道的协商IP地址： 192.168.1.100 -&gt; 设备ETH0协商IP 隧道的协商端口： 4558 隧道每次通过的文件数： 100 隔离设备证书路径： ../.. dev.cer 设定 -&gt; 配置链路信息 链路名称： Link-1 目的IP地址： 192.168.1.13 -&gt; 接收端IP地址，正反向隔离在相同环境的情况下，配置虚拟IP。 目的端口： 7777 发送失败等待周期(秒)： 30 使用隧道： Tunnel-1]]></content>
      <categories>
        <category>Eletric Power</category>
      </categories>
      <tags>
        <tag>eletric power</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Openstack Too many open files 错误与解决办法]]></title>
    <url>%2F2019%2F06%2F20%2FTooManyFiles%2F</url>
    <content type="text"><![CDATA[Openstack WebUI页面无法打开，页面报500错误，查看httpd-&gt;error_log日志报如下错误: 1234567891011[Tue Apr 02 14:01:05.658276 2019] [:error] [pid 9245] File "/usr/lib/python2.7/site-packages/requests/sessions.py", line 518, in request[Tue Apr 02 14:01:05.658280 2019] [:error] [pid 9245] File "/usr/lib/python2.7/site-packages/requests/sessions.py", line 639, in send[Tue Apr 02 14:01:05.658284 2019] [:error] [pid 9245] File "/usr/lib/python2.7/site-packages/requests/adapters.py", line 438, in send[Tue Apr 02 14:01:05.658287 2019] [:error] [pid 9245] File "/usr/lib/python2.7/site-packages/requests/packages/urllib3/connectionpool.py", line 588, in urlopen[Tue Apr 02 14:01:05.658291 2019] [:error] [pid 9245] File "/usr/lib/python2.7/site-packages/requests/packages/urllib3/connectionpool.py", line 241, in _get_conn[Tue Apr 02 14:01:05.658296 2019] [:error] [pid 9245] File "/usr/lib/python2.7/site-packages/requests/packages/urllib3/util/connection.py", line 27, in is_connection_dropped[Tue Apr 02 14:01:05.658300 2019] [:error] [pid 9245] File "/usr/lib/python2.7/site-packages/requests/packages/urllib3/util/wait.py", line 33, in wait_for_read[Tue Apr 02 14:01:05.658304 2019] [:error] [pid 9245] File "/usr/lib/python2.7/site-packages/requests/packages/urllib3/util/wait.py", line 22, in _wait_for_io_events[Tue Apr 02 14:01:05.658308 2019] [:error] [pid 9245] File "/usr/lib/python2.7/site-packages/requests/packages/urllib3/util/selectors.py", line 581, in DefaultSelector[Tue Apr 02 14:01:05.658312 2019] [:error] [pid 9245] File "/usr/lib/python2.7/site-packages/requests/packages/urllib3/util/selectors.py", line 394, in __init__[Tue Apr 02 14:01:05.658316 2019] [:error] [pid 9245] IOError: [Errno 24] Too many open files 解决方式：修改操作系统打开的文件数；登录到Controller节点，执行: 1234567891011121314151617[root@controller ~]# ulimit -acore file size (blocks, -c) 0data seg size (kbytes, -d) unlimitedscheduling priority (-e) 0file size (blocks, -f) unlimitedpending signals (-i) 60587max locked memory (kbytes, -l) 64max memory size (kbytes, -m) unlimitedopen files (-n) 1024pipe size (512 bytes, -p) 8POSIX message queues (bytes, -q) 819200real-time priority (-r) 0stack size (kbytes, -s) 8192cpu time (seconds, -t) unlimitedmax user processes (-u) 60587virtual memory (kbytes, -v) unlimitedfile locks (-x) unlimited 系统默认设置为1024。 使用命令查看当前打开文件数: 12[root@controller ~]# lsof | wc -l174911 修改vim /etc/security/limits.conf，在文件最后加入如下信息： 12* soft nofile 1024000* hard nofile 1024000 *表示所有用户，修改后重启服务器，配置生效。]]></content>
      <categories>
        <category>Openstack</category>
      </categories>
      <tags>
        <tag>openstack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[虚拟化纵向安装配置说明]]></title>
    <url>%2F2019%2F06%2F20%2F%E8%99%9A%E6%8B%9F%E5%8C%96%E7%BA%B5%E5%90%91%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE%E6%89%8B%E5%86%8C%2F</url>
    <content type="text"><![CDATA[虚拟化纵向安装配置说明创建纵向组件[电力纵向] PS。创建主机组件[win7] PC。创建UKey组件 UKey。UKey插入PS。 配置PC。 IP地址：169.254.200.201 掩码：255.255.255.0 网关：169.254.200.1 PC网卡0与PS网卡4连接网线。PC本地安装纵向管理工具。 目录地址：d:/software/PSTunnel2000加密装置千兆管理工具.exe 添加新操作员。 右键 -&gt; 以管理员身份运行 -&gt; 操作员 -&gt; 操作员管理 -&gt; 添加：+操作员： user -&gt; 确定 导出导入证书。 -&gt; 初始化 -&gt; 生成设备密钥及证书请求 -&gt; 下一步 -&gt; 填写省/市/设备名 -&gt; 下一步 -&gt; 生成.csr证书 证书转换。使用证书工具2.0将.csr证书转换为.cer证书。传入对端。 -&gt; 证书 -&gt; 证书导入 -&gt; 远程设备证书 -&gt; 选择证书 -&gt; 确定 VLAN配置。 本地ETH1：192.168.1.100 255.255.255.0 VLAN：0 远程ETH1：192.168.1.200 255.255.255.0 VLAN：0 安全隧道。 -&gt; 恢复隧道配置 -&gt; 11.pbak -&gt; 配置写入装置 -&gt; 确定 -&gt; 修改隧道 -&gt; 确定 隧道名标识： 11 -&gt; 不支持修改 本地IP： 192.168.1.100 -&gt; 本地ETH1 远程IP： 192.168.1.200 -&gt; 远程ETH1 255.255.255.0 0.0.0.0 0.0.0.0 MTU： 1500 安全策略。 -&gt; 恢复策略配置 -&gt; 11.pbak -&gt; 配置写入装置 -&gt; 确定 -&gt; 修改策略 -&gt; 确定 标识id：0 本地IP： 192.168.1.100 -&gt; 本地ETH1 远程IP： 192.168.1.200 -&gt; 远程ETH1 本地起始IP：192.168.1.1 本地终止IP：192.168.1.254 远程起始IP：192.168.1.1 远程终止IP：192.168.1.254 方向：双向 重置隧道。 -&gt; 管理 -&gt; 重置隧道 -&gt; OPENED 注意： *新纵向的用户名,密码：root, Tun-2000 纵向内部查询路由命令：1$ monipead.arm -all]]></content>
      <categories>
        <category>Eletric Power</category>
      </categories>
      <tags>
        <tag>eletric power</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Openstack Queens 环境搭建（七）Horizon服务]]></title>
    <url>%2F2019%2F03%2F19%2FOpenstackQueens7%2F</url>
    <content type="text"><![CDATA[Controller节点：安装及配置：12345678910111213141516171819202122# yum install openstack-dashboard# vi /etc/openstack-dashboard/local_settingsOPENSTACK_HOST = "controller"ALLOWED_HOSTS = ['*']SESSION_ENGINE = 'django.contrib.sessions.backends.cache'CACHES = &#123; 'default': &#123; 'BACKEND': 'django.core.cache.backends.memcached.MemcachedCache', 'LOCATION': 'controller:11211', &#125;&#125;OPENSTACK_KEYSTONE_URL = "http://%s:5000/v3" % OPENSTACK_HOSTOPENSTACK_KEYSTONE_MULTIDOMAIN_SUPPORT = TrueOPENSTACK_API_VERSIONS = &#123; "identity": 3, "image": 2, "volume": 2,&#125;OPENSTACK_KEYSTONE_DEFAULT_DOMAIN = "Default"OPENSTACK_KEYSTONE_DEFAULT_ROLE = "user"TIME_ZONE = "Asia/Shanghai" 123# vi /etc/httpd/conf.d/openstack-dashboard.conf 在文件开头添加WSGIApplicationGroup %&#123;GLOBAL&#125;... 完成安装：1# systemctl restart httpd.service memcached.service 使用 http://controller/dashboard 上的Web浏览器访问Dashboard。]]></content>
      <categories>
        <category>Openstack</category>
      </categories>
      <tags>
        <tag>openstack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Openstack Queens 环境搭建（六）Neutron服务]]></title>
    <url>%2F2019%2F03%2F19%2FOpenstackQueens6%2F</url>
    <content type="text"><![CDATA[Controller节点：Neutron服务安装网络类型：vxlanLayer2 二层插件采用：openvswitch 创建neutron数据库，授予权限：123456$ mysql -u root -pMariaDB [(none)] CREATE DATABASE neutron;MariaDB [(none)]&gt; GRANT ALL PRIVILEGES ON neutron.* TO 'neutron'@'localhost' IDENTIFIED BY '123456';MariaDB [(none)]&gt; GRANT ALL PRIVILEGES ON neutron.* TO 'neutron'@'%' IDENTIFIED BY '123456';MariaDB [(none)]&gt; exit; 创建neutron用户：1234567891011121314151617$ . admin-openrc$ openstack user create --domain default --password-prompt neutronUser Password: 123456Repeat User Password: 123456+---------------------+----------------------------------+| Field | Value |+---------------------+----------------------------------+| domain_id | default || enabled | True || id | 463fd14bf95b4cc49c0378623de56ffa || name | neutron || options | &#123;&#125; || password_expires_at | None |+---------------------+----------------------------------+$ openstack role add --project service --user neutron admin 创建neutron服务实体：12345678910$ openstack service create --name neutron --description "OpenStack Networking" network+-------------+----------------------------------+| Field | Value |+-------------+----------------------------------+| description | OpenStack Networking || enabled | True || id | e10e48790ede425ea81e1a62250f124a || name | neutron || type | network |+-------------+----------------------------------+ 创建网络服务API端点：1234567891011121314$ openstack endpoint create --region RegionOne network public http://controller:9696+--------------+----------------------------------+| Field | Value |+--------------+----------------------------------+| enabled | True || id | f688ed8f1bf340d78794b600fa512145 || interface | public || region | RegionOne || region_id | RegionOne || service_id | e10e48790ede425ea81e1a62250f124a || service_name | neutron || service_type | network || url | http://controller:9696 |+--------------+----------------------------------+ 1234567891011121314$ openstack endpoint create --region RegionOne network internal http://controller:9696+--------------+----------------------------------+| Field | Value |+--------------+----------------------------------+| enabled | True || id | 571a008230c54cf8bcb1e38a75787c3f || interface | internal || region | RegionOne || region_id | RegionOne || service_id | e10e48790ede425ea81e1a62250f124a || service_name | neutron || service_type | network || url | http://controller:9696 |+--------------+----------------------------------+ 1234567891011121314$ openstack endpoint create --region RegionOne network admin http://controller:9696+--------------+----------------------------------+| Field | Value |+--------------+----------------------------------+| enabled | True || id | a8d654c1c878423789aab3fa7cf634cb || interface | admin || region | RegionOne || region_id | RegionOne || service_id | e10e48790ede425ea81e1a62250f124a || service_name | neutron || service_type | network || url | http://controller:9696 |+--------------+----------------------------------+ 安装及配置：12345678910111213141516171819202122232425262728293031323334# yum install openstack-neutron openstack-neutron-ml2 openstack-neutron-openvswitch ebtables# vi /etc/neutron/neutron.conf[DEFAULT]auth_strategy = keystonecore_plugin = ml2service_plugins = routerallow_overlapping_ips = Truetransport_url = rabbit://openstack:123456@controllernotify_nova_on_port_status_changes = truenotify_nova_on_port_data_changes = true[database]connection = mysql+pymysql://neutron:123456@controller/neutron[keystone_authtoken]auth_uri = http://controller:5000auth_url = http://controller:35357memcached_servers = controller:11211auth_type = passwordproject_domain_name = defaultuser_domain_name = defaultproject_name = serviceusername = neutronpassword = 123456[nova]auth_url = http://controller:35357auth_type = passwordproject_domain_name = defaultuser_domain_name = defaultregion_name = RegionOneproject_name = serviceusername = novapassword = 123456[oslo_concurrency]lock_path = /var/lib/neutron/tmp 1234567891011121314# vi /etc/neutron/plugins/ml2/ml2_conf.ini[ml2]type_drivers = flat,vlan,vxlantenant_network_types = vxlanmechanism_drivers = openvswitch,l2populationextension_drivers = port_security[ml2_type_flat]flat_networks = provider[ml2_type_vlan]#network_vlan_ranges = provider:1:1000[ml2_type_vxlan]vni_ranges = 1:1000[securitygroup]enable_ipset = true 123# vi /etc/sysctl.confnet.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1 1234# vi /etc/neutron/l3_agent.ini[DEFAULT]interface_driver = openvswitchexternal_network_bridge = 12345# vi /etc/neutron/dhcp_agent.ini[DEFAULT]interface_driver = openvswitchdhcp_driver = neutron.agent.linux.dhcp.Dnsmasqenable_isolated_metadata = true 1234# vi /etc/neutron/metadata_agent.ini[DEFAULT]nova_metadata_host = controllermetadata_proxy_shared_secret = 123456 123456789# vi /etc/neutron/plugins/ml2/openvswitch_agent.ini[agent]tunnel_types = vxlanl2_population = True[ovs]bridge_mappings = provider:br-providerlocal_ip = 10.0.0.11[securitygroup]firewall_driver = iptables_hybrid 完成安装：123456789101112131415161718# ln -s /etc/neutron/plugins/ml2/ml2_conf.ini /etc/neutron/plugin.ini# su -s /bin/sh -c "neutron-db-manage --config-file /etc/neutron/neutron.conf --config-file /etc/neutron/plugins/ml2/ml2_conf.ini upgrade head" neutron# systemctl restart openstack-nova-api.service# systemctl start neutron-server.service# systemctl start neutron-openvswitch-agent.service# ovs-vsctl add-br br-provider# ifconfig eth0 0.0.0.0# ifconfig br-provider 192.100.10.160/24# route add default gw 192.100.10.1# systemctl restart neutron-server.service# systemctl restart neutron-openvswitch-agent.service# systemctl enable neutron-server.service neutron-openvswitch-agent.service neutron-dhcp-agent.service neutron-metadata-agent.service# systemctl start neutron-dhcp-agent.service neutron-metadata-agent.service# systemctl enable neutron-l3-agent.service# systemctl start neutron-l3-agent.service Compute节点：安装及配置：123456789101112131415161718# yum install openstack-neutron-openvswitch ebtables ipset# vi /etc/neutron/neutron.conf[DEFAULT]transport_url = rabbit://openstack:123456@controllerauth_strategy = keystone[keystone_authtoken]auth_uri = http://controller:5000auth_url = http://controller:35357memcached_servers = controller:11211auth_type = passwordproject_domain_name = defaultuser_domain_name = defaultproject_name = serviceusername = neutronpassword = 123456[oslo_concurrency]lock_path = /var/lib/neutron/tmp 12345678# vi /etc/neutron/plugins/ml2/openvswitch_agent.ini[ovs]local_ip = 10.0.0.21[agent]tunnel_types = vxlanl2_population = True# systemctl restart neutron-openvswitch-agent.service 123456789101112# vi /etc/nova/nova.conf...[neutron]url = http://controller:9696auth_url = http://controller:35357auth_type = passwordproject_domain_name = defaultuser_domain_name = defaultregion_name = RegionOneproject_name = serviceusername = neutronpassword = 123456 完成安装：123# systemctl restart openstack-nova-compute.service# systemctl enable neutron-openvswitch-agent.service# systemctl start neutron-openvswitch-agent.service Controller节点：验证操作：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051$ . admin-openrc$ openstack extension list --network+----------------------------------------------------------------------------------------------+---------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------+| Name | Alias | Description |+----------------------------------------------------------------------------------------------+---------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------+| Default Subnetpools | default-subnetpools | Provides ability to mark and use a subnetpool as the default. || Availability Zone | availability_zone | The availability zone extension. || Network Availability Zone | network_availability_zone | Availability zone support for network. || Auto Allocated Topology Services | auto-allocated-topology | Auto Allocated Topology Services. || Neutron L3 Configurable external gateway mode | ext-gw-mode | Extension of the router abstraction for specifying whether SNAT should occur on the external gateway || Port Binding | binding | Expose port bindings of a virtual port to external application || agent | agent | The agent management extension. || Subnet Allocation | subnet_allocation | Enables allocation of subnets from a subnet pool || L3 Agent Scheduler | l3_agent_scheduler | Schedule routers among l3 agents || Tag support | tag | Enables to set tag on resources. || Neutron external network | external-net | Adds external network attribute to network resource. || Tag support for resources with standard attribute: trunk, policy, security_group, floatingip | standard-attr-tag | Enables to set tag on resources with standard attribute. || Neutron Service Flavors | flavors | Flavor specification for Neutron advanced services. || Network MTU | net-mtu | Provides MTU attribute for a network resource. || Network IP Availability | network-ip-availability | Provides IP availability data for each network and subnet. || Quota management support | quotas | Expose functions for quotas management per tenant || If-Match constraints based on revision_number | revision-if-match | Extension indicating that If-Match based on revision_number is supported. || HA Router extension | l3-ha | Adds HA capability to routers. || Provider Network | provider | Expose mapping of virtual networks to physical networks || Multi Provider Network | multi-provider | Expose mapping of virtual networks to multiple physical networks || Quota details management support | quota_details | Expose functions for quotas usage statistics per project || Address scope | address-scope | Address scopes extension. || Neutron Extra Route | extraroute | Extra routes configuration for L3 router || Network MTU (writable) | net-mtu-writable | Provides a writable MTU attribute for a network resource. || Subnet service types | subnet-service-types | Provides ability to set the subnet service_types field || Resource timestamps | standard-attr-timestamp | Adds created_at and updated_at fields to all Neutron resources that have Neutron standard attributes. || Neutron 服务类型管理 | service-type | 用于为 Neutron 高级服务检索服务提供程序的 API || Router Flavor Extension | l3-flavors | Flavor support for routers. || Port Security | port-security | Provides port security || Neutron Extra DHCP options | extra_dhcp_opt | Extra options configuration for DHCP. For example PXE boot options to DHCP clients can be specified (e.g. tftp-server, server-ip-address, bootfile-name) || Resource revision numbers | standard-attr-revisions | This extension will display the revision number of neutron resources. || Pagination support | pagination | Extension that indicates that pagination is enabled. || Sorting support | sorting | Extension that indicates that sorting is enabled. || security-group | security-group | The security groups extension. || DHCP Agent Scheduler | dhcp_agent_scheduler | Schedule networks among dhcp agents || Router Availability Zone | router_availability_zone | Availability zone support for router. || RBAC Policies | rbac-policies | Allows creation and modification of policies that control tenant access to resources. || Tag support for resources: subnet, subnetpool, port, router | tag-ext | Extends tag support to more L2 and L3 resources. || standard-attr-description | standard-attr-description | Extension to add descriptions to standard attributes || IP address substring filtering | ip-substring-filtering | Provides IP address substring filtering when listing ports || Neutron L3 Router | router | Router abstraction for basic L3 forwarding between L2 Neutron networks and access to external networks via a NAT gateway. || Allowed Address Pairs | allowed-address-pairs | Provides allowed address pairs || project_id field enabled | project-id | Extension that indicates that project_id field is enabled. || Distributed Virtual Router | dvr | Enables configuration of Distributed Virtual Routers. |+----------------------------------------------------------------------------------------------+---------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------+ 12345678910$ openstack network agent list+--------------------------------------+--------------------+------------+-------------------+-------+-------+---------------------------+| ID | Agent Type | Host | Availability Zone | Alive | State | Binary |+--------------------------------------+--------------------+------------+-------------------+-------+-------+---------------------------+| 0fcf4aa9-3592-4552-9b4c-f2b55e23ef6b | DHCP agent | controller | nova | :-) | UP | neutron-dhcp-agent || 1a08e5eb-d867-4697-850d-bd2400134162 | Metadata agent | controller | None | :-) | UP | neutron-metadata-agent || 9a33be1e-61bd-4d6b-9ee1-bda6dc7b44cd | Linux bridge agent | controller | None | :-) | UP | neutron-linuxbridge-agent || bfdb443d-feee-4006-8618-558b73c3c4a2 | L3 agent | controller | nova | :-) | UP | neutron-l3-agent || ce5abc8d-504a-4164-ae0f-801e56a06653 | Linux bridge agent | compute | None | :-) | UP | neutron-linuxbridge-agent |+--------------------------------------+--------------------+------------+-------------------+-------+-------+---------------------------+]]></content>
      <categories>
        <category>Openstack</category>
      </categories>
      <tags>
        <tag>openstack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Openstack Queens 环境搭建（五）Nova服务]]></title>
    <url>%2F2019%2F03%2F19%2FOpenstackQueens5%2F</url>
    <content type="text"><![CDATA[Controller节点：创建 nova_api, nova,和 nova_cell0 的数据库，授予权限：1234567891011121314$ mysql -u root -pMariaDB [(none)]&gt;CREATE DATABASE nova_api;MariaDB [(none)]&gt; CREATE DATABASE nova;MariaDB [(none)]&gt; CREATE DATABASE nova_cell0;MariaDB [(none)]&gt; GRANT ALL PRIVILEGES ON nova_api.* TO 'nova'@'localhost' IDENTIFIED BY '123456';MariaDB [(none)]&gt; GRANT ALL PRIVILEGES ON nova_api.* TO 'nova'@'%' IDENTIFIED BY '123456';MariaDB [(none)]&gt; GRANT ALL PRIVILEGES ON nova.* TO 'nova'@'localhost' IDENTIFIED BY '123456';MariaDB [(none)]&gt; GRANT ALL PRIVILEGES ON nova.* TO 'nova'@'%' IDENTIFIED BY '123456';MariaDB [(none)]&gt; GRANT ALL PRIVILEGES ON nova_cell0.* TO 'nova'@'localhost' IDENTIFIED BY '123456';MariaDB [(none)]&gt; GRANT ALL PRIVILEGES ON nova_cell0.* TO 'nova'@'%' IDENTIFIED BY '123456';MariaDB [(none)]&gt; exit; 创建nova用户：1234567891011121314151617$ . admin-openrc$ openstack user create --domain default --password-prompt novaUser Password: 123456Repeat User Password: 123456+---------------------+----------------------------------+| Field | Value |+---------------------+----------------------------------+| domain_id | default || enabled | True || id | 81f1d5dfad5a42bb806d197ceb9881ce || name | nova || options | &#123;&#125; || password_expires_at | None |+---------------------+----------------------------------+$ openstack role add --project service --user nova admin 创建nova服务实体：12345678910$ openstack service create --name nova --description "OpenStack Compute" compute+-------------+----------------------------------+| Field | Value |+-------------+----------------------------------+| description | OpenStack Compute || enabled | True || id | 3e011d345e4442fe8a232ab5ab1f8323 || name | nova || type | compute |+-------------+----------------------------------+ 创建Compute API服务端点：1234567891011121314$ openstack endpoint create --region RegionOne compute public http://controller:8774/v2.1+--------------+----------------------------------+| Field | Value |+--------------+----------------------------------+| enabled | True || id | 343b6a8fc9564623aca0097b2383650d || interface | public || region | RegionOne || region_id | RegionOne || service_id | 3e011d345e4442fe8a232ab5ab1f8323 || service_name | nova || service_type | compute || url | http://controller:8774/v2.1 |+--------------+----------------------------------+ 1234567891011121314$ openstack endpoint create --region RegionOne compute internal http://controller:8774/v2.1+--------------+----------------------------------+| Field | Value |+--------------+----------------------------------+| enabled | True || id | 3458cf55ac8b44d58c949fe88bf9afe3 || interface | internal || region | RegionOne || region_id | RegionOne || service_id | 3e011d345e4442fe8a232ab5ab1f8323 || service_name | nova || service_type | compute || url | http://controller:8774/v2.1 |+--------------+----------------------------------+ 1234567891011121314$ openstack endpoint create --region RegionOne compute admin http://controller:8774/v2.1+--------------+----------------------------------+| Field | Value |+--------------+----------------------------------+| enabled | True || id | 9f9115389c2a49a2874761b92c849bb0 || interface | admin || region | RegionOne || region_id | RegionOne || service_id | 3e011d345e4442fe8a232ab5ab1f8323 || service_name | nova || service_type | compute || url | http://controller:8774/v2.1 |+--------------+----------------------------------+ 创建Placement服务相关：123456789101112131415$ openstack user create --domain default --password-prompt placementUser Password: 123456Repeat User Password: 123456+---------------------+----------------------------------+| Field | Value |+---------------------+----------------------------------+| domain_id | default || enabled | True || id | 74870bc86a7c4108869c620099bffc30 || name | placement || options | &#123;&#125; || password_expires_at | None |+---------------------+----------------------------------+$ openstack role add --project service --user placement admin 12345678910$ openstack service create --name placement --description "Placement API" placement+-------------+----------------------------------+| Field | Value |+-------------+----------------------------------+| description | Placement API || enabled | True || id | bbd270a97c3a499fb73765120094e9da || name | placement || type | placement |+-------------+----------------------------------+ 1234567891011121314$ openstack endpoint create --region RegionOne placement public http://controller:8778+--------------+----------------------------------+| Field | Value |+--------------+----------------------------------+| enabled | True || id | d79b3b62302a4055924762ac676fc9b4 || interface | public || region | RegionOne || region_id | RegionOne || service_id | bbd270a97c3a499fb73765120094e9da || service_name | placement || service_type | placement || url | http://controller:8778 |+--------------+----------------------------------+ 1234567891011121314$ openstack endpoint create --region RegionOne placement internal http://controller:8778+--------------+----------------------------------+| Field | Value |+--------------+----------------------------------+| enabled | True || id | 5424919fbee34a7a92946c607706b38a || interface | internal || region | RegionOne || region_id | RegionOne || service_id | bbd270a97c3a499fb73765120094e9da || service_name | placement || service_type | placement || url | http://controller:8778 |+--------------+----------------------------------+ 1234567891011121314$ openstack endpoint create --region RegionOne placement admin http://controller:8778+--------------+----------------------------------+| Field | Value |+--------------+----------------------------------+| enabled | True || id | d9d5626cdb5442ac91dff8c1588f4726 || interface | admin || region | RegionOne || region_id | RegionOne || service_id | bbd270a97c3a499fb73765120094e9da || service_name | placement || service_type | placement || url | http://controller:8778 |+--------------+----------------------------------+ 安装和配置：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556# yum install openstack-nova-api openstack-nova-conductor openstack-nova-console openstack-nova-novncproxy openstack-nova-scheduler openstack-nova-placement-api# vi /etc/nova/nova.conf[DEFAULT]my_ip=192.100.10.160use_neutron=truefirewall_driver=nova.virt.firewall.NoopFirewallDriverenabled_apis=osapi_compute,metadatatransport_url=rabbit://openstack:123456@controller[api]auth_strategy=keystone[api_database]connection = mysql+pymysql://nova:123456@controller/nova_api[database]connection = mysql+pymysql://nova:123456@controller/nova[glance]api_servers = http://controller:9292[keystone_authtoken]auth_url = http://controller:5000/v3memcached_servers = controller:11211auth_type = passwordproject_domain_name = defaultuser_domain_name = defaultproject_name = serviceusername = novapassword = 123456[libvirt]#virt_type=kvm[neutron]url = http://controller:9696auth_url = http://controller:35357auth_type = passwordproject_domain_name = defaultuser_domain_name = defaultregion_name = RegionOneproject_name = serviceusername = neutronpassword = 123456service_metadata_proxy = truemetadata_proxy_shared_secret = 123456[oslo_concurrency]lock_path=/var/lib/nova/tmp[placement]os_region_name = RegionOneproject_domain_name = Defaultproject_name = serviceauth_type = passworduser_domain_name = Defaultauth_url = http://controller:5000/v3username = placementpassword = 123456[vnc]enabled=trueserver_listen=$my_ipserver_proxyclient_address=$my_ip#novncproxy_base_url=http://127.0.0.1:6080/vnc_auto.html 12345678910# vi /etc/httpd/conf.d/00-nova-placement-api.conf 在最下方加入&lt;Directory /usr/bin&gt; &lt;IfVersion &gt;= 2.4&gt; Require all granted &lt;/IfVersion&gt; &lt;IfVersion &lt; 2.4&gt; Order allow,deny Allow from all &lt;/IfVersion&gt;&lt;/Directory&gt; 完成安装：1234567891011121314# systemctl restart httpd# su -s /bin/sh -c "nova-manage api_db sync" nova# su -s /bin/sh -c "nova-manage cell_v2 map_cell0" nova# su -s /bin/sh -c "nova-manage cell_v2 create_cell --name=cell1 --verbose" nova# su -s /bin/sh -c "nova-manage db sync" nova# nova-manage cell_v2 list_cells+-------+--------------------------------------+------------------------------------+-------------------------------------------------+| 名称 | UUID | Transport URL | 数据库连接 |+-------+--------------------------------------+------------------------------------+-------------------------------------------------+| cell0 | 00000000-0000-0000-0000-000000000000 | none:/ | mysql+pymysql://nova:****@controller/nova_cell0 || cell1 | c795b2eb-4814-4fe7-b9ff-090a1b1b2be5 | rabbit://openstack:****@controller | mysql+pymysql://nova:****@controller/nova |+-------+--------------------------------------+------------------------------------+-------------------------------------------------+ 12# systemctl enable openstack-nova-api.service openstack-nova-consoleauth.service openstack-nova-scheduler.service openstack-nova-conductor.service openstack-nova-novncproxy.service# systemctl start openstack-nova-api.service openstack-nova-consoleauth.service openstack-nova-scheduler.service openstack-nova-conductor.service openstack-nova-novncproxy.service Compute节点：安装和配置：1234567891011121314151617181920212223242526272829303132333435363738# yum install openstack-nova-compute# vi /etc/nova/nova.conf[DEFAULT]my_ip = 192.100.10.161enabled_apis = osapi_compute,metadatause_neutron = Truefirewall_driver = nova.virt.firewall.NoopFirewallDrivertransport_url = rabbit://openstack:123456@controller[api]auth_strategy = keystone[vnc]enabled = Trueserver_listen = 0.0.0.0server_proxyclient_address = $my_ipnovncproxy_base_url = http://controller:6080/vnc_auto.html[glance]api_servers = http://controller:9292[oslo_concurrency]lock_path = /var/lib/nova/tmp[placement]os_region_name = RegionOneproject_domain_name = Defaultproject_name = serviceauth_type = passworduser_domain_name = Defaultauth_url = http://controller:5000/v3username = placementpassword = 123456[keystone_authtoken]auth_url = http://controller:5000/v3memcached_servers = controller:11211auth_type = passwordproject_domain_name = defaultuser_domain_name = defaultproject_name = serviceusername = novapassword = 123456 完成安装12# systemctl enable libvirtd.service openstack-nova-compute.service# systemctl start libvirtd.service openstack-nova-compute.service Controller节点：将计算节点添加到cell数据库：12345678$ . admin-openrc$ openstack compute service list --service nova-compute+----+--------------+-----------------------+------+---------+-------+----------------------------+| ID | Binary | Host | Zone | Status | State | Updated At |+----+--------------+-----------------------+------+---------+-------+----------------------------+| 9 | nova-compute | localhost.localdomain | nova | enabled | up | 2018-09-13T02:59:06.000000 |+----+--------------+-----------------------+------+---------+-------+----------------------------+ 发现计算主机：123456789101112131415# su -s /bin/sh -c "nova-manage cell_v2 discover_hosts --verbose" nova/usr/lib/python2.7/site-packages/oslo_db/sqlalchemy/enginefacade.py:332: NotSupportedWarning: Configuration option(s) ['use_tpool'] not supported exception.NotSupportedWarningFound 2 cell mappings.Skipping cell0 since it does not contain hosts.Getting computes from cell 'cell1': c795b2eb-4814-4fe7-b9ff-090a1b1b2be5Checking host mapping for compute host 'localhost.localdomain': 58be78ad-5220-4869-ab31-33c9674ecfd1Creating host mapping for compute host 'localhost.localdomain': 58be78ad-5220-4869-ab31-33c9674ecfd1Found 1 unmapped computes in cell: c795b2eb-4814-4fe7-b9ff-090a1b1b2be5注意：添加新计算节点时，必须在控制器节点上运行nova-manage cell_v2 discover_hosts以注册这些新计算节点。或者，您可以在 /etc/nova/nova.conf 中设置适当的间隔：[scheduler]discover_hosts_in_cells_interval = 300 验证：1234567891011$ . admin-openrc$ openstack compute service list+----+------------------+-----------------------+----------+---------+-------+----------------------------+| ID | Binary | Host | Zone | Status | State | Updated At |+----+------------------+-----------------------+----------+---------+-------+----------------------------+| 1 | nova-conductor | controller | internal | enabled | up | 2018-09-13T03:00:28.000000 || 3 | nova-consoleauth | controller | internal | enabled | up | 2018-09-13T03:00:29.000000 || 4 | nova-scheduler | controller | internal | enabled | up | 2018-09-13T03:00:29.000000 || 9 | nova-compute | localhost.localdomain | nova | enabled | up | 2018-09-13T03:00:26.000000 |+----+------------------+-----------------------+----------+---------+-------+----------------------------+ 123456789101112131415161718192021222324252627282930313233$ openstack catalog list+-----------+-----------+-----------------------------------------+| Name | Type | Endpoints |+-----------+-----------+-----------------------------------------+| keystone | identity | RegionOne || | | public: http://controller:5000/v3/ || | | RegionOne || | | internal: http://controller:5000/v3/ || | | RegionOne || | | admin: http://controller:5000/v3/ || | | || nova | compute | RegionOne || | | public: http://controller:8774/v2.1 || | | RegionOne || | | internal: http://controller:8774/v2.1 || | | RegionOne || | | admin: http://controller:8774/v2.1 || | | || glance | image | RegionOne || | | internal: http://controller:9292 || | | RegionOne || | | admin: http://controller:9292 || | | RegionOne || | | public: http://controller:9292 || | | || placement | placement | RegionOne || | | internal: http://controller:8778 || | | RegionOne || | | public: http://controller:8778 || | | RegionOne || | | admin: http://controller:8778 || | | |+-----------+-----------+-----------------------------------------+ 123456$ openstack image list+--------------------------------------+--------+--------+| ID | Name | Status |+--------------------------------------+--------+--------+| ad7da2d4-cb83-4a41-836f-e58e47e899f5 | cirros | active |+--------------------------------------+--------+--------+ 123456789101112131415161718192021222324252627# nova-status upgrade check/usr/lib/python2.7/site-packages/oslo_db/sqlalchemy/enginefacade.py:332: NotSupportedWarning: Configuration option(s) ['use_tpool'] not supported exception.NotSupportedWarningOption "os_region_name" from group "placement" is deprecated. Use option "region-name" from group "placement".+-------------------------------+| 升级检查结果 |+-------------------------------+| 检查: Cells v2 || 结果: 成功 || 详情: None |+-------------------------------+| 检查: Placement API || 结果: 成功 || 详情: None |+-------------------------------+| 检查: Resource Providers || 结果: 成功 || 详情: None |+-------------------------------+| 检查: Ironic Flavor Migration || 结果: 成功 || 详情: None |+-------------------------------+| 检查: API Service Version || 结果: 成功 || 详情: None |+-------------------------------+]]></content>
      <categories>
        <category>Openstack</category>
      </categories>
      <tags>
        <tag>openstack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Openstack Queens 环境搭建（四）Glance服务]]></title>
    <url>%2F2019%2F03%2F19%2FOpenstackQueens4%2F</url>
    <content type="text"><![CDATA[Controller节点：创建glance数据库，授予权限：12345$ mysql -u root -pMariaDB [(none)]&gt; CREATE DATABASE glance;MariaDB [(none)]&gt; GRANT ALL PRIVILEGES ON glance.* TO 'glance'@'localhost' IDENTIFIED BY '123456';MariaDB [(none)]&gt; GRANT ALL PRIVILEGES ON glance.* TO 'glance'@'%' IDENTIFIED BY '123456';MariaDB [(none)]&gt; exit; 创建glance用户：1234567891011121314151617$ . admin-openrc$ openstack user create --domain default --password-prompt glanceUser Password: 123456Repeat User Password: 123456+---------------------+----------------------------------+| Field | Value |+---------------------+----------------------------------+| domain_id | default || enabled | True || id | 5b7e76213b4b4945b7c702be5b595c0e || name | glance || options | &#123;&#125; || password_expires_at | None |+---------------------+----------------------------------+$ openstack role add --project service --user glance admin 创建glance服务实体：12345678910$ openstack service create --name glance --description "OpenStack Image" image+-------------+----------------------------------+| Field | Value |+-------------+----------------------------------+| description | OpenStack Image || enabled | True || id | b9cfd97d134e4ec2bf19d78306e85a5a || name | glance || type | image |+-------------+----------------------------------+ 创建API端点：1234567891011121314$ openstack endpoint create --region RegionOne image public http://controller:9292+--------------+----------------------------------+| Field | Value |+--------------+----------------------------------+| enabled | True || id | b9c90172de704ea4a867190ba44fc931 || interface | public || region | RegionOne || region_id | RegionOne || service_id | b9cfd97d134e4ec2bf19d78306e85a5a || service_name | glance || service_type | image || url | http://controller:9292 |+--------------+----------------------------------+ 1234567891011121314$ openstack endpoint create --region RegionOne image internal http://controller:9292+--------------+----------------------------------+| Field | Value |+--------------+----------------------------------+| enabled | True || id | 074bde7662044e93830f4eca15d9c887 || interface | internal || region | RegionOne || region_id | RegionOne || service_id | b9cfd97d134e4ec2bf19d78306e85a5a || service_name | glance || service_type | image || url | http://controller:9292 |+--------------+----------------------------------+ 1234567891011121314$ openstack endpoint create --region RegionOne image admin http://controller:9292+--------------+----------------------------------+| Field | Value |+--------------+----------------------------------+| enabled | True || id | 17030061f9b84301ac515765706933b2 || interface | admin || region | RegionOne || region_id | RegionOne || service_id | b9cfd97d134e4ec2bf19d78306e85a5a || service_name | glance || service_type | image || url | http://controller:9292 |+--------------+----------------------------------+ 安装和配置：12345678910111213141516171819202122232425262728293031323334353637383940# yum install openstack-glance# vi /etc/glance/glance-api.conf[database]connection = mysql+pymysql://glance:123456@controller/glance[glance_store]stores = file,httpdefault_store = filefilesystem_store_datadir = /var/lib/glance/images/[keystone_authtoken]auth_uri = http://controller:5000auth_url = http://controller:5000memcached_servers = controller:11211auth_type = passwordproject_domain_name = Defaultuser_domain_name = Defaultproject_name = serviceusername = glancepassword = 123456[paste_deploy]flavor = keystone# vi /etc/glance/glance-registry.conf[database]connection = mysql+pymysql://glance:123456@controller/glance[keystone_authtoken]auth_uri = http://controller:5000auth_url = http://controller:5000memcached_servers = controller:11211auth_type = passwordproject_domain_name = Defaultuser_domain_name = Defaultproject_name = serviceusername = glancepassword = 123456[paste_deploy]flavor = keystone# su -s /bin/sh -c "glance-manage db_sync" glance 完成安装12# systemctl enable openstack-glance-api.service openstack-glance-registry.service# systemctl start openstack-glance-api.service openstack-glance-registry.service 验证操作123456789101112131415161718192021222324252627282930$ . admin-openrc$ wget http://download.cirros-cloud.net/0.3.5/cirros-0.3.5-x86_64-disk.img$ openstack image create "cirros" \ --file cirros-0.3.5-x86_64-disk.img \ --disk-format qcow2 --container-format bare \ --public+------------------+------------------------------------------------------+| Field | Value |+------------------+------------------------------------------------------+| checksum | f8ab98ff5e73ebab884d80c9dc9c7290 || container_format | bare || created_at | 2018-09-13T00:55:04Z || disk_format | qcow2 || file | /v2/images/ad7da2d4-cb83-4a41-836f-e58e47e899f5/file || id | ad7da2d4-cb83-4a41-836f-e58e47e899f5 || min_disk | 0 || min_ram | 0 || name | cirros || owner | 4a5e42dd8cbf410f85a5f145039d69a6 || protected | False || schema | /v2/schemas/image || size | 13267968 || status | active || tags | || updated_at | 2018-09-13T00:55:04Z || virtual_size | None || visibility | public |+------------------+------------------------------------------------------+ 123456$ openstack image list+--------------------------------------+--------+--------+| ID | Name | Status |+--------------------------------------+--------+--------+| ad7da2d4-cb83-4a41-836f-e58e47e899f5 | cirros | active |+--------------------------------------+--------+--------+]]></content>
      <categories>
        <category>Openstack</category>
      </categories>
      <tags>
        <tag>openstack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Openstack Queens 环境搭建（三）Keystone服务]]></title>
    <url>%2F2019%2F03%2F19%2FOpenstackQueens3%2F</url>
    <content type="text"><![CDATA[Controller节点：创建keystone数据库，授予权限：12345678$ mysql -u root -p密码：123456MariaDB [(none)]&gt; CREATE DATABASE keystone;MariaDB [(none)]&gt; GRANT ALL PRIVILEGES ON keystone.* TO 'keystone'@'localhost' \IDENTIFIED BY '123456';MariaDB [(none)]&gt; GRANT ALL PRIVILEGES ON keystone.* TO 'keystone'@'%' \IDENTIFIED BY '123456';MariaDB [(none)]&gt; exit; 安装及配置组件12345678910111213141516# yum install openstack-keystone httpd mod_wsgi# vi /etc/keystone/keystone.conf[database]connection = mysql+pymysql://keystone:123456@controller/keystone[token]provider = fernet# su -s /bin/sh -c "keystone-manage db_sync" keystone# keystone-manage fernet_setup --keystone-user keystone --keystone-group keystone# keystone-manage credential_setup --keystone-user keystone --keystone-group keystone# keystone-manage bootstrap --bootstrap-password 123456 \ --bootstrap-admin-url http://controller:5000/v3/ \ --bootstrap-internal-url http://controller:5000/v3/ \ --bootstrap-public-url http://controller:5000/v3/ \ --bootstrap-region-id RegionOne 配置Apache HTTP Server1234# vi /etc/httpd/conf/httpd.confServerName controller# ln -s /usr/share/keystone/wsgi-keystone.conf /etc/httpd/conf.d/ 完成安装：12# systemctl enable httpd.service# systemctl start httpd.service 配置管理帐户1234567$ export OS_USERNAME=admin$ export OS_PASSWORD=123456$ export OS_PROJECT_NAME=admin$ export OS_USER_DOMAIN_NAME=Default$ export OS_PROJECT_DOMAIN_NAME=Default$ export OS_AUTH_URL=http://controller:35357/v3$ export OS_IDENTITY_API_VERSION=3 创建域、项目、用户和角色：12345678910$ openstack domain create --description "An Example Domain" example+-------------+----------------------------------+| Field | Value |+-------------+----------------------------------+| description | An Example Domain || enabled | True || id | 2f338489f6c64472a0b2b6db54ecc2df || name | example || tags | [] |+-------------+----------------------------------+ 12345678910111213$ openstack project create --domain default --description "Service Project" service+-------------+----------------------------------+| Field | Value |+-------------+----------------------------------+| description | Service Project || domain_id | default || enabled | True || id | 84218999229845e2ad7f4e88208b3bee || is_domain | False || name | service || parent_id | default || tags | [] |+-------------+----------------------------------+ 12345678910111213$ openstack project create --domain default --description "Demo Project" demo+-------------+----------------------------------+| Field | Value |+-------------+----------------------------------+| description | Demo Project || domain_id | default || enabled | True || id | 5c4692ce6659454eb830e7e9633a09f1 || is_domain | False || name | demo || parent_id | default || tags | [] |+-------------+----------------------------------+ 12345678910111213$ openstack user create --domain default --password-prompt demoUser Password:123456Repeat User Password:123456+---------------------+----------------------------------+| Field | Value |+---------------------+----------------------------------+| domain_id | default || enabled | True || id | 803e7ad2e94b4af39f9be9e0742b45fd || name | demo || options | &#123;&#125; || password_expires_at | None |+---------------------+----------------------------------+ 12345678910$ openstack role create user+-----------+----------------------------------+| Field | Value |+-----------+----------------------------------+| domain_id | None || id | cbe4799bac204eacbf0012a77dc349c4 || name | user |+-----------+----------------------------------+$ openstack role add --project demo --user demo user 验证操作：123456789101112131415161718192021222324252627$ unset OS_AUTH_URL OS_PASSWORD$ openstack --os-auth-url http://controller:35357/v3 \ --os-project-domain-name Default --os-user-domain-name Default \ --os-project-name admin --os-username admin token issuePassword: 123456+------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| Field | Value |+------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| expires | 2018-09-12T09:43:34+0000 || id | gAAAAABbmNG25wIya-0xFYb3zCW3ljtDTWnr8ZCpB4iAZPMfQnP-62EGiIr6aKEjO847h6jH5nNONRqeLXO2BC_bJ0O-b5Fwj2GZpYGWRSSucAU4Mh6MqLQzetbOsRCv9-ZGO6VQYkmr0cPTEm7kzuzUL2bwTcUCbAVCpuFvCnRUZ7Hu4FE5bAI || project_id | 4a5e42dd8cbf410f85a5f145039d69a6 || user_id | 2ffffa1e6cbe4d239bdacc9760a54dd5 |+------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+$ openstack --os-auth-url http://controller:5000/v3 \ --os-project-domain-name Default --os-user-domain-name Default \ --os-project-name demo --os-username demo token issuePassword: 123456+------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| Field | Value |+------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| expires | 2018-09-12T09:45:20+0000 || id | gAAAAABbmNIgtMBObdQXwOlGu-HMLvKNTBZuYvVizTCn3aDJLMvqzQRTyjhfm5RjEkAgIWcYfal9TrjZan2VWL_AZ8cASpkBwoa0TQn_rWlZw1wh8xcDeb5XNES3jMNxhtZA87peDCnMkGJoMaJVhvkR4gsDQiIUmCImzjYv6ZvJjLgGEotBszY || project_id | 5c4692ce6659454eb830e7e9633a09f1 || user_id | 803e7ad2e94b4af39f9be9e0742b45fd |+------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ 创建OpenStack客户端环境脚本：12345678910111213141516171819# vi /root/admin-openrcexport OS_PROJECT_DOMAIN_NAME=Defaultexport OS_USER_DOMAIN_NAME=Defaultexport OS_PROJECT_NAME=adminexport OS_USERNAME=adminexport OS_PASSWORD=123456export OS_AUTH_URL=http://controller:5000/v3export OS_IDENTITY_API_VERSION=3export OS_IMAGE_API_VERSION=2# vi /root/demo-openrcexport OS_PROJECT_DOMAIN_NAME=Defaultexport OS_USER_DOMAIN_NAME=Defaultexport OS_PROJECT_NAME=demoexport OS_USERNAME=demoexport OS_PASSWORD=123456export OS_AUTH_URL=http://controller:5000/v3export OS_IDENTITY_API_VERSION=3export OS_IMAGE_API_VERSION=2 使用脚本验证：1234567891011$ . admin-openrc$ openstack token issue+------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| Field | Value |+------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| expires | 2018-09-12T09:55:59+0000 || id | gAAAAABbmNSfM00gw3qvJi-U8ytTcBxfuVhgNkETRa-gh3PqLp6Md9cW_5FfbkUL1nyQGW4Bg_XvvdIhSBv7fXRnbfyqGxTxOUloe7BmnWgM9LqLn8Fm2FLQp8qcuFamyW-9_FZA5SPqxbYS1Ozk6fO7TRDWAIWdzy5i0-qqB4Ypt6vQOyW-pqk || project_id | 4a5e42dd8cbf410f85a5f145039d69a6 || user_id | 2ffffa1e6cbe4d239bdacc9760a54dd5 |+------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+]]></content>
      <categories>
        <category>Openstack</category>
      </categories>
      <tags>
        <tag>openstack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Openstack Queens 环境搭建（二）环境相关服务]]></title>
    <url>%2F2019%2F03%2F19%2FOpenstackQueens2%2F</url>
    <content type="text"><![CDATA[Controller节点：安装NTP服务1234567891011121314# yum install chrony# vi /etc/chrony.confserver 0.centos.pool.ntp.org iburstserver 1.centos.pool.ntp.org iburstserver 2.centos.pool.ntp.org iburstserver 3.centos.pool.ntp.org iburst...allow 192.100.10.0/24...# systemctl enable chronyd.service 开机启用NTP# systemctl start chronyd.service 开启NTP服务 验证NTP服务： 1234567# chronyc sources 210 Number of sources = 2 MS Name/IP address Stratum Poll Reach LastRx Last sample =============================================================================== ^- 192.0.2.11 2 7 12 137 -2814us[-3000us] +/- 43ms ^* 192.0.2.12 2 6 177 46 +17us[ -23us] +/- 68ms 安装Openstack相关库1234# yum install centos-release-openstack-queens 安装Openstack库# yum upgrade 更新包# yum install python-openstackclient 安装Openstack客户端# yum install openstack-selinux 安装openstack-selinux用来管理Openstack服务的安全策略 关闭防火墙12# systemctl stop firewalld 关闭防火墙服务# systemctl disable firewalld 永久防火墙开机自启动 关闭SELINUX服务123# setenforce 0 关闭selinux服务# vi /etc/selinux/config 永久关闭selinux服务 SELINUX=disabled 安装数据库服务12345678910111213141516# yum install mariadb mariadb-server python2-PyMySQL# vi /etc/my.cnf.d/openstack.cnf[mysqld]bind-address = 192.100.10.160default-storage-engine = innodbinnodb_file_per_table = onmax_connections = 4096collation-server = utf8_general_cicharacter-set-server = utf8# systemctl enable mariadb.service 开机启用Mysql服务# systemctl start mariadb.service 开启Mysql服务# mysql_secure_installation 设置Mysql密码-&gt;123456 安装消息队列1234567# yum install rabbitmq-server# systemctl enable rabbitmq-server.service# systemctl start rabbitmq-server.service# rabbitmqctl add_user openstack 123456# rabbitmqctl set_permissions openstack ".*" ".*" ".*" 安装Memcached缓存1234567# yum install memcached python-memcached# vi /etc/sysconfig/memcachedOPTIONS="-l 127.0.0.1,::1,controller"# systemctl enable memcached.service# systemctl start memcached.service 安装Etcd1234567891011121314151617# yum install etcd# vi /etc/etcd/etcd.conf#[Member]ETCD_DATA_DIR="/var/lib/etcd/default.etcd"ETCD_LISTEN_PEER_URLS="http://192.100.10.160:2380"ETCD_LISTEN_CLIENT_URLS="http://192.100.10.160:2379"ETCD_NAME="controller"#[Clustering]ETCD_INITIAL_ADVERTISE_PEER_URLS="http://192.100.10.160:2380"ETCD_ADVERTISE_CLIENT_URLS="http://192.100.10.160:2379"ETCD_INITIAL_CLUSTER="controller=http://192.100.10.160:2380"ETCD_INITIAL_CLUSTER_TOKEN="etcd-cluster-01"ETCD_INITIAL_CLUSTER_STATE="new"# systemctl enable etcd# systemctl start etcd Compute节点：安装NTP服务1234567891011# yum install chrony# vi /etc/chrony.confserver controller iburst...allow 192.100.10.0/24...# systemctl enable chronyd.service 开机启用NTP# systemctl start chronyd.service 开启NTP服务 安装Openstack相关库1234# yum install centos-release-openstack-queens 安装Openstack库# yum upgrade 更新包# yum install python-openstackclient 安装Openstack客户端# yum install openstack-selinux 安装openstack-selinux用来管理Openstack服务的安全策略 关闭防火墙12# systemctl stop firewalld 关闭防火墙服务# systemctl disable firewalld 永久防火墙开机自启动 关闭SELINUX服务123# setenforce 0 关闭selinux服务# vi /etc/selinux/config 永久关闭selinux服务 SELINUX=disabled]]></content>
      <categories>
        <category>Openstack</category>
      </categories>
      <tags>
        <tag>openstack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Openstack Queens 环境搭建（一）环境准备]]></title>
    <url>%2F2019%2F03%2F19%2FOpenstackQueens1%2F</url>
    <content type="text"><![CDATA[环境准备：基于CentOS Linux release 7.6.1810 (Core) 控制节点（Controller）：eth0：192.100.10.160/24eth1：10.0.0.11/24eth2：预留 计算节点（Compute):eth0：192.100.10.161/24eth1：10.0.0.12/24eth2：预留 网卡0接口为管理网络 -&gt; 交换机 + 路由器网卡1接口为Overlay网络 -&gt; 目前直连 / 交换机连接网卡2接口为外部网络 -&gt; 路由器 -（可以先使用eth1作为外部网络下载Openstack安装所需资源，后修改） 通用密码： 123456 Controller节点：配置网卡信息： 12345# vi /etc/sysconfig/network-scripts/ifcfg-eth0BOOTPROTO=staticIPADDR=192.100.10.160NETMASK=255.255.255.0GATEWAY=192.100.10.1 1234# vi /etc/sysconfig/network-scripts/ifcfg-eth1BOOTPROTO=staticIPADDR=10.0.0.11NETMASK=255.255.255.0 配置主机信息： 12345# vi /etc/hosts# controller192.100.10.160 controller# compute192.100.10.161 compute 配置主机名：控制节点的主机名为controller，设置如下： 1~# hostnamectl set-hostname controller 对主机名进行验证： 1~# hostname 看到输出为controller即可 配置DNS： 12# vi /etc/resolv.confnameserver 114.114.114.114 Compute节点：配置管理网络： 12345# vi /etc/sysconfig/network-scripts/ifcfg-eth0BOOTPROTO=staticIPADDR=192.100.10.161NETMASK=255.255.255.0GATEWAY=192.100.10.1 1234# vi /etc/sysconfig/network-scripts/ifcfg-eth1BOOTPROTO=staticIPADDR=10.0.0.21NETMASK=255.255.255.0 配置主机信息： 12345# vi /etc/hosts# controller192.100.10.160 controller# compute192.100.10.161 compute 配置主机名：计算节点的主机名为compute，设置如下： 1~# hostnamectl set-hostname compute 对主机名进行验证： 1~# hostname 看到输出为compute即可 配置DNS： 12# vi /etc/resolv.confnameserver 114.114.114.114]]></content>
      <categories>
        <category>Openstack</category>
      </categories>
      <tags>
        <tag>openstack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[KDPA RestAPI]]></title>
    <url>%2F2019%2F03%2F10%2FKDPA%20RestAPI%2F</url>
    <content type="text"><![CDATA[KDPA RestAPI定义及说明 1 主机实例 1-1 创建主机实例 1-2 删除主机实例 1-3 配置主机实例 1-4 启动主机实例 1-5 关闭主机实例 1-6 挂起主机实例 1-7 恢复主机实例 1-8 获取主机实例基本信息 1-9 主机实例控制台 1-10 重命名主机实例 1-11 创建浮动IP 1-12 删除浮动IP 1-13 获取浮动IP信息 1-14 批量关闭实例组件 1-15 获取所有组件的状态 2 网线 2-1 创建网线 2-2 删除网线 3 通用 3-1 清除当前用户的实验室环境 3-2 设置实训平台系统配置 3-3 获取系统网卡信息及预留网段信息 3-4 用户实验室环境未保存退出 3-5 获取系统资源 3-6 获取系统资源使用率 3-7 按日期获取系统资源使用率 4 系统 4-1 创建镜像 4-2 删除镜像 5 纵向 5-1 创建纵向 5-2 删除纵向 6 串口线 6-1 创建串口线连接 6-2 删除串口线连接 7 UKEY 7-1 创建UKEY 7-2 删除UKEY 7-3 创建UKEY与纵向实例的连接 7-4 删除UKEY与纵向实例的连接 8 隔离 8-1 创建隔离 8-2 删除隔离 8-3 启动隔离 8-4 关闭隔离 8-5 挂起隔离 8-6 恢复隔离 8-7 隔离控制台 8-8 隔离创建浮动IP 8-9 隔离删除浮动IP 8-10 创建隔离镜像 8-11 删除隔离镜像 6-2 删除串口线连接 9 虚拟交换机 9-1 创建Untag虚拟交换机 9-2 删除Untag虚拟交换机 9-3 创建虚拟交换机 9-4 删除虚拟交换机 9-5 启动虚拟交换机 9-6 关闭虚拟交换机 9-7 设置虚拟交换机端口 9-8 获取被占用vlan 10 虚拟路由器 10-1 创建虚拟路由器 10-2 删除虚拟路由器 10-3 查看虚拟路由器配置 10-4 虚拟路由器上传镜像 10-5 虚拟路由器删除镜像 Notice: 1.以下所有API的方法都为POST 2.传参及返回值都为json格式，通用返回值格式为: {“code”: 状态码, “data”: 数据 }。 状态码为 0 代表操作成功，其他代表操作失败 3.api地址中，controller为计算节点的IP地址（已配置在主机配置文件中。8000为系统默认提供服务的端口号。 4.*参数为必填参数，其余为非必填，非必填参数系统传参默认值。 限制: 1.所有组件: 所有组件在关机状态下不能连接网线，不能删除网线。 实验室环境在未点击保存时会被清理，如有需要，手动点击保存按钮。 2.主机: 处于有网线连接或者关机状态下的主机不能修改配置。 处于挂起状态下的主机只能执行恢复操作，不能执行其他操作。 3.纵向: UKEY与纵向连接后，不能直接删除纵向，必须先拔出UKEY。 纵向与纵向之间通过网卡1连接，处理业务，如果纵向与纵向之间通过其他网卡连接，不予处理。 4.串口线: 串口线目前仅支持主机、隔离。 5.虚拟路由器: 虚拟路由器定义的配置在初始化后不能修改。 虚拟路由器与主机通过网线连接两端的网段必须相同。 6.虚拟交换机: 虚拟交换机与虚拟交换机不能通过网线连接。 虚拟交换机VLAN接口只能连接虚拟组件，虚实接口只能连接实体组件。 不同虚拟交换机中的VLAN ID不能重复。 在接口有连接的情况下，不能修改虚拟交换机端口的类型（VLAN接口/虚实接口）。 7.其他问题: 虚拟路由器与虚拟交换机网线连接后，目前虚拟路由器重启DHCP获取不到IP，导致虚拟路由器不可用。 虚拟路由器与主机组件之间有其他组件的情况下，需要手动为主机配置远端静态路由。 1主机实例1-1创建主机实例Request Method: POST API: 123456789101112- Params: - **uuid**(str.)*: 主机UUID，唯一标识，必填，长度1~63 - **name**(str.)*: 主机名称，必填，长度1~63 - **imageName**(str.)*: 镜像名称，必填，长度1~63 - **userId**(int.)*: 用户ID，必填 - template(int.): 模板类型，非必填， - 1代表 1 vcpu, 1G ram, 10G disk - 2代表 2 vcpu, 2G ram, 20G disk### 1-2删除主机实例#### Request- Method: **POST**- API: ```http://controller:8000/api/instance/delete Params: uuid(str.)*: 主机UUID，唯一标识，必填，长度1~63 1-3配置主机实例Request Method: POST API: 1234567891011- Params: - **uuid**(str.)*: 主机UUID，唯一标识，必填，长度1~63 - **ipAddr**(str.)*: IPv4地址，必填，例如:192.168.1.100 - netmask(str.): 掩码地址，非必填，默认值为”255.255.255.0” - gateway(str.): 网关地址，非必填 - number(int.): 网卡编号，非必填，默认值为1，表示主机目前都为1块网卡### 1-4启动主机实例#### Request- Method: **POST**- API: ```http://controller:8000/api/instance/start Params: uuid(str.)*: 主机UUID，唯一标识，必填，长度1~63 1-5关闭主机实例Request Method: POST API: 1234567- Params: - **uuid**(str.)*: 主机UUID，唯一标识，必填，长度1~63### 1-6挂起主机实例#### Request- Method: **POST**- API: ```http://controller:8000/api/instance/suspend Params: uuid(str.)*: 主机UUID，唯一标识，必填，长度1~63 1-7恢复主机实例Request Method: POST API: 1234567- Params: - **uuid**(str.)*: 主机UUID，唯一标识，必填，长度1~63### 1-8获取主机实例基本信息#### Request- Method: **POST**- API: ```http://controller:8000/api/instance/show Params: uuid(str.)*: 主机UUID，唯一标识，必填，长度1~63 Response Body123456789101112131415&#123; "code": 0, "data": &#123; "name": "HOST_1", "state": "Up", "interface": [&#123; "macAddr": "fa:16:3e:62:7b:cb", // MAC地址 "ipAddr": "192.168.1.5", // IP地址 "number": 0, // 网卡编号 "netmask": "255.255.255.0", // 掩码地址 "cidr": "192.168.1.0/24", // 网段地址 "gateway": "192.168.1.1" // 网关地址 &#125;] &#125;&#125; 1-9主机实例控制台Request Method: POST API: 12345- Params: - **uuid**(str.)*: 主机UUID，唯一标识，必填，长度1~63#### Response- Body { “code”: 0, “data”: { “console”: { “url”: “http://controller:6080/vnc_auto.html?token=aca31aec-fd05-46e4-9618-0e409c1e8b1e&quot;, “type”: “novnc” } }} 12345### 1-10重命名主机实例#### Request- Method: **POST**- API: ```http://controller:8000/api/instance/rename Params: uuid(str.)*: 主机UUID，唯一标识，必填，长度1~63 currentName(str.)*: 修改后主机名称，必填，长度1~63 1-11创建浮动IPRequest Method: POST API: 1234567- Params: - **uuid**(str.)*: 主机UUID，唯一标识，必填，长度1~63 ### 1-12删除主机浮动IP#### Request- Method: **POST**- API: ```http://controller:8000/api/instance/deleteFIP Params: uuid(str.)*: 主机UUID，唯一标识，必填，长度1~63 1-13获取浮动IP信息Request Method: POST API: 12345- Params: - **uuid**(str.)*: 主机UUID，唯一标识，必填，长度1~63#### Response- Body { “code”: 0, “data”: { “ftpHost”: “192.100.10.146”, “ftpPath”: ftp://192.100.10.146/ “username”: “ftpuser”, “passwd”: “ftpuser123”, “windowsPath”: “c:\ftpuser\“, “linuxPath”: “/home/ftpuser/“, }} 12345### 1-14批量关闭实例组件#### Request- Method: **POST**- API: ```http://controller:8000/api/instance/batchStop Params: userId(int.)*: 用户ID，必填 1-15获取所有组件的状态Request Method: POST API: 12345- Params: - **userId**(int.)*: 用户ID，必填#### Response- Body { “code”: 0, “data”: { “lineList”: [], “instanceList”: [{ “state”: “Up”, “uuid”: “n-djsgs10124o”, “ukeyid”: “n-fds52326574vf” }] }} 12345678state: 包括 ”UP”: 开机，”Down”：关机，“Suspend”：挂起。## 2网线### 2-1创建网线#### Request- Method: **POST**- API: ```http://controller:8000/api/netline/create Params: uuid(str.)*: 源组件UUID，唯一标识，必填，长度1~63 dstUuid(str.)*: 网线对端组件UUID，必填，长度1~63 netlineUuid(int.)*: 网线UUID，必填，网线的唯一标识 userId(int.)*: 用户ID，必填 number(int.): 网卡编号，非必填，默认值为1，表示主机的第1块网卡 dstNumber(int.): 网线对端网设备卡编号，非必填，默认值为1 vlan(int.): 连接网线时所占用的vlan标签，非必填，默认值为0 0: 默认值，表示当前连接的网线为常规网线，即虚拟组件与虚拟组件连接 14094: 当vlan为14094之前的正整数时，表示此网线为虚实连线，vlan标签表示外部实体设备实际的vlan标签，此标签每个实体设备间不能相同。 2-2删除网线Request Method: POST API: 12345678910- Params: - **uuid**(str.)*: 网线UUID，唯一标识，必填，长度1~63 - **userId**(int.)*: 用户ID，必填## 3通用### 3-1清除当前用户的实验室环境#### Request- Method: **POST**- API: ```http://controller:8000/api/env/clear Params: userId(int.)*: 用户ID，必填 3-2设置实训平台系统配置Request Method: POST API: 1234567- Params: - **maxVmNumber**(str.)*: 用户最大创建虚拟机数量，默认值为10### 3-3获取系统网卡信息及预留网段信息#### Request- Method: **POST**- API: ```http://controller:8000/api/system/info Params: 无 Response Body12345678910111213141516171819202122232425262728293031323334353637383940414243444546&#123; "code": 0, "data": &#123; "networkParameters": [&#123; "nodeType": "控制节点", "nodeName": "controller", "interfaceInfo": [&#123; "ethName": "enp2s0", "ipAddr": "192.100.10.58" &#125;, &#123; "ethName": "enp3s0", "ipAddr": "10.0.0.11" &#125;] &#125;,&#123; "nodeType": "计算节点", "nodeName": "compute1", "interfaceInfo": [&#123; "ethName": "enp0s31f6", "ipAddr": "192.100.10.160" &#125;, &#123; "ethName": "enp0s20f0u8", "ipAddr": "10.0.0.31" &#125;] &#125;] , "reserveCidr": [ &#123; "name": "浮动IP网段", "cidr": [&#123; "start": "192.100.10.140", "end": "192.100.10.141" &#125;, &#123; "start": "192.100.10.161", "end": "192.100.10.169" &#125;, &#123; "start": "192.100.10.144", "end": "192.100.10.149" &#125;] &#125;,&#123; "name": "Overlay网段", "cidr": [&#123; "start": "10.0.0.11", "end": "10.0.0.30" &#125; ] &#125;,&#123; "name": "内部网段", "cidr": [&#123; "start": "20.0.0.2", "end": "20.0.0.254" &#125; ] &#125; ] &#125;&#125; 3-4用户实验室环境未保存退出Request Method: POST API: 12345678910- Params: - **instanceList**(list.)*: 实例组件uuid列表，必填 - **netlineList**(list.)*: 网线组件uuid列表，必填 - **userId**(int.)*: 用户ID，必填 - **clickF5**(str)*: 是否点击F5进行刷新，默认为&quot;False&quot;,非必填 ### 3-5获取系统资源#### Request- Method: **POST**- API: ```http://controller:8000/api/system/resource Params: 无 Response Body1234567891011121314151617&#123; "code": 0, "data": &#123; "nodeNumber": 2, # 节点数量 "runningNodeNumber": 2, # 正在运行的节点数量 "instanceNumber": 2, # 实例数量 "vcpuUsed": 3, # VCPU使用量 个数 "vcpuTotal": 24, # VCPU总量 个数 "vcpu": "3/24", # VCPU使用情况 个数 "memoryUsed": 4.0, # 内存使用量 GB "memoryTotal": 23.8, # 内存总量 GB "memory": "4.0/23.8", # 内存使用情况 GB "diskUsed": 30, # 磁盘使用量 GB "diskTotal": 2198, # 磁盘总量 GB "disk": "30/2198" # 磁盘使用情况 GB &#125;&#125; 3-6获取系统资源使用率Request Method: POST API: 12345- Params: - 无 #### Response- Body { “code”: 0, “data”: { “cpu”: [“4.1”, “4.1”, “3.9”, “4.6”], “memory”: [“67.6”, “67.8”, “67.8”, “67.9”], “disk”: [“5.7”, “5.7”, “5.7”, “5.7”], “net_out”: [“0”, “653”, “17702”, “0”], “net_in”: [“0”, “2203”, “18517”, “0”], “time”: [ “2019-06-14 07:38:01”, “2019-06-14 07:38:10”, “2019-06-14 07:38:20”, “2019-06-14 07:38:30”] }} 12345### 3-7按日期获取系统资源使用率#### Request- Method: **POST**- API: ```http://controller:8000/api/system/dateUsage Params: date(str.)*: 日期，例：”2019-06-17” Response Body1234567891011121314151617&#123; "code": 0, "data": &#123; "memory": ["71.8", "74.5", "74.5", "74.5", "74.1"], "net_out": ["0", "772940", "130350", "167966", "172898"], "net_in": ["0", "14704", "3530", "4194", "4442"], "time": [ "2019-06-17 02:00:02", "2019-06-17 03:00:02", "2019-06-17 04:00:02", "2019-06-17 05:00:02", "2019-06-17 06:00:02" ], "disk": ["5.7", "5.7", "5.7", "5.7", "5.7"], "cpu": ["4.6", "13.2", "13.8", "13.6", "13.5"] &#125;&#125; 4系统4-1创建镜像Request Method: POST API: 1234567891011121314- Params: - **name**(str.)*: 镜像名称，必填 - **url**(str.)*: 镜像路径，必填 - defaultVCPU(int.): 默认虚拟CPU个数，非必填，默认值1 - defaultRAM(int.): 默认内存大小，非必填，单位MB，默认值1024 - defaultDISK (int.): 默认磁盘大小，非必填，单位GB，默认值10 - advancedVCPU(int.): 高级虚拟CPU个数，非必填，默认值2 - advancedRAM (int.): 高级内存大小，非必填，默认值2048 - advancedDISK (int.): 高级磁盘大小，非必填，单位GB，默认值20 ### 4-2删除镜像#### Request- Method: **POST**- API: ```http://controller:8000/api/image/delete Params: name(str.)*: 镜像名称，必填 5纵向5-1创建纵向Request Method: POST API: 123456789101112- Params: - **uuid**(str.)*: 纵向UUID，唯一标识，必填，长度1~63 - **name**(str.)*: 纵向名称，必填，长度1~63 - **userId**(int.)*: 用户ID，必填 - template(int.): 模板类型，非必填， - 1代表 1 vcpu, 1G ram, 10G disk - 2代表 2 vcpu, 2G ram, 20G disk### 5-2删除纵向#### Request- Method: **POST**- API: ```http://controller:8000/api/pstunnel/delete Params: uuid(str.)*: 纵向UUID，唯一标识，必填，长度1~63 6串口线6-1创建串口线连接Request Method: POST API: 123456789101112- Params: - **uuid**(str.)*: 实例组件UUID，唯一标识，必填，长度1~63 - **dstUuid**(str.)*: 目的实例组件UUID，唯一标识，必填，长度1~63 - **seriallineUuid**(str.)*: 串口线UUID，唯一标识，必填，长度1~63 - **userId**(int.)*: 用户ID，必填 - number(int.): 默认值为0。对于主机，数值无意义。对于隔离，0==内隔离串口，1==外隔离串口 - dstNumber(int.): 默认值为0。对于主机，数值无意义。对于隔离，0==内隔离串口，1==外隔离串口### 6-2删除串口线连接#### Request- Method: **POST**- API: ```http://controller:8000/api/serline/delete Params: uuid(str.)*: 串口线UUID，唯一标识，必填，长度1~63 userId(int.)*: 用户ID，必填 7UKEY7-1创建UKEYRequest Method: POST API: 123456789- Params: - **uuid**(str.)*: UKEY UUID，唯一标识，必填，长度1~63 - **name**(str.)*: UKEY 名称，必填，长度1~63 - **userId**(int.)*: 用户ID，必填### 7-2删除UKEY#### Request- Method: **POST**- API: ```http://controller:8000/api/ukey/delete Params: uuid(str.)*: UKEY UUID，唯一标识，必填，长度1~63 userId(int.)*: 用户ID，必填 7-3创建UKEY与纵向实例的连接Request Method: POST API: 123456789- Params: - **uuid**(str.)*: UKEY UUID，唯一标识，必填，长度1~63 - **psUuid**(str.)*: 纵向UUID，唯一标识，必填，长度1~63 - **userId**(int.)*: 用户ID，必填### 7-4删除UKEY与纵向实例的连接#### Request- Method: **POST**- API: ```http://controller:8000/api/ukey/disconnect Params: uuid(str.)*: UKEY UUID，唯一标识，必填，长度1~63 psUuid(str.)*: 纵向UUID，唯一标识，必填，长度1~63 userId(int.)*: 用户ID，必填 8隔离8-1创建隔离Request Method: POST API: 12345678910111213- Params: - **uuid**(str.)*: 隔离UUID，唯一标识，必填，长度1~63 - **name**(str.)*: 隔离名称，必填，长度1~63 - **imageName**(str.)*: 隔离镜像名称，必填，长度1~63 - **userId**(int.)*: 用户ID，必填 - template(int.): 模板类型，非必填， - 1代表 1 vcpu, 1G ram, 10G disk - 2代表 2 vcpu, 2G ram, 20G disk### 8-2删除隔离#### Request- Method: **POST**- API: ```http://controller:8000/api/stonewall/delete Params: uuid(str.)*: 隔离UUID，唯一标识，必填，长度1~63 userId(int.)*: 用户ID，必填 8-3启动隔离Request Method: POST API: 1234567- Params: - **uuid**(str.)*: 隔离UUID，唯一标识，必填，长度1~63### 8-4关闭隔离#### Request- Method: **POST**- API: ```http://controller:8000/api/stonewall/stop Params: uuid(str.)*: 隔离UUID，唯一标识，必填，长度1~63 8-5挂起隔离Request Method: POST API: 1234567- Params: - **uuid**(str.)*: 隔离UUID，唯一标识，必填，长度1~63### 8-6恢复隔离#### Request- Method: **POST**- API: ```http://controller:8000/api/stonewall/resume Params: uuid(str.)*: 隔离UUID，唯一标识，必填，长度1~63 8-7隔离控制台Request Method: POST API: 123456- Params: - **uuid**(str.)*: 隔离UUID，唯一标识，必填，长度1~63 - number(int.): 标识内网/外网隔离，值范围：0/1，0代表内，1代表外，默认值为0#### Response- Body { “code”: 0, “data”: { “console”: { “url”: “http://controller:6080/vnc_auto.html?token=aca31aec-fd05-46e4-9618-0e409c1e8b1e&quot;, “type”: “novnc” } }} 12345### 8-8隔离创建浮动IP#### Request- Method: **POST**- API: ```http://controller:8000/api/stonewall/createFIP Params: uuid(str.)*: 隔离UUID，唯一标识，必填，长度1~63 number(int.): 标识内网/外网隔离，值范围：0/1，0代表内，1代表外，默认值为0 8-9隔离删除浮动IPRequest Method: POST API: 12345678- Params: - **uuid**(str.)*: 隔离UUID，唯一标识，必填，长度1~63 - number(int.): 标识内网/外网隔离，值范围：0/1，0代表内，1代表外，默认值为0### 8-10创建隔离镜像#### Request- Method: **POST**- API: ```http://controller:8000/api/stonewall/createImg Params: name(str.)*: 镜像名称，必填 urlInt(str.)*: 内镜像路径，必填 urlExt(str.)*: 外镜像路径，必填 defaultVCPU(int.): 默认虚拟CPU个数，非必填，默认值1 defaultRAM(int.): 默认内存大小，非必填，单位MB，默认值1024 defaultDISK (int.): 默认磁盘大小，非必填，单位GB，默认值10 advancedVCPU(int.): 高级虚拟CPU个数，非必填，默认值2 advancedRAM (int.): 高级内存大小，非必填，默认值2048 advancedDISK (int.): 高级磁盘大小，非必填，单位GB，默认值20 8-11删除隔离镜像Request Method: POST API: 123456789101112131415- Params: - **name**(str.)*: 镜像名称，必填## 9虚拟交换机### 9-1创建Untag虚拟交换机#### Request- Method: **POST**- API: `http://controller:8000/api/switch/createUntagSwitch`- Params: - **uuid&lt;str, 必填&gt;**: 虚拟交换机UUID，唯一标识吗 - **name&lt;str, 必填&gt;**: 虚拟交换机名称 - **userId&lt;str, 必填&gt;**: 用户ID - *number&lt;str, 非必填&gt;*: 虚拟交换机网口数量，默认值8#### Response { “code”: API执行结果码，0-执行成功，其他正整数-执行失败的错误信息编码;} 1234567### 9-2删除Untag虚拟交换机#### Request- Method: **POST**- API: `http://controller:8000/api/switch/deleteUntagSwitch`- Params: - **uuid&lt;str, 必填&gt;**: 虚拟交换机UUID，唯一标识吗#### Response { “code”: API执行结果码，0-执行成功，其他正整数-执行失败的错误信息编码;} 12345678910### 9-3创建虚拟交换机#### Request- Method: **POST**- API: `http://controller:8000/api/switch/create`- Params: - **uuid&lt;str, 必填&gt;**: 虚拟交换机UUID，唯一标识吗 - **name&lt;str, 必填&gt;**: 虚拟交换机名称 - **userId&lt;str, 必填&gt;**: 用户ID - *number&lt;str, 非必填&gt;*: 虚拟交换机网口数量，默认值8#### Response { “code”: API执行结果码，0-执行成功，其他正整数-执行失败的错误信息编码;} 1234567### 9-4删除虚拟交换机#### Request- Method: **POST**- API: `http://controller:8000/api/switch/delete`- Params: - **uuid&lt;str, 必填&gt;**: 虚拟交换机UUID，唯一标识吗#### Response { “code”: API执行结果码，0-执行成功，其他正整数-执行失败的错误信息编码;} 1234567### 9-5启动虚拟交换机#### Request- Method: **POST**- API: `http://controller:8000/api/switch/start`- Params: - **uuid&lt;str, 必填&gt;**: 虚拟交换机UUID，唯一标识吗#### Response { “code”: API执行结果码，0-执行成功，其他正整数-执行失败的错误信息编码;} 1234567### 9-6关闭虚拟交换机#### Request- Method: **POST**- API: `http://controller:8000/api/switch/stop`- Params: - **uuid&lt;str, 必填&gt;**: 虚拟交换机UUID，唯一标识吗#### Response { “code”: API执行结果码，0-执行成功，其他正整数-执行失败的错误信息编码;} 123456789101112### 9-7设置虚拟交换机端口#### Request- Method: **POST**- API: `http://controller:8000/api/switch/configSwitchPort`- Params: - **uuid&lt;str, 必填&gt;**: 虚拟交换机UUID，唯一标识吗 - **number&lt;str, 必填&gt;**: 虚拟交换机端口序号，从0开始，最大值由交换机端口数量决定，必填 - **vlan&lt;str, 必填&gt;**: 端口将要设置的具体vlan标签 - *-1*：表示端口要设置成为【虚实口】 - *0*： 表示端口默认状态，此状态下端口不可用，即不能连接网线 - *1~4094*：表示端口设置为【vlan口】，vlan标签为1至4094之间的任意正整数#### Response { “code”: API执行结果码，0-执行成功，其他正整数-执行失败的错误信息编码;} 1234567### 9-8获取被占用vlan#### Request- Method: **POST**- API: `http://controller:8000/api/switch/getUsedVlanList`- Params: - **uuid&lt;str, 必填&gt;**: 虚拟交换机UUID，唯一标识吗#### Response { “code”: API执行结果码，0-执行成功，其他正整数-执行失败的错误信息编码; “data”: [1, 23, 129, …]} 1234567## 10虚拟路由器### 10-1创建虚拟路由器#### Request- Method: **POST**- API: ```http://controller:8000/api/router/create Params: uuid(str.)*: 虚拟路由器UUID，唯一标识，必填，长度1~63 name(str.)*: 虚拟路由器名称，必填，长度1~63 number(int.)*: 虚拟路由器接口数量，默认值为2，必填 cidrList(list.)*: 虚拟路由器网段列表，必填 例如：[“192.168.1.0/24”, “192.168.2.0/24”] 网段的格式：网段地址/掩码位数， 网段不能重复。 imageName(str.)*: 镜像名称，必填，长度1~63 userId(int.)*: 用户ID，必填 10-2删除虚拟路由器Request Method: POST API: 1234567- Params: - **uuid**(str.)*: 虚拟路由器UUID，唯一标识，必填，长度1~63### 10-3查看虚拟路由器配置 #### Request- Method: **POST**- API: ```http://controller:8000/api/router/show Params: uuid(str.)*: 虚拟路由器UUID，唯一标识，必填，长度1~63 Response Body123456789101112131415&#123; "code": 0, "data": [ &#123; "number": 0, "cidr": "192.168.1.0/24", "gateway": "192.168.1.1" &#125;, &#123; "number": 1, "cidr": "192.168.2.0/24", "gateway": "192.168.2.1" &#125; ]&#125; 10-4虚拟路由器上传镜像Request Method: POST API: 12345678910- Params: - **name**(str.)*: 镜像名称，必填 - **url**(str.)*: 镜像路径，必填 虚拟路由器镜像默认1VCPU，1G内存，10G磁盘### 10-5虚拟路由器删除镜像 #### Request- Method: **POST**- API: ```http://controller:8000/api/router/deleteImg Params: name(str.)*: 镜像名称，必填]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
</search>
